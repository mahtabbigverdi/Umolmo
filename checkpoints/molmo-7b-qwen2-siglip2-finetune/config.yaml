run_name: multitask_train
model:
  model_name: molmo
  llm:
    d_model: 3584
    n_heads: 28
    n_kv_heads: 4
    head_dim: null
    qkv_bias: true
    clip_qkv: null
    n_layers: 28
    mlp_ratio: 4
    mlp_hidden_size: 37888
    activation_type: swiglu
    block_type: sequential
    rope: true
    rope_full_precision: true
    rope_theta: 1000000.0
    rope_type: default
    rope_factor: null
    rope_high_freq_factor: null
    rope_low_freq_factor: null
    rope_original_max_position_embeddings: null
    attention_type: sdpa
    float32_attention: true
    attention_dropout: 0.0
    attention_layer_norm: false
    attention_layer_norm_type: olmo
    residual_dropout: 0.1
    response_residual_dropout: 0.0
    layer_norm_type: rms
    layer_norm_with_affine: true
    layer_norm_eps: 1.0e-06
    attention_layer_norm_with_affine: true
    max_sequence_length: 4096
    max_position_embeddings: null
    include_bias: false
    bias_for_layer_norm: null
    norm_after: false
    moe_num_experts: 8
    moe_top_k: 2
    moe_mlp_impl: sparse
    moe_log_expert_assignment: false
    moe_shared_expert: false
    moe_lbl_in_fp32: false
    moe_interleave: false
    moe_loss_weight: 0.1
    moe_zloss_weight: null
    moe_dropless: true
    moe_capacity_factor: 1.25
    embedding_dropout: 0.0
    scale_logits: false
    vocab_size: 152064
    additional_vocab_size: 128
    weight_tying: false
    embedding_size: 152064
    use_position_ids: true
    tokenizer:
      identifier: Qwen/Qwen2.5-7B
      tokenizer_dir: null
    init_path: gs://mm-olmo/pretrained_llms/qwen2.5-7b.pt
    init_incremental: null
    new_embedding_init_range: 0.02
    initializer_range: 0.02
    normalize_input_embeds: false
    activation_checkpoint: whole_layer
    compile: blocks
    fix_pad_tokenizer: false
    init_std: 0.02
    init_fn: normal
    init_cutoff_factor: null
  vision_backbone:
    vit:
      image_model_type: siglip
      image_default_input_size:
      - 378
      - 378
      image_patch_size: 14
      image_pos_patch_size: 14
      image_emb_dim: 1152
      image_num_heads: 16
      image_num_key_value_heads: 16
      image_num_layers: 27
      image_head_dim: 72
      image_mlp_dim: 4304
      image_mlp_activations: gelu_pytorch_tanh
      image_dropout_rate: 0.0
      image_num_pos: 729
      image_norm_eps: 1.0e-06
      attention_dropout: 0.0
      residual_dropout: 0.0
      initializer_range: 0.02
      float32_attention: true
      attention_type: sdpa
      activation_checkpointing: true
      init_path: gs://mm-olmo/pretrained_image_encoders/siglip2-so400m-14-384.pt
      resize_mode: siglip
      pad_value: 0.0
      normalize: siglip
    image_pooling_2d: attention_meanq
    pooling_attention_mask: false
    image_projector: mlp
    image_padding_embed: null
    vit_layers:
    - -3
    - -9
    skip_unused_layers: true
    image_feature_dropout: 0.0
    connector_activation_checkpointing: true
    compile_vit: blocks
  data_formatter:
    prompt_templates: uber_model
    message_format: role
    system_prompt: demo_or_style
    always_start_with_space: false
    default_inference_len: 65
    select_answer: best
    debug: false
    image_last: false
    format_message_list: null
    p_one_message: 0.0
  mm_preprocessor:
    crop_mode: overlap-and-resize-c2
    max_crops: 8
    max_images: null
    pooling_w: 2
    pooling_h: 2
    overlap_margins:
    - 4
    - 4
    use_col_tokens: true
    loss_token_weighting: root_subsegments
    legacy_image_mask: false
  bi_directional_attn: null
  image_encoder: SigLip2
  vision_head_type: Linear
  per_image_output_tokens: 64
seed: 6198
epoch: null
dry_run: false
ft_llm: true
ft_gen: true
ft_vit: true
ft_connector: true
ft_embedding: all
optimizer:
  name: adamw
  learning_rate: 0.0001
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-05
  gen_learning_rate: 1.0e-05
  connector_learning_rate: 5.0e-06
  vit_learning_rate: 5.0e-06
  llm_learning_rate: 1.0e-05
  gen_weight_decay: 0.0
  connector_weight_decay: 0.0
  vit_weight_decay: 0.0
  llm_weight_decay: 0.0
  gen_betas:
  - 0.9
  - 0.95
  connector_betas:
  - 0.9
  - 0.95
  vit_betas:
  - 0.9
  - 0.95
  llm_betas:
  - 0.9
  - 0.95
  gen_eps: 1.0e-06
  connector_eps: 1.0e-06
  vit_eps: 1.0e-06
  llm_eps: 1.0e-06
  metrics_log_interval: -1
scheduler:
  name: multimodal
  units: steps
  t_warmup: 100
  t_max: null
  alpha_f: 0.1
  connector_t_warmup: 200
  vit_t_warmup: 200
  llm_t_warmup: 200
  gen_t_warmup: 200
  grad_clip_warmup_steps: null
  grad_clip_warmup_factor: null
  warmup_min_lr: 0.0
data:
  dataset: null
  mixture: null
  root_size_mixture:
  - rate: 1.0
    mixture:
      depth: null
  kwargs_mixture: null
  split: train
  seed: 50189
  pad: to_max
  sequence_length: 2304
  max_text_seq_len: null
  shuffle: true
  start_index: 0
  num_workers: 0
  drop_last: true
  pin_memory: true
  prefetch_factor: null
  persistent_workers: false
  timeout: 0
restore_dataloader: true
fast_forward_batches: null
evaluators: []
eval_interval: 2000
inf_evaluators:
- label: depth
  data:
    dataset: depth
    mixture: null
    root_size_mixture: null
    kwargs_mixture: null
    split: validation
    seed: 691203
    pad: to_max
    sequence_length: 1792
    max_text_seq_len: null
    shuffle: true
    start_index: 0
    num_workers: 0
    drop_last: true
    pin_memory: true
    prefetch_factor: null
    persistent_workers: true
    timeout: 0
  evaluator:
    n_to_log: 0
    num_wandb_examples: 32
    save_predictions: null
    save_tokens: false
    vqa_eval: ''
    pointing_eval: false
    count_eval: false
    point_count_eval: false
    android_eval: false
    clock_eval: false
    clock_bench_eval: false
    math_vista_eval: false
    temp_compass_eval: ''
    temp_compass_disable_api: false
    video_mme_eval: ''
    mlvu_gen_eval: false
    long_video_bench_eval: false
    plm_fgqa_eval: false
  max_new_tokens: 12
  device_batch_size: 4
  subset_num_batches: null
  max_examples: 2048
  console_log_interval: 20
  include_image: false
inf_eval_interval: 2000
eval_on_last_step: true
eval_on_load: false
save_folder: /mmfs1/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune
checkpointer_config:
  save_thread_count: null
  load_thread_count: null
  pre_download: false
  work_dir: null
  throttle_uploads: false
canceled_check_interval: 50
save_interval: 2000
save_num_checkpoints_to_keep: 1
save_final_unsharded_checkpoint: false
save_interval_ephemeral: null
save_overwrite: true
load_path: null
reset_optimizer_state: false
reset_trainer_state: false
initial_model_checkpoint: /mmfs1/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded
allow_resume: true
max_duration: 30000
global_train_batch_size: 256
device_train_microbatch_size: 4
max_grad_norm: 1.0
multi_component_grad_norm: true
batch_divisor: global_batch
max_grad_norm_ratio: null
precision: amp_bf16
wandb:
  project: mmseek
  entity: allenai-team1
  group: null
  name: molmo-7b-qwen2-siglip2-finetune
  tags:
  - watching
  log_artifacts: false
  rank_zero_only: true
  log_interval: 20
  allow_resume: false
beaker_log_interval: 50
speed_monitor:
  window_size: 20
  gpu_flops_available: null
console_log_interval: 20
gen1_gc_interval: 1
compile:
  mode: default
  fullgraph: false
  dynamic: false
  backend: inductor
activation_checkpointing: true
fsdp:
  fsdp2: true
  precision: float
  use_orig_params: true
  wrapping_strategy: by_block_and_size
  sharding_strategy: FULL_SHARD
  hybrid_sharding_num_model_replicas: null
softmax_auxiliary_loss: true
softmax_auxiliary_loss_scale: 0.0001
time_limit: null
extra_steps_after_cancel: 10
python_profiling: false
torch_profiling: false
stop_at: 30000
stop_after: null
fused_loss: false
compile_loss: true
runtime_data:
  args: launch_scripts/train_multitask_model.py smallmahtab /mmfs1/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded
    --wandb.name=molmo-7b-qwen2-siglip2-finetune --wandb.entity=allenai-team1 --wandb.project=mmseek
    --save_folder=/mmfs1/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune
    --save_overwrite
  hostname: g3086
  date: 07/16/2025, 18:59
  world_size: 4
  resuming_from: null
  beaker_experiment_id: null
  beaker_experiment_url: null
  wandb_id: u51zs12f
  wandb_url: https://wandb.ai/allenai-team1/mmseek/runs/u51zs12f
image_generation_loss_type: cosine

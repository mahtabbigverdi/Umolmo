2025-07-16 18:59:05,453 INFO    MainThread:2488 [wandb_setup.py:_flush():81] Current SDK version is 0.20.1
2025-07-16 18:59:05,453 INFO    MainThread:2488 [wandb_setup.py:_flush():81] Configure stats pid to 2488
2025-07-16 18:59:05,453 INFO    MainThread:2488 [wandb_setup.py:_flush():81] Loading settings from /mmfs1/home/mahtab/.config/wandb/settings
2025-07-16 18:59:05,453 INFO    MainThread:2488 [wandb_setup.py:_flush():81] Loading settings from /mmfs1/gscratch/krishna/mahtab/Umolmo/wandb/settings
2025-07-16 18:59:05,455 INFO    MainThread:2488 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-07-16 18:59:05,455 INFO    MainThread:2488 [wandb_init.py:setup_run_log_directory():703] Logging user logs to /mmfs1/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune/wandb/wandb/run-20250716_185905-u51zs12f/logs/debug.log
2025-07-16 18:59:05,455 INFO    MainThread:2488 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to /mmfs1/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune/wandb/wandb/run-20250716_185905-u51zs12f/logs/debug-internal.log
2025-07-16 18:59:05,456 INFO    MainThread:2488 [wandb_init.py:init():831] calling init triggers
2025-07-16 18:59:05,456 INFO    MainThread:2488 [wandb_init.py:init():836] wandb.init called with sweep_config: {}
config: {'run_name': 'multitask_train', 'model': {'model_name': 'molmo', 'llm': {'d_model': 3584, 'n_heads': 28, 'n_kv_heads': 4, 'head_dim': None, 'qkv_bias': True, 'clip_qkv': None, 'n_layers': 28, 'mlp_ratio': 4, 'mlp_hidden_size': 37888, 'activation_type': 'swiglu', 'block_type': 'sequential', 'rope': True, 'rope_full_precision': True, 'rope_theta': 1000000.0, 'rope_type': 'default', 'rope_factor': None, 'rope_high_freq_factor': None, 'rope_low_freq_factor': None, 'rope_original_max_position_embeddings': None, 'attention_type': 'sdpa', 'float32_attention': True, 'attention_dropout': 0.0, 'attention_layer_norm': False, 'attention_layer_norm_type': 'olmo', 'residual_dropout': 0.1, 'response_residual_dropout': 0.0, 'layer_norm_type': 'rms', 'layer_norm_with_affine': True, 'layer_norm_eps': 1e-06, 'attention_layer_norm_with_affine': True, 'max_sequence_length': 4096, 'max_position_embeddings': None, 'include_bias': False, 'bias_for_layer_norm': None, 'norm_after': False, 'moe_num_experts': 8, 'moe_top_k': 2, 'moe_mlp_impl': 'sparse', 'moe_log_expert_assignment': False, 'moe_shared_expert': False, 'moe_lbl_in_fp32': False, 'moe_interleave': False, 'moe_loss_weight': 0.1, 'moe_zloss_weight': None, 'moe_dropless': True, 'moe_capacity_factor': 1.25, 'embedding_dropout': 0.0, 'scale_logits': False, 'vocab_size': 152064, 'additional_vocab_size': 128, 'weight_tying': False, 'embedding_size': 152064, 'use_position_ids': True, 'tokenizer': {'identifier': 'Qwen/Qwen2.5-7B', 'tokenizer_dir': None}, 'init_path': 'gs://mm-olmo/pretrained_llms/qwen2.5-7b.pt', 'init_incremental': None, 'new_embedding_init_range': 0.02, 'initializer_range': 0.02, 'normalize_input_embeds': False, 'activation_checkpoint': 'whole_layer', 'compile': 'blocks', 'fix_pad_tokenizer': False, 'init_std': 0.02, 'init_fn': 'normal', 'init_cutoff_factor': None}, 'vision_backbone': {'vit': {'image_model_type': 'siglip', 'image_default_input_size': (378, 378), 'image_patch_size': 14, 'image_pos_patch_size': 14, 'image_emb_dim': 1152, 'image_num_heads': 16, 'image_num_key_value_heads': 16, 'image_num_layers': 27, 'image_head_dim': 72, 'image_mlp_dim': 4304, 'image_mlp_activations': 'gelu_pytorch_tanh', 'image_dropout_rate': 0.0, 'image_num_pos': 729, 'image_norm_eps': 1e-06, 'attention_dropout': 0.0, 'residual_dropout': 0.0, 'initializer_range': 0.02, 'float32_attention': True, 'attention_type': 'sdpa', 'activation_checkpointing': True, 'init_path': 'gs://mm-olmo/pretrained_image_encoders/siglip2-so400m-14-384.pt', 'resize_mode': 'siglip', 'pad_value': 0.0, 'normalize': 'siglip'}, 'image_pooling_2d': 'attention_meanq', 'pooling_attention_mask': False, 'image_projector': 'mlp', 'image_padding_embed': None, 'vit_layers': (-3, -9), 'skip_unused_layers': True, 'image_feature_dropout': 0.0, 'connector_activation_checkpointing': True, 'compile_vit': 'blocks'}, 'data_formatter': {'prompt_templates': 'uber_model', 'message_format': 'role', 'system_prompt': 'demo_or_style', 'always_start_with_space': False, 'default_inference_len': 65, 'select_answer': 'best', 'debug': False, 'image_last': False, 'format_message_list': None, 'p_one_message': 0.0}, 'mm_preprocessor': {'crop_mode': 'overlap-and-resize-c2', 'max_crops': 8, 'max_images': None, 'pooling_w': 2, 'pooling_h': 2, 'overlap_margins': [4, 4], 'use_col_tokens': True, 'loss_token_weighting': 'root_subsegments', 'legacy_image_mask': False}, 'bi_directional_attn': None, 'image_encoder': 'SigLip2', 'vision_head_type': 'Linear', 'per_image_output_tokens': 64}, 'seed': 6198, 'epoch': None, 'dry_run': False, 'ft_llm': True, 'ft_gen': True, 'ft_vit': True, 'ft_connector': True, 'ft_embedding': 'all', 'optimizer': {'name': 'adamw', 'learning_rate': 0.0001, 'weight_decay': 0.01, 'betas': (0.9, 0.95), 'eps': 1e-05, 'gen_learning_rate': 1e-05, 'connector_learning_rate': 5e-06, 'vit_learning_rate': 5e-06, 'llm_learning_rate': 1e-05, 'gen_weight_decay': 0.0, 'connector_weight_decay': 0.0, 'vit_weight_decay': 0.0, 'llm_weight_decay': 0.0, 'gen_betas': (0.9, 0.95), 'connector_betas': (0.9, 0.95), 'vit_betas': (0.9, 0.95), 'llm_betas': (0.9, 0.95), 'gen_eps': 1e-06, 'connector_eps': 1e-06, 'vit_eps': 1e-06, 'llm_eps': 1e-06, 'metrics_log_interval': -1}, 'scheduler': {'name': 'multimodal', 'units': 'steps', 't_warmup': 100, 't_max': None, 'alpha_f': 0.1, 'connector_t_warmup': 200, 'vit_t_warmup': 200, 'llm_t_warmup': 200, 'gen_t_warmup': 200, 'grad_clip_warmup_steps': None, 'grad_clip_warmup_factor': None, 'warmup_min_lr': 0.0}, 'data': {'dataset': None, 'mixture': None, 'root_size_mixture': [{'rate': 1.0, 'mixture': {'depth': None}}], 'kwargs_mixture': None, 'split': 'train', 'seed': 50189, 'pad': 'to_max', 'sequence_length': 2304, 'max_text_seq_len': None, 'shuffle': True, 'start_index': 0, 'num_workers': 0, 'drop_last': True, 'pin_memory': True, 'prefetch_factor': None, 'persistent_workers': False, 'timeout': 0}, 'restore_dataloader': True, 'fast_forward_batches': None, 'evaluators': [], 'eval_interval': 2000, 'inf_evaluators': [{'label': 'depth', 'data': {'dataset': 'depth', 'mixture': None, 'root_size_mixture': None, 'kwargs_mixture': None, 'split': 'validation', 'seed': 691203, 'pad': 'to_max', 'sequence_length': 1792, 'max_text_seq_len': None, 'shuffle': True, 'start_index': 0, 'num_workers': 0, 'drop_last': True, 'pin_memory': True, 'prefetch_factor': None, 'persistent_workers': True, 'timeout': 0}, 'evaluator': {'n_to_log': 0, 'num_wandb_examples': 32, 'save_predictions': None, 'save_tokens': False, 'vqa_eval': '', 'pointing_eval': False, 'count_eval': False, 'point_count_eval': False, 'android_eval': False, 'clock_eval': False, 'clock_bench_eval': False, 'math_vista_eval': False, 'temp_compass_eval': '', 'temp_compass_disable_api': False, 'video_mme_eval': '', 'mlvu_gen_eval': False, 'long_video_bench_eval': False, 'plm_fgqa_eval': False}, 'max_new_tokens': 12, 'device_batch_size': 4, 'subset_num_batches': None, 'max_examples': 2048, 'console_log_interval': 20, 'include_image': False}], 'inf_eval_interval': 2000, 'eval_on_last_step': True, 'eval_on_load': False, 'save_folder': '/mmfs1/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune', 'checkpointer_config': {'save_thread_count': None, 'load_thread_count': None, 'pre_download': False, 'work_dir': None, 'throttle_uploads': False}, 'canceled_check_interval': 50, 'save_interval': 2000, 'save_num_checkpoints_to_keep': 1, 'save_final_unsharded_checkpoint': False, 'save_interval_ephemeral': None, 'save_overwrite': True, 'load_path': None, 'reset_optimizer_state': False, 'reset_trainer_state': False, 'initial_model_checkpoint': '/mmfs1/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded', 'allow_resume': True, 'max_duration': 30000, 'global_train_batch_size': 256, 'device_train_microbatch_size': 4, 'max_grad_norm': 1.0, 'multi_component_grad_norm': True, 'batch_divisor': 'global_batch', 'max_grad_norm_ratio': None, 'precision': 'amp_bf16', 'beaker_log_interval': 50, 'speed_monitor': {'window_size': 20, 'gpu_flops_available': None}, 'console_log_interval': 20, 'gen1_gc_interval': 1, 'compile': {'mode': 'default', 'fullgraph': False, 'dynamic': False, 'backend': 'inductor'}, 'activation_checkpointing': True, 'fsdp': {'fsdp2': True, 'precision': 'float', 'use_orig_params': True, 'wrapping_strategy': 'by_block_and_size', 'sharding_strategy': <ShardingStrategy.FULL_SHARD: 1>, 'hybrid_sharding_num_model_replicas': None}, 'softmax_auxiliary_loss': True, 'softmax_auxiliary_loss_scale': 0.0001, 'time_limit': None, 'extra_steps_after_cancel': 10, 'python_profiling': False, 'torch_profiling': False, 'stop_at': 30000, 'stop_after': None, 'fused_loss': False, 'compile_loss': True, 'runtime_data': None, 'image_generation_loss_type': 'cosine', '_wandb': {}}
2025-07-16 18:59:05,456 INFO    MainThread:2488 [wandb_init.py:init():872] starting backend
2025-07-16 18:59:06,056 INFO    MainThread:2488 [wandb_init.py:init():875] sending inform_init request
2025-07-16 18:59:06,101 INFO    MainThread:2488 [wandb_init.py:init():883] backend started and connected
2025-07-16 18:59:06,105 INFO    MainThread:2488 [wandb_init.py:init():956] updated telemetry
2025-07-16 18:59:06,132 INFO    MainThread:2488 [wandb_init.py:init():980] communicating run to backend with 90.0 second timeout
2025-07-16 18:59:06,609 INFO    MainThread:2488 [wandb_init.py:init():1032] starting run threads in backend
2025-07-16 18:59:11,259 INFO    MainThread:2488 [wandb_run.py:_console_start():2453] atexit reg
2025-07-16 18:59:11,259 INFO    MainThread:2488 [wandb_run.py:_redirect():2301] redirect: wrap_raw
2025-07-16 18:59:11,259 INFO    MainThread:2488 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-07-16 18:59:11,259 INFO    MainThread:2488 [wandb_run.py:_redirect():2393] Redirects installed.
2025-07-16 18:59:11,373 INFO    MainThread:2488 [wandb_init.py:init():1078] run started, returning control to user process

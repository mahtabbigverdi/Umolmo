2025-06-26 13:45:28.460	g3085:1	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:45:28.472	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:45:28.473	g3085:0	olmo.util:177	CRITICAL	Uncaught NotImplementedError: smallmahtab
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 225, in <module>
    raise NotImplementedError(args.mixture)
NotImplementedError: smallmahtab
2025-06-26 13:45:28.462	g3085:1	olmo.util:177	CRITICAL	Uncaught NotImplementedError: smallmahtab
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 225, in <module>
    raise NotImplementedError(args.mixture)
NotImplementedError: smallmahtab
2025-06-26 13:47:21.950	g3085:1	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:47:21.963	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:47:22.124	g3085:0	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:47:22.124	g3085:1	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:47:23.315	g3085:1	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:47:23.315	g3085:0	train:54	INFO	Configuration:
2025-06-26 13:47:23.316	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=3584, n_heads=28, n_kv_heads=4, head_dim=None, qkv_bias=True, clip_qkv=None, n_layers=28, mlp_ratio=4, mlp_hidden_size=37888, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=1000000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='rms', layer_norm_with_affine=True, layer_norm_eps=1e-06, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=False, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.0, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=152064, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2.5-7B', tokenizer_dir=None), init_path='gs://mm-olmo/pretrained_llms/qwen2.5-7b.pt', init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='siglip', image_default_input_size=(378, 378), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1152, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=27, image_head_dim=72, image_mlp_dim=4304, image_mlp_activations='gelu_pytorch_tanh', image_dropout_rate=0.0, image_num_pos=729, image_norm_eps=1e-06, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path='gs://mm-olmo/pretrained_image_encoders/siglip2-so400m-14-384.pt', resize_mode='siglip', pad_value=0.0, normalize='siglip'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-3, -9), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='overlap-and-resize-c2', max_crops=8, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=2000, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=2048, console_log_interval=20, include_image=False)], inf_eval_interval=2000, eval_on_last_step=True, eval_on_load=False, save_folder='/weka/oe-training-default/mahtabb/mm_olmo/checkpoints/molmo-7b-qwen2-siglip2-finetune', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint='/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded', allow_resume=True, max_duration=30000, global_train_batch_size=256, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mmseek', entity='allenai-team1', group=None, name='molmo-7b-qwen2-siglip2-finetune', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=20, allow_resume=False), beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=20, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=30000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-26 13:47:23.344	g3085:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:47:23.610	g3085:0	olmo.util:177	CRITICAL	Uncaught OSError: PermissionError at /weka when downloading Qwen/Qwen2.5-7B. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.
Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/utils/hub.py", line 470, in cached_files
    hf_hub_download(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1125, in _hf_hub_download_to_cache_dir
    os.makedirs(os.path.dirname(blob_path), exist_ok=True)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  [Previous line repeated 4 more times]
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/weka'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 131, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 162, in build
    return build_tokenizer(
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 99, in build_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 982, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 814, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/utils/hub.py", line 312, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/utils/hub.py", line 515, in cached_files
    raise OSError(
OSError: PermissionError at /weka when downloading Qwen/Qwen2.5-7B. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.
2025-06-26 13:50:51.141	g3085:1	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:50:51.141	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:50:51.367	g3085:1	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:50:51.367	g3085:0	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:50:52.403	g3085:1	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:50:52.403	g3085:0	train:54	INFO	Configuration:
2025-06-26 13:50:52.404	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=3584, n_heads=28, n_kv_heads=4, head_dim=None, qkv_bias=True, clip_qkv=None, n_layers=28, mlp_ratio=4, mlp_hidden_size=37888, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=1000000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='rms', layer_norm_with_affine=True, layer_norm_eps=1e-06, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=False, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.0, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=152064, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2.5-7B', tokenizer_dir=None), init_path='gs://mm-olmo/pretrained_llms/qwen2.5-7b.pt', init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='siglip', image_default_input_size=(378, 378), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1152, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=27, image_head_dim=72, image_mlp_dim=4304, image_mlp_activations='gelu_pytorch_tanh', image_dropout_rate=0.0, image_num_pos=729, image_norm_eps=1e-06, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path='gs://mm-olmo/pretrained_image_encoders/siglip2-so400m-14-384.pt', resize_mode='siglip', pad_value=0.0, normalize='siglip'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-3, -9), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='overlap-and-resize-c2', max_crops=8, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=2000, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=2048, console_log_interval=20, include_image=False)], inf_eval_interval=2000, eval_on_last_step=True, eval_on_load=False, save_folder='/weka/oe-training-default/mahtabb/mm_olmo/checkpoints/molmo-7b-qwen2-siglip2-finetune', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint='/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded', allow_resume=True, max_duration=30000, global_train_batch_size=256, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mmseek', entity='allenai-team1', group=None, name='molmo-7b-qwen2-siglip2-finetune', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=20, allow_resume=False), beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=20, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=30000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-26 13:50:52.432	g3085:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:50:57.574	g3085:1	root:119	INFO	Padding tokenizer with 399 tokens
2025-06-26 13:50:57.603	g3085:0	root:119	INFO	Padding tokenizer with 399 tokens
2025-06-26 13:50:58.104	g3085:1	train:138	INFO	Freezing LLM: wte
2025-06-26 13:50:58.115	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-26 13:51:03.284	g3085:1	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 13:51:03.285	g3085:0	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 13:51:03.997	g3085:1	train:189	INFO	Total number of parameters: 8,119,417,808
2025-06-26 13:51:03.998	g3085:0	train:189	INFO	Total number of parameters: 8,119,417,808
2025-06-26 13:51:03.999	g3085:1	train:190	INFO	Number of non-embedding parameters: 7,573,961,680
2025-06-26 13:51:03.999	g3085:1	train:193	INFO	Peak GPU Memory (MB) after FSDP: 16317
2025-06-26 13:51:04.000	g3085:1	train:194	INFO	Model:
2025-06-26 13:51:04.000	g3085:0	train:190	INFO	Number of non-embedding parameters: 7,573,961,680
2025-06-26 13:51:04.001	g3085:0	train:193	INFO	Peak GPU Memory (MB) after FSDP: 16317
2025-06-26 13:51:04.000	g3085:1	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0-27): 28 x FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=3584, out_features=3584, bias=False)
          (ff_out): Linear(in_features=18944, out_features=3584, bias=False)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): RMSLayerNorm()
          (ff_norm): RMSLayerNorm()
          (att_proj): Linear(in_features=3584, out_features=4608, bias=True)
          (ff_proj): Linear(in_features=3584, out_features=37888, bias=False)
        )
      )
    )
    (ln_f): FSDPRMSLayerNorm()
    (emb_drop): Dropout(p=0.0, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=3584, out_features=152064, bias=False)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=2304, out_features=1152, bias=True)
        (wk): Linear(in_features=2304, out_features=1152, bias=True)
        (wv): Linear(in_features=2304, out_features=1152, bias=True)
        (wo): Linear(in_features=1152, out_features=1152, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1152, out_features=18944, bias=False)
        (w2): Linear(in_features=18944, out_features=3584, bias=False)
        (w3): Linear(in_features=1152, out_features=18944, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPSiglipVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0-24): 25 x FSDPCheckpointWrapper(
            (_checkpoint_wrapped_module): ResidualAttentionBlock(
              (attention): ViTMultiHeadDotProductAttention(
                (wq): Linear(in_features=1152, out_features=1152, bias=True)
                (wk): Linear(in_features=1152, out_features=1152, bias=True)
                (wv): Linear(in_features=1152, out_features=1152, bias=True)
                (wo): Linear(in_features=1152, out_features=1152, bias=True)
                (residual_dropout): Dropout(p=0.0, inplace=False)
              )
              (feed_forward): ViTMLP(
                (w1): Linear(in_features=1152, out_features=4304, bias=True)
                (act): PytorchGELUTanh()
                (w2): Linear(in_features=4304, out_features=1152, bias=True)
              )
              (attention_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (ffn_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
  )
)
2025-06-26 13:51:04.002	g3085:0	train:194	INFO	Model:
2025-06-26 13:51:04.008	g3085:1	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 13:51:04.009	g3085:1	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 13:51:04.009	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 13:51:04.002	g3085:0	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0-27): 28 x FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=3584, out_features=3584, bias=False)
          (ff_out): Linear(in_features=18944, out_features=3584, bias=False)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): RMSLayerNorm()
          (ff_norm): RMSLayerNorm()
          (att_proj): Linear(in_features=3584, out_features=4608, bias=True)
          (ff_proj): Linear(in_features=3584, out_features=37888, bias=False)
        )
      )
    )
    (ln_f): FSDPRMSLayerNorm()
    (emb_drop): Dropout(p=0.0, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=3584, out_features=152064, bias=False)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=2304, out_features=1152, bias=True)
        (wk): Linear(in_features=2304, out_features=1152, bias=True)
        (wv): Linear(in_features=2304, out_features=1152, bias=True)
        (wo): Linear(in_features=1152, out_features=1152, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1152, out_features=18944, bias=False)
        (w2): Linear(in_features=18944, out_features=3584, bias=False)
        (w3): Linear(in_features=1152, out_features=18944, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPSiglipVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0-24): 25 x FSDPCheckpointWrapper(
            (_checkpoint_wrapped_module): ResidualAttentionBlock(
              (attention): ViTMultiHeadDotProductAttention(
                (wq): Linear(in_features=1152, out_features=1152, bias=True)
                (wk): Linear(in_features=1152, out_features=1152, bias=True)
                (wv): Linear(in_features=1152, out_features=1152, bias=True)
                (wo): Linear(in_features=1152, out_features=1152, bias=True)
                (residual_dropout): Dropout(p=0.0, inplace=False)
              )
              (feed_forward): ViTMLP(
                (w1): Linear(in_features=1152, out_features=4304, bias=True)
                (act): PytorchGELUTanh()
                (w2): Linear(in_features=4304, out_features=1152, bias=True)
              )
              (attention_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (ffn_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
  )
)
2025-06-26 13:51:04.020	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 13:51:04.021	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 13:51:04.022	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 13:51:04.095	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 13:51:04.095	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 13:51:04.203	g3085:1	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-26 13:51:04.203	g3085:1	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-26 13:51:04.203	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-26 13:51:04.203	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-26 13:51:04.249	g3085:1	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=9 pooled_patches_idx=1414
2025-06-26 13:51:04.249	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=9 pooled_patches_idx=1414
2025-06-26 13:51:04.250	g3085:1	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-26 13:51:04.250	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-26 13:51:04.250	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-26 13:51:04.250	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-26 13:51:04.272	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-26 13:51:04.272	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-26 13:51:04.308	g3085:1	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=9 pooled_patches_idx=1414
2025-06-26 13:51:04.308	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=9 pooled_patches_idx=1414
2025-06-26 13:51:04.308	g3085:0	olmo.util:177	CRITICAL	Uncaught PermissionError: [Errno 13] Permission denied: '/weka'
Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default/mahtabb/mm_olmo/checkpoints/molmo-7b-qwen2-siglip2-finetune/wandb'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default/mahtabb/mm_olmo/checkpoints/molmo-7b-qwen2-siglip2-finetune'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default/mahtabb/mm_olmo/checkpoints'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default/mahtabb/mm_olmo'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default/mahtabb'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/weka/oe-training-default'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 232, in run_trainer
    wandb_dir.mkdir(parents=True, exist_ok=True)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1179, in mkdir
    self.parent.mkdir(parents=True, exist_ok=True)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1179, in mkdir
    self.parent.mkdir(parents=True, exist_ok=True)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1179, in mkdir
    self.parent.mkdir(parents=True, exist_ok=True)
  [Previous line repeated 3 more times]
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
PermissionError: [Errno 13] Permission denied: '/weka'
2025-06-26 13:53:42.486	g3085:1	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:53:42.501	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 13:53:42.583	g3085:0	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:53:42.585	g3085:1	root:265	INFO	Setting eval subset batches to 256
2025-06-26 13:53:42.694	g3085:0	train:54	INFO	Configuration:
2025-06-26 13:53:42.695	g3085:1	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:53:42.695	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=3584, n_heads=28, n_kv_heads=4, head_dim=None, qkv_bias=True, clip_qkv=None, n_layers=28, mlp_ratio=4, mlp_hidden_size=37888, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=1000000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='rms', layer_norm_with_affine=True, layer_norm_eps=1e-06, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=False, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.0, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=152064, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2.5-7B', tokenizer_dir=None), init_path='gs://mm-olmo/pretrained_llms/qwen2.5-7b.pt', init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='siglip', image_default_input_size=(378, 378), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1152, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=27, image_head_dim=72, image_mlp_dim=4304, image_mlp_activations='gelu_pytorch_tanh', image_dropout_rate=0.0, image_num_pos=729, image_norm_eps=1e-06, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path='gs://mm-olmo/pretrained_image_encoders/siglip2-so400m-14-384.pt', resize_mode='siglip', pad_value=0.0, normalize='siglip'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-3, -9), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='overlap-and-resize-c2', max_crops=8, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=2000, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=2048, console_log_interval=20, include_image=False)], inf_eval_interval=2000, eval_on_last_step=True, eval_on_load=False, save_folder='/gscratch/krishna/mahtab/Umolmo/checkpoints/molmo-7b-qwen2-siglip2-finetune', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint='/gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded', allow_resume=True, max_duration=30000, global_train_batch_size=256, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mmseek', entity='allenai-team1', group=None, name='molmo-7b-qwen2-siglip2-finetune', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=20, allow_resume=False), beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=20, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=30000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-26 13:53:42.723	g3085:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 13:53:44.777	g3085:1	root:119	INFO	Padding tokenizer with 399 tokens
2025-06-26 13:53:44.817	g3085:0	root:119	INFO	Padding tokenizer with 399 tokens
2025-06-26 13:53:45.292	g3085:1	train:138	INFO	Freezing LLM: wte
2025-06-26 13:53:45.317	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-26 13:53:46.832	g3085:1	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 13:53:46.834	g3085:0	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 13:53:47.196	g3085:1	train:189	INFO	Total number of parameters: 8,119,417,808
2025-06-26 13:53:47.198	g3085:1	train:190	INFO	Number of non-embedding parameters: 7,573,961,680
2025-06-26 13:53:47.198	g3085:1	train:193	INFO	Peak GPU Memory (MB) after FSDP: 16317
2025-06-26 13:53:47.199	g3085:1	train:194	INFO	Model:
2025-06-26 13:53:47.199	g3085:1	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0-27): 28 x FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=3584, out_features=3584, bias=False)
          (ff_out): Linear(in_features=18944, out_features=3584, bias=False)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): RMSLayerNorm()
          (ff_norm): RMSLayerNorm()
          (att_proj): Linear(in_features=3584, out_features=4608, bias=True)
          (ff_proj): Linear(in_features=3584, out_features=37888, bias=False)
        )
      )
    )
    (ln_f): FSDPRMSLayerNorm()
    (emb_drop): Dropout(p=0.0, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=3584, out_features=152064, bias=False)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=2304, out_features=1152, bias=True)
        (wk): Linear(in_features=2304, out_features=1152, bias=True)
        (wv): Linear(in_features=2304, out_features=1152, bias=True)
        (wo): Linear(in_features=1152, out_features=1152, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1152, out_features=18944, bias=False)
        (w2): Linear(in_features=18944, out_features=3584, bias=False)
        (w3): Linear(in_features=1152, out_features=18944, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPSiglipVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0-24): 25 x FSDPCheckpointWrapper(
            (_checkpoint_wrapped_module): ResidualAttentionBlock(
              (attention): ViTMultiHeadDotProductAttention(
                (wq): Linear(in_features=1152, out_features=1152, bias=True)
                (wk): Linear(in_features=1152, out_features=1152, bias=True)
                (wv): Linear(in_features=1152, out_features=1152, bias=True)
                (wo): Linear(in_features=1152, out_features=1152, bias=True)
                (residual_dropout): Dropout(p=0.0, inplace=False)
              )
              (feed_forward): ViTMLP(
                (w1): Linear(in_features=1152, out_features=4304, bias=True)
                (act): PytorchGELUTanh()
                (w2): Linear(in_features=4304, out_features=1152, bias=True)
              )
              (attention_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (ffn_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
  )
)
2025-06-26 13:53:47.204	g3085:0	train:189	INFO	Total number of parameters: 8,119,417,808
2025-06-26 13:53:47.206	g3085:0	train:190	INFO	Number of non-embedding parameters: 7,573,961,680
2025-06-26 13:53:47.207	g3085:1	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 13:53:47.207	g3085:0	train:193	INFO	Peak GPU Memory (MB) after FSDP: 16317
2025-06-26 13:53:47.208	g3085:0	train:194	INFO	Model:
2025-06-26 13:53:47.208	g3085:1	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 13:53:47.208	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 13:53:47.208	g3085:0	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0-27): 28 x FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=3584, out_features=3584, bias=False)
          (ff_out): Linear(in_features=18944, out_features=3584, bias=False)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): RMSLayerNorm()
          (ff_norm): RMSLayerNorm()
          (att_proj): Linear(in_features=3584, out_features=4608, bias=True)
          (ff_proj): Linear(in_features=3584, out_features=37888, bias=False)
        )
      )
    )
    (ln_f): FSDPRMSLayerNorm()
    (emb_drop): Dropout(p=0.0, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=3584, out_features=152064, bias=False)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=2304, out_features=1152, bias=True)
        (wk): Linear(in_features=2304, out_features=1152, bias=True)
        (wv): Linear(in_features=2304, out_features=1152, bias=True)
        (wo): Linear(in_features=1152, out_features=1152, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1152, out_features=18944, bias=False)
        (w2): Linear(in_features=18944, out_features=3584, bias=False)
        (w3): Linear(in_features=1152, out_features=18944, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPSiglipVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0-24): 25 x FSDPCheckpointWrapper(
            (_checkpoint_wrapped_module): ResidualAttentionBlock(
              (attention): ViTMultiHeadDotProductAttention(
                (wq): Linear(in_features=1152, out_features=1152, bias=True)
                (wk): Linear(in_features=1152, out_features=1152, bias=True)
                (wv): Linear(in_features=1152, out_features=1152, bias=True)
                (wo): Linear(in_features=1152, out_features=1152, bias=True)
                (residual_dropout): Dropout(p=0.0, inplace=False)
              )
              (feed_forward): ViTMLP(
                (w1): Linear(in_features=1152, out_features=4304, bias=True)
                (act): PytorchGELUTanh()
                (w2): Linear(in_features=4304, out_features=1152, bias=True)
              )
              (attention_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (ffn_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            )
          )
        )
      )
    )
  )
)
2025-06-26 13:53:47.226	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 13:53:47.227	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 13:53:47.227	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 13:53:47.264	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 13:53:47.265	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 13:53:47.388	g3085:1	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-26 13:53:47.388	g3085:1	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-26 13:53:47.388	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-26 13:53:47.388	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-26 13:53:47.412	g3085:1	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=9 pooled_patches_idx=1414
2025-06-26 13:53:47.412	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=9 pooled_patches_idx=1414
2025-06-26 13:53:47.412	g3085:1	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-26 13:53:47.413	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-26 13:53:47.413	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-26 13:53:47.413	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-26 13:53:47.416	g3085:1	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-26 13:53:47.417	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-26 13:53:47.420	g3085:1	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=9 pooled_patches_idx=1414
2025-06-26 13:53:47.420	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=9 pooled_patches_idx=1414
2025-06-26 13:53:51.171	g3085:1	train:306	INFO	Loading unshared model from /gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded
2025-06-26 13:53:51.170	g3085:0	train:306	INFO	Loading unshared model from /gscratch/krishna/mahtab/Umolmo/pretrained/step22347-unsharded
2025-06-26 13:54:17.445	g3085:1	train:321	INFO	Checkpoint successfully loaded in 26.3 seconds
2025-06-26 13:54:26.468	g3085:0	train:321	INFO	Checkpoint successfully loaded in 35.3 seconds
2025-06-26 13:54:27.380	g3085:1	train:330	INFO	Starting training...
2025-06-26 13:54:27.381	g3085:0	train:330	INFO	Starting training...
2025-06-26 13:54:27.382	g3085:1	olmo.train.trainer:759	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=18,496
2025-06-26 13:54:27.384	g3085:0	olmo.train.trainer:759	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=18,496
2025-06-26 14:00:56.553	g3085:1	olmo.util:177	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 1 has a total capacity of 79.25 GiB of which 4.77 GiB is free. Including non-PyTorch memory, this process has 74.46 GiB memory in use. Of the allocated memory 68.91 GiB is allocated by PyTorch, and 4.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 331, in run_trainer
    trainer.fit()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 985, in fit
    metrics = self.train_step(batch, compute_metrics=should_log_this_step)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 652, in train_step
    loss = self.train_batch(batch, True)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 636, in train_batch
    loss.backward()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 1 has a total capacity of 79.25 GiB of which 4.77 GiB is free. Including non-PyTorch memory, this process has 74.46 GiB memory in use. Of the allocated memory 68.91 GiB is allocated by PyTorch, and 4.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-26 14:00:57.400	g3085:0	olmo.util:177	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 0 has a total capacity of 79.25 GiB of which 4.77 GiB is free. Including non-PyTorch memory, this process has 74.46 GiB memory in use. Of the allocated memory 68.91 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 331, in run_trainer
    trainer.fit()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 985, in fit
    metrics = self.train_step(batch, compute_metrics=should_log_this_step)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 652, in train_step
    loss = self.train_batch(batch, True)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 636, in train_batch
    loss.backward()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 0 has a total capacity of 79.25 GiB of which 4.77 GiB is free. Including non-PyTorch memory, this process has 74.46 GiB memory in use. Of the allocated memory 68.91 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-26 14:10:16.545	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 14:14:22.418	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 266, in <module>
    logging.info(f"Setting eval subset batches to {eval_subset_batches}")
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 266, in <module>
    logging.info(f"Setting eval subset batches to {eval_subset_batches}")
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-26 14:15:06.977	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 14:15:07.050	g3085:0	train:54	INFO	Configuration:
2025-06-26 14:15:07.050	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None, 'doc_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False), InfDatasetEvaluatorConfig(label='doc_qa', data=DataLoaderConfig(dataset='doc_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='ansl,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-26 14:15:07.079	g3085:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 14:15:11.397	g3085:0	root:119	INFO	Padding tokenizer with 418 tokens
2025-06-26 14:15:11.855	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-26 14:15:13.341	g3085:0	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 14:15:14.588	g3085:0	train:189	INFO	Total number of parameters: 57,810,304
2025-06-26 14:15:14.589	g3085:0	train:190	INFO	Number of non-embedding parameters: 38,329,728
2025-06-26 14:15:14.590	g3085:0	train:193	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-26 14:15:14.590	g3085:0	train:194	INFO	Model:
2025-06-26 14:15:14.590	g3085:0	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152064, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-26 14:15:14.599	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 14:15:14.600	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 14:15:14.600	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 14:15:14.655	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 14:15:14.762	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset doc_qa/train
2025-06-26 14:15:14.763	g3085:0	root:390	INFO	Loading docqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/docqa/train_v1.0_withQT.json
2025-06-26 14:15:14.764	g3085:0	olmo.util:177	CRITICAL	Uncaught FileNotFoundError: file /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/docqa/train_v1.0_withQT.json not found
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 203, in run_trainer
    train_loader = cfg.data.build_train_dataloader(cfg.model, cfg.global_train_batch_size, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/data/data_loader.py", line 198, in build_train_dataloader
    dataset = get_dataset_by_name(name, self.split)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/data/get_dataset.py", line 196, in get_dataset_by_name
    return DocQa(split)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/data/academic_datasets_manual.py", line 380, in __init__
    super().__init__(split)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/data/dataset.py", line 79, in __init__
    self.data = self.load()[:self.sample]
  File "/gscratch/krishna/mahtab/Umolmo/olmo/data/academic_datasets_manual.py", line 391, in load
    with open(cached_path(src, cache_dir=environ.get("MOLMO_CACHE_DIR"))) as f:
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/cached_path/_cached_path.py", line 243, in cached_path
    raise FileNotFoundError(f"file {url_or_filename} not found")
FileNotFoundError: file /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/docqa/train_v1.0_withQT.json not found
2025-06-26 14:16:00.127	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-26 14:16:00.192	g3085:0	train:54	INFO	Configuration:
2025-06-26 14:16:00.192	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-26 14:16:00.218	g3085:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-26 14:16:02.105	g3085:0	root:119	INFO	Padding tokenizer with 418 tokens
2025-06-26 14:16:02.566	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-26 14:16:03.928	g3085:0	train:178	INFO	Wrapping model with FSDP2...
2025-06-26 14:16:04.053	g3085:0	train:189	INFO	Total number of parameters: 57,810,304
2025-06-26 14:16:04.054	g3085:0	train:190	INFO	Number of non-embedding parameters: 38,329,728
2025-06-26 14:16:04.054	g3085:0	train:193	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-26 14:16:04.055	g3085:0	train:194	INFO	Model:
2025-06-26 14:16:04.055	g3085:0	train:195	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152064, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-26 14:16:04.064	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-26 14:16:04.064	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-26 14:16:04.065	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-26 14:16:04.087	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-26 14:16:04.168	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-26 14:16:04.168	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-26 14:16:04.170	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-26 14:16:04.170	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-26 14:16:04.171	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-26 14:16:04.174	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-26 14:16:04.177	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-26 14:16:04.379	g3085:0	train:330	INFO	Starting training...
2025-06-26 14:16:04.380	g3085:0	olmo.train.trainer:759	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=369.6
2025-06-26 14:16:38.016	g3085:0	olmo.train.trainer:759	INFO	[step=5/1000]
    train/CrossEntropyLoss=11.97
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=92,160
    throughput/device/tokens_per_second=61,535
    throughput/device/batches_per_second=3.339
2025-06-26 14:16:39.424	g3085:0	olmo.train.trainer:759	INFO	[step=10/1000]
    train/CrossEntropyLoss=11.98
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=184,320
    throughput/device/tokens_per_second=63,668
    throughput/device/batches_per_second=3.454
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:16:40.833	g3085:0	olmo.train.trainer:759	INFO	[step=15/1000]
    train/CrossEntropyLoss=12.04
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=276,480
    throughput/device/tokens_per_second=64,271
    throughput/device/batches_per_second=3.487
2025-06-26 14:16:42.242	g3085:0	olmo.train.trainer:759	INFO	[step=20/1000]
    train/CrossEntropyLoss=11.96
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=368,640
    throughput/device/tokens_per_second=64,569
    throughput/device/batches_per_second=3.503
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:16:42.243	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:16:56.647	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:16:56.663	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:16:56.664	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 14.4 seconds
2025-06-26 14:16:58.079	g3085:0	olmo.train.trainer:759	INFO	[step=25/1000]
    train/CrossEntropyLoss=12.04
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=460,800
    throughput/device/tokens_per_second=65,281
    throughput/device/batches_per_second=3.542
2025-06-26 14:16:59.495	g3085:0	olmo.train.trainer:759	INFO	[step=30/1000]
    train/CrossEntropyLoss=11.98
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=552,960
    throughput/device/tokens_per_second=65,196
    throughput/device/batches_per_second=3.537
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:00.906	g3085:0	olmo.train.trainer:759	INFO	[step=35/1000]
    train/CrossEntropyLoss=11.98
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=645,120
    throughput/device/tokens_per_second=65,217
    throughput/device/batches_per_second=3.538
2025-06-26 14:17:02.318	g3085:0	olmo.train.trainer:759	INFO	[step=40/1000]
    train/CrossEntropyLoss=11.93
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=737,280
    throughput/device/tokens_per_second=65,236
    throughput/device/batches_per_second=3.539
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:02.320	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:02.655	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:02.671	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:02.671	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:04.085	g3085:0	olmo.train.trainer:759	INFO	[step=45/1000]
    train/CrossEntropyLoss=11.90
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=829,440
    throughput/device/tokens_per_second=65,351
    throughput/device/batches_per_second=3.546
2025-06-26 14:17:05.497	g3085:0	olmo.train.trainer:759	INFO	[step=50/1000]
    train/CrossEntropyLoss=11.85
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=921,600
    throughput/device/tokens_per_second=65,301
    throughput/device/batches_per_second=3.543
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:06.909	g3085:0	olmo.train.trainer:759	INFO	[step=55/1000]
    train/CrossEntropyLoss=11.81
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,013,760
    throughput/device/tokens_per_second=65,297
    throughput/device/batches_per_second=3.543
2025-06-26 14:17:08.321	g3085:0	olmo.train.trainer:759	INFO	[step=60/1000]
    train/CrossEntropyLoss=11.77
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,105,920
    throughput/device/tokens_per_second=65,293
    throughput/device/batches_per_second=3.542
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:08.322	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:08.675	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:08.691	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:08.692	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:10.105	g3085:0	olmo.train.trainer:759	INFO	[step=65/1000]
    train/CrossEntropyLoss=11.78
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,198,080
    throughput/device/tokens_per_second=65,360
    throughput/device/batches_per_second=3.546
2025-06-26 14:17:11.518	g3085:0	olmo.train.trainer:759	INFO	[step=70/1000]
    train/CrossEntropyLoss=11.69
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,290,240
    throughput/device/tokens_per_second=65,305
    throughput/device/batches_per_second=3.543
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:12.928	g3085:0	olmo.train.trainer:759	INFO	[step=75/1000]
    train/CrossEntropyLoss=11.61
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,382,400
    throughput/device/tokens_per_second=65,317
    throughput/device/batches_per_second=3.544
2025-06-26 14:17:14.346	g3085:0	olmo.train.trainer:759	INFO	[step=80/1000]
    train/CrossEntropyLoss=11.67
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,474,560
    throughput/device/tokens_per_second=65,235
    throughput/device/batches_per_second=3.539
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:14.348	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:14.703	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:14.720	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:14.720	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:16.136	g3085:0	olmo.train.trainer:759	INFO	[step=85/1000]
    train/CrossEntropyLoss=11.64
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,566,720
    throughput/device/tokens_per_second=65,244
    throughput/device/batches_per_second=3.540
2025-06-26 14:17:17.547	g3085:0	olmo.train.trainer:759	INFO	[step=90/1000]
    train/CrossEntropyLoss=11.62
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,658,880
    throughput/device/tokens_per_second=65,295
    throughput/device/batches_per_second=3.543
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:18.959	g3085:0	olmo.train.trainer:759	INFO	[step=95/1000]
    train/CrossEntropyLoss=11.57
    train/ZLoss=0.0143
    train/Accuracy=0.0256
    throughput/total_tokens=1,751,040
    throughput/device/tokens_per_second=65,269
    throughput/device/batches_per_second=3.541
2025-06-26 14:17:20.370	g3085:0	olmo.train.trainer:759	INFO	[step=100/1000]
    train/CrossEntropyLoss=11.54
    train/ZLoss=0.0143
    train/Accuracy=0.1087
    throughput/total_tokens=1,843,200
    throughput/device/tokens_per_second=65,287
    throughput/device/batches_per_second=3.542
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:20.372	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:20.726	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:20.742	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:20.742	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:22.156	g3085:0	olmo.train.trainer:759	INFO	[step=105/1000]
    train/CrossEntropyLoss=11.45
    train/ZLoss=0.0143
    train/Accuracy=0.1702
    throughput/total_tokens=1,935,360
    throughput/device/tokens_per_second=65,317
    throughput/device/batches_per_second=3.544
2025-06-26 14:17:23.568	g3085:0	olmo.train.trainer:759	INFO	[step=110/1000]
    train/CrossEntropyLoss=11.53
    train/ZLoss=0.0143
    train/Accuracy=0.1750
    throughput/total_tokens=2,027,520
    throughput/device/tokens_per_second=65,295
    throughput/device/batches_per_second=3.543
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:24.987	g3085:0	olmo.train.trainer:759	INFO	[step=115/1000]
    train/CrossEntropyLoss=11.49
    train/ZLoss=0.0143
    train/Accuracy=0.1778
    throughput/total_tokens=2,119,680
    throughput/device/tokens_per_second=65,188
    throughput/device/batches_per_second=3.537
2025-06-26 14:17:26.401	g3085:0	olmo.train.trainer:759	INFO	[step=120/1000]
    train/CrossEntropyLoss=11.51
    train/ZLoss=0.0143
    train/Accuracy=0.1395
    throughput/total_tokens=2,211,840
    throughput/device/tokens_per_second=65,186
    throughput/device/batches_per_second=3.537
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:26.402	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:26.757	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:26.772	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:26.773	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:28.187	g3085:0	olmo.train.trainer:759	INFO	[step=125/1000]
    train/CrossEntropyLoss=11.44
    train/ZLoss=0.0143
    train/Accuracy=0.1351
    throughput/total_tokens=2,304,000
    throughput/device/tokens_per_second=65,334
    throughput/device/batches_per_second=3.545
2025-06-26 14:17:29.598	g3085:0	olmo.train.trainer:759	INFO	[step=130/1000]
    train/CrossEntropyLoss=11.51
    train/ZLoss=0.0143
    train/Accuracy=0.1321
    throughput/total_tokens=2,396,160
    throughput/device/tokens_per_second=65,323
    throughput/device/batches_per_second=3.544
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:31.014	g3085:0	olmo.train.trainer:759	INFO	[step=135/1000]
    train/CrossEntropyLoss=11.39
    train/ZLoss=0.0143
    train/Accuracy=0.1795
    throughput/total_tokens=2,488,320
    throughput/device/tokens_per_second=65,246
    throughput/device/batches_per_second=3.540
2025-06-26 14:17:32.427	g3085:0	olmo.train.trainer:759	INFO	[step=140/1000]
    train/CrossEntropyLoss=11.39
    train/ZLoss=0.0143
    train/Accuracy=0.1698
    throughput/total_tokens=2,580,480
    throughput/device/tokens_per_second=65,239
    throughput/device/batches_per_second=3.539
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:32.428	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:32.781	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:32.797	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:32.798	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:34.213	g3085:0	olmo.train.trainer:759	INFO	[step=145/1000]
    train/CrossEntropyLoss=11.37
    train/ZLoss=0.0143
    train/Accuracy=0.1579
    throughput/total_tokens=2,672,640
    throughput/device/tokens_per_second=65,293
    throughput/device/batches_per_second=3.542
2025-06-26 14:17:35.626	g3085:0	olmo.train.trainer:759	INFO	[step=150/1000]
    train/CrossEntropyLoss=11.33
    train/ZLoss=0.0142
    train/Accuracy=0.2353
    throughput/total_tokens=2,764,800
    throughput/device/tokens_per_second=65,244
    throughput/device/batches_per_second=3.540
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:37.042	g3085:0	olmo.train.trainer:759	INFO	[step=155/1000]
    train/CrossEntropyLoss=11.36
    train/ZLoss=0.0142
    train/Accuracy=0.1915
    throughput/total_tokens=2,856,960
    throughput/device/tokens_per_second=65,187
    throughput/device/batches_per_second=3.537
2025-06-26 14:17:38.457	g3085:0	olmo.train.trainer:759	INFO	[step=160/1000]
    train/CrossEntropyLoss=11.31
    train/ZLoss=0.0142
    train/Accuracy=0.1923
    throughput/total_tokens=2,949,120
    throughput/device/tokens_per_second=65,182
    throughput/device/batches_per_second=3.536
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:38.458	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:38.814	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:38.830	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:38.830	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:40.242	g3085:0	olmo.train.trainer:759	INFO	[step=165/1000]
    train/CrossEntropyLoss=11.39
    train/ZLoss=0.0142
    train/Accuracy=0.1489
    throughput/total_tokens=3,041,280
    throughput/device/tokens_per_second=65,420
    throughput/device/batches_per_second=3.549
2025-06-26 14:17:41.657	g3085:0	olmo.train.trainer:759	INFO	[step=170/1000]
    train/CrossEntropyLoss=11.40
    train/ZLoss=0.0142
    train/Accuracy=0.1765
    throughput/total_tokens=3,133,440
    throughput/device/tokens_per_second=65,276
    throughput/device/batches_per_second=3.541
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:43.072	g3085:0	olmo.train.trainer:759	INFO	[step=175/1000]
    train/CrossEntropyLoss=11.23
    train/ZLoss=0.0142
    train/Accuracy=0.2500
    throughput/total_tokens=3,225,600
    throughput/device/tokens_per_second=65,231
    throughput/device/batches_per_second=3.539
2025-06-26 14:17:44.506	g3085:0	olmo.train.trainer:759	INFO	[step=180/1000]
    train/CrossEntropyLoss=11.30
    train/ZLoss=0.0142
    train/Accuracy=0.1522
    throughput/total_tokens=3,317,760
    throughput/device/tokens_per_second=64,988
    throughput/device/batches_per_second=3.526
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:44.508	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:44.859	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:44.875	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:44.876	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:46.295	g3085:0	olmo.train.trainer:759	INFO	[step=185/1000]
    train/CrossEntropyLoss=11.25
    train/ZLoss=0.0142
    train/Accuracy=0.2222
    throughput/total_tokens=3,409,920
    throughput/device/tokens_per_second=65,060
    throughput/device/batches_per_second=3.530
2025-06-26 14:17:47.712	g3085:0	olmo.train.trainer:759	INFO	[step=190/1000]
    train/CrossEntropyLoss=11.19
    train/ZLoss=0.0142
    train/Accuracy=0.2286
    throughput/total_tokens=3,502,080
    throughput/device/tokens_per_second=65,064
    throughput/device/batches_per_second=3.530
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:49.131	g3085:0	olmo.train.trainer:759	INFO	[step=195/1000]
    train/CrossEntropyLoss=11.44
    train/ZLoss=0.0142
    train/Accuracy=0.1290
    throughput/total_tokens=3,594,240
    throughput/device/tokens_per_second=65,021
    throughput/device/batches_per_second=3.528
2025-06-26 14:17:50.560	g3085:0	olmo.train.trainer:759	INFO	[step=200/1000]
    train/CrossEntropyLoss=11.31
    train/ZLoss=0.0142
    train/Accuracy=0.1860
    throughput/total_tokens=3,686,400
    throughput/device/tokens_per_second=64,894
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:50.561	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:50.916	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:50.932	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:50.932	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:52.355	g3085:0	olmo.train.trainer:759	INFO	[step=205/1000]
    train/CrossEntropyLoss=11.37
    train/ZLoss=0.0142
    train/Accuracy=0.1143
    throughput/total_tokens=3,778,560
    throughput/device/tokens_per_second=64,929
    throughput/device/batches_per_second=3.523
2025-06-26 14:17:53.777	g3085:0	olmo.train.trainer:759	INFO	[step=210/1000]
    train/CrossEntropyLoss=11.22
    train/ZLoss=0.0142
    train/Accuracy=0.2000
    throughput/total_tokens=3,870,720
    throughput/device/tokens_per_second=64,864
    throughput/device/batches_per_second=3.519
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:55.199	g3085:0	olmo.train.trainer:759	INFO	[step=215/1000]
    train/CrossEntropyLoss=11.17
    train/ZLoss=0.0142
    train/Accuracy=0.2444
    throughput/total_tokens=3,962,880
    throughput/device/tokens_per_second=64,847
    throughput/device/batches_per_second=3.518
2025-06-26 14:17:56.621	g3085:0	olmo.train.trainer:759	INFO	[step=220/1000]
    train/CrossEntropyLoss=11.22
    train/ZLoss=0.0142
    train/Accuracy=0.1852
    throughput/total_tokens=4,055,040
    throughput/device/tokens_per_second=64,836
    throughput/device/batches_per_second=3.518
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:17:56.623	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:17:56.978	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:17:56.994	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:17:56.995	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:17:58.416	g3085:0	olmo.train.trainer:759	INFO	[step=225/1000]
    train/CrossEntropyLoss=11.32
    train/ZLoss=0.0142
    train/Accuracy=0.1333
    throughput/total_tokens=4,147,200
    throughput/device/tokens_per_second=64,982
    throughput/device/batches_per_second=3.526
2025-06-26 14:17:59.838	g3085:0	olmo.train.trainer:759	INFO	[step=230/1000]
    train/CrossEntropyLoss=11.21
    train/ZLoss=0.0142
    train/Accuracy=0.1905
    throughput/total_tokens=4,239,360
    throughput/device/tokens_per_second=64,902
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:01.259	g3085:0	olmo.train.trainer:759	INFO	[step=235/1000]
    train/CrossEntropyLoss=11.17
    train/ZLoss=0.0141
    train/Accuracy=0.1739
    throughput/total_tokens=4,331,520
    throughput/device/tokens_per_second=64,889
    throughput/device/batches_per_second=3.520
2025-06-26 14:18:02.674	g3085:0	olmo.train.trainer:759	INFO	[step=240/1000]
    train/CrossEntropyLoss=11.23
    train/ZLoss=0.0141
    train/Accuracy=0.1290
    throughput/total_tokens=4,423,680
    throughput/device/tokens_per_second=64,952
    throughput/device/batches_per_second=3.524
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:02.675	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:03.031	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:03.047	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:03.047	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:04.473	g3085:0	olmo.train.trainer:759	INFO	[step=245/1000]
    train/CrossEntropyLoss=11.17
    train/ZLoss=0.0141
    train/Accuracy=0.2000
    throughput/total_tokens=4,515,840
    throughput/device/tokens_per_second=64,809
    throughput/device/batches_per_second=3.516
2025-06-26 14:18:05.891	g3085:0	olmo.train.trainer:759	INFO	[step=250/1000]
    train/CrossEntropyLoss=11.21
    train/ZLoss=0.0141
    train/Accuracy=0.2250
    throughput/total_tokens=4,608,000
    throughput/device/tokens_per_second=64,898
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:07.312	g3085:0	olmo.train.trainer:759	INFO	[step=255/1000]
    train/CrossEntropyLoss=11.23
    train/ZLoss=0.0141
    train/Accuracy=0.1429
    throughput/total_tokens=4,700,160
    throughput/device/tokens_per_second=64,878
    throughput/device/batches_per_second=3.520
2025-06-26 14:18:08.735	g3085:0	olmo.train.trainer:759	INFO	[step=260/1000]
    train/CrossEntropyLoss=11.09
    train/ZLoss=0.0141
    train/Accuracy=0.1667
    throughput/total_tokens=4,792,320
    throughput/device/tokens_per_second=64,853
    throughput/device/batches_per_second=3.519
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:08.737	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:09.090	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:09.106	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:09.107	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:10.530	g3085:0	olmo.train.trainer:759	INFO	[step=265/1000]
    train/CrossEntropyLoss=11.16
    train/ZLoss=0.0141
    train/Accuracy=0.1842
    throughput/total_tokens=4,884,480
    throughput/device/tokens_per_second=64,891
    throughput/device/batches_per_second=3.521
2025-06-26 14:18:11.952	g3085:0	olmo.train.trainer:759	INFO	[step=270/1000]
    train/CrossEntropyLoss=11.15
    train/ZLoss=0.0141
    train/Accuracy=0.1667
    throughput/total_tokens=4,976,640
    throughput/device/tokens_per_second=64,867
    throughput/device/batches_per_second=3.519
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:13.372	g3085:0	olmo.train.trainer:759	INFO	[step=275/1000]
    train/CrossEntropyLoss=11.08
    train/ZLoss=0.0141
    train/Accuracy=0.2083
    throughput/total_tokens=5,068,800
    throughput/device/tokens_per_second=64,870
    throughput/device/batches_per_second=3.519
2025-06-26 14:18:14.798	g3085:0	olmo.train.trainer:759	INFO	[step=280/1000]
    train/CrossEntropyLoss=11.07
    train/ZLoss=0.0141
    train/Accuracy=0.1591
    throughput/total_tokens=5,160,960
    throughput/device/tokens_per_second=64,813
    throughput/device/batches_per_second=3.516
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:14.799	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:15.154	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:15.170	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:15.171	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:16.595	g3085:0	olmo.train.trainer:759	INFO	[step=285/1000]
    train/CrossEntropyLoss=11.12
    train/ZLoss=0.0141
    train/Accuracy=0.1389
    throughput/total_tokens=5,253,120
    throughput/device/tokens_per_second=64,861
    throughput/device/batches_per_second=3.519
2025-06-26 14:18:18.015	g3085:0	olmo.train.trainer:759	INFO	[step=290/1000]
    train/CrossEntropyLoss=11.23
    train/ZLoss=0.0141
    train/Accuracy=0.1481
    throughput/total_tokens=5,345,280
    throughput/device/tokens_per_second=64,892
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:19.428	g3085:0	olmo.train.trainer:759	INFO	[step=295/1000]
    train/CrossEntropyLoss=11.07
    train/ZLoss=0.0141
    train/Accuracy=0.1509
    throughput/total_tokens=5,437,440
    throughput/device/tokens_per_second=64,987
    throughput/device/batches_per_second=3.526
2025-06-26 14:18:20.848	g3085:0	olmo.train.trainer:759	INFO	[step=300/1000]
    train/CrossEntropyLoss=10.97
    train/ZLoss=0.0141
    train/Accuracy=0.1778
    throughput/total_tokens=5,529,600
    throughput/device/tokens_per_second=64,973
    throughput/device/batches_per_second=3.525
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:20.850	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:21.207	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:21.223	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:21.224	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:22.648	g3085:0	olmo.train.trainer:759	INFO	[step=305/1000]
    train/CrossEntropyLoss=11.06
    train/ZLoss=0.0141
    train/Accuracy=0.1471
    throughput/total_tokens=5,621,760
    throughput/device/tokens_per_second=64,873
    throughput/device/batches_per_second=3.520
2025-06-26 14:18:24.071	g3085:0	olmo.train.trainer:759	INFO	[step=310/1000]
    train/CrossEntropyLoss=11.05
    train/ZLoss=0.0140
    train/Accuracy=0.1489
    throughput/total_tokens=5,713,920
    throughput/device/tokens_per_second=64,811
    throughput/device/batches_per_second=3.516
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:25.503	g3085:0	olmo.train.trainer:759	INFO	[step=315/1000]
    train/CrossEntropyLoss=10.98
    train/ZLoss=0.0140
    train/Accuracy=0.1714
    throughput/total_tokens=5,806,080
    throughput/device/tokens_per_second=64,655
    throughput/device/batches_per_second=3.508
2025-06-26 14:18:26.927	g3085:0	olmo.train.trainer:759	INFO	[step=320/1000]
    train/CrossEntropyLoss=10.91
    train/ZLoss=0.0140
    train/Accuracy=0.1842
    throughput/total_tokens=5,898,240
    throughput/device/tokens_per_second=64,676
    throughput/device/batches_per_second=3.509
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:26.928	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:27.285	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:27.301	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:27.302	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:28.721	g3085:0	olmo.train.trainer:759	INFO	[step=325/1000]
    train/CrossEntropyLoss=11.06
    train/ZLoss=0.0140
    train/Accuracy=0.1333
    throughput/total_tokens=5,990,400
    throughput/device/tokens_per_second=65,079
    throughput/device/batches_per_second=3.531
2025-06-26 14:18:30.140	g3085:0	olmo.train.trainer:759	INFO	[step=330/1000]
    train/CrossEntropyLoss=10.94
    train/ZLoss=0.0140
    train/Accuracy=0.1795
    throughput/total_tokens=6,082,560
    throughput/device/tokens_per_second=65,033
    throughput/device/batches_per_second=3.528
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:31.560	g3085:0	olmo.train.trainer:759	INFO	[step=335/1000]
    train/CrossEntropyLoss=10.92
    train/ZLoss=0.0140
    train/Accuracy=0.2308
    throughput/total_tokens=6,174,720
    throughput/device/tokens_per_second=64,979
    throughput/device/batches_per_second=3.525
2025-06-26 14:18:32.984	g3085:0	olmo.train.trainer:759	INFO	[step=340/1000]
    train/CrossEntropyLoss=10.89
    train/ZLoss=0.0140
    train/Accuracy=0.1957
    throughput/total_tokens=6,266,880
    throughput/device/tokens_per_second=64,911
    throughput/device/batches_per_second=3.522
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:32.986	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:33.343	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:33.359	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:33.360	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:34.783	g3085:0	olmo.train.trainer:759	INFO	[step=345/1000]
    train/CrossEntropyLoss=10.92
    train/ZLoss=0.0140
    train/Accuracy=0.2000
    throughput/total_tokens=6,359,040
    throughput/device/tokens_per_second=64,916
    throughput/device/batches_per_second=3.522
2025-06-26 14:18:36.207	g3085:0	olmo.train.trainer:759	INFO	[step=350/1000]
    train/CrossEntropyLoss=10.90
    train/ZLoss=0.0140
    train/Accuracy=0.2632
    throughput/total_tokens=6,451,200
    throughput/device/tokens_per_second=64,820
    throughput/device/batches_per_second=3.517
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:37.627	g3085:0	olmo.train.trainer:759	INFO	[step=355/1000]
    train/CrossEntropyLoss=10.98
    train/ZLoss=0.0140
    train/Accuracy=0.1522
    throughput/total_tokens=6,543,360
    throughput/device/tokens_per_second=64,837
    throughput/device/batches_per_second=3.518
2025-06-26 14:18:39.049	g3085:0	olmo.train.trainer:759	INFO	[step=360/1000]
    train/CrossEntropyLoss=10.98
    train/ZLoss=0.0140
    train/Accuracy=0.1905
    throughput/total_tokens=6,635,520
    throughput/device/tokens_per_second=64,839
    throughput/device/batches_per_second=3.518
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:39.050	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:39.407	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:39.423	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:39.424	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:40.845	g3085:0	olmo.train.trainer:759	INFO	[step=365/1000]
    train/CrossEntropyLoss=10.92
    train/ZLoss=0.0140
    train/Accuracy=0.2051
    throughput/total_tokens=6,727,680
    throughput/device/tokens_per_second=64,987
    throughput/device/batches_per_second=3.526
2025-06-26 14:18:42.264	g3085:0	olmo.train.trainer:759	INFO	[step=370/1000]
    train/CrossEntropyLoss=10.84
    train/ZLoss=0.0140
    train/Accuracy=0.1860
    throughput/total_tokens=6,819,840
    throughput/device/tokens_per_second=64,981
    throughput/device/batches_per_second=3.525
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:43.683	g3085:0	olmo.train.trainer:759	INFO	[step=375/1000]
    train/CrossEntropyLoss=10.96
    train/ZLoss=0.0140
    train/Accuracy=0.1707
    throughput/total_tokens=6,912,000
    throughput/device/tokens_per_second=64,965
    throughput/device/batches_per_second=3.525
2025-06-26 14:18:45.108	g3085:0	olmo.train.trainer:759	INFO	[step=380/1000]
    train/CrossEntropyLoss=10.96
    train/ZLoss=0.0140
    train/Accuracy=0.1250
    throughput/total_tokens=7,004,160
    throughput/device/tokens_per_second=64,894
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:45.109	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:45.466	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:45.483	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:45.483	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:46.908	g3085:0	olmo.train.trainer:759	INFO	[step=385/1000]
    train/CrossEntropyLoss=10.99
    train/ZLoss=0.0139
    train/Accuracy=0.1463
    throughput/total_tokens=7,096,320
    throughput/device/tokens_per_second=64,807
    throughput/device/batches_per_second=3.516
2025-06-26 14:18:48.329	g3085:0	olmo.train.trainer:759	INFO	[step=390/1000]
    train/CrossEntropyLoss=10.94
    train/ZLoss=0.0139
    train/Accuracy=0.2391
    throughput/total_tokens=7,188,480
    throughput/device/tokens_per_second=64,858
    throughput/device/batches_per_second=3.519
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:49.753	g3085:0	olmo.train.trainer:759	INFO	[step=395/1000]
    train/CrossEntropyLoss=10.98
    train/ZLoss=0.0139
    train/Accuracy=0.1860
    throughput/total_tokens=7,280,640
    throughput/device/tokens_per_second=64,801
    throughput/device/batches_per_second=3.516
2025-06-26 14:18:51.181	g3085:0	olmo.train.trainer:759	INFO	[step=400/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0139
    train/Accuracy=0.2093
    throughput/total_tokens=7,372,800
    throughput/device/tokens_per_second=64,733
    throughput/device/batches_per_second=3.512
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:51.183	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:51.543	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:51.559	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:51.559	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:52.985	g3085:0	olmo.train.trainer:759	INFO	[step=405/1000]
    train/CrossEntropyLoss=10.87
    train/ZLoss=0.0139
    train/Accuracy=0.2000
    throughput/total_tokens=7,464,960
    throughput/device/tokens_per_second=64,789
    throughput/device/batches_per_second=3.515
2025-06-26 14:18:54.403	g3085:0	olmo.train.trainer:759	INFO	[step=410/1000]
    train/CrossEntropyLoss=10.89
    train/ZLoss=0.0139
    train/Accuracy=0.2000
    throughput/total_tokens=7,557,120
    throughput/device/tokens_per_second=64,901
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:55.823	g3085:0	olmo.train.trainer:759	INFO	[step=415/1000]
    train/CrossEntropyLoss=10.89
    train/ZLoss=0.0139
    train/Accuracy=0.1556
    throughput/total_tokens=7,649,280
    throughput/device/tokens_per_second=64,885
    throughput/device/batches_per_second=3.520
2025-06-26 14:18:57.246	g3085:0	olmo.train.trainer:759	INFO	[step=420/1000]
    train/CrossEntropyLoss=10.86
    train/ZLoss=0.0139
    train/Accuracy=0.1591
    throughput/total_tokens=7,741,440
    throughput/device/tokens_per_second=64,875
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:18:57.247	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:18:57.603	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:18:57.619	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:18:57.620	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:18:59.043	g3085:0	olmo.train.trainer:759	INFO	[step=425/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0139
    train/Accuracy=0.1765
    throughput/total_tokens=7,833,600
    throughput/device/tokens_per_second=64,918
    throughput/device/batches_per_second=3.522
2025-06-26 14:19:00.476	g3085:0	olmo.train.trainer:759	INFO	[step=430/1000]
    train/CrossEntropyLoss=10.90
    train/ZLoss=0.0139
    train/Accuracy=0.1481
    throughput/total_tokens=7,925,760
    throughput/device/tokens_per_second=64,608
    throughput/device/batches_per_second=3.505
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:01.895	g3085:0	olmo.train.trainer:759	INFO	[step=435/1000]
    train/CrossEntropyLoss=11.31
    train/ZLoss=0.0139
    train/Accuracy=0.0984
    throughput/total_tokens=8,017,920
    throughput/device/tokens_per_second=64,719
    throughput/device/batches_per_second=3.511
2025-06-26 14:19:03.324	g3085:0	olmo.train.trainer:759	INFO	[step=440/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0139
    train/Accuracy=0.1600
    throughput/total_tokens=8,110,080
    throughput/device/tokens_per_second=64,663
    throughput/device/batches_per_second=3.508
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:03.326	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:03.682	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:03.698	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:03.698	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:05.120	g3085:0	olmo.train.trainer:759	INFO	[step=445/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0139
    train/Accuracy=0.1731
    throughput/total_tokens=8,202,240
    throughput/device/tokens_per_second=64,945
    throughput/device/batches_per_second=3.523
2025-06-26 14:19:06.541	g3085:0	olmo.train.trainer:759	INFO	[step=450/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0139
    train/Accuracy=0.2051
    throughput/total_tokens=8,294,400
    throughput/device/tokens_per_second=64,939
    throughput/device/batches_per_second=3.523
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:07.960	g3085:0	olmo.train.trainer:759	INFO	[step=455/1000]
    train/CrossEntropyLoss=10.92
    train/ZLoss=0.0139
    train/Accuracy=0.1538
    throughput/total_tokens=8,386,560
    throughput/device/tokens_per_second=64,916
    throughput/device/batches_per_second=3.522
2025-06-26 14:19:09.384	g3085:0	olmo.train.trainer:759	INFO	[step=460/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0138
    train/Accuracy=0.1628
    throughput/total_tokens=8,478,720
    throughput/device/tokens_per_second=64,875
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:09.385	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:09.743	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:09.758	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:09.759	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:11.180	g3085:0	olmo.train.trainer:759	INFO	[step=465/1000]
    train/CrossEntropyLoss=10.81
    train/ZLoss=0.0138
    train/Accuracy=0.1538
    throughput/total_tokens=8,570,880
    throughput/device/tokens_per_second=64,986
    throughput/device/batches_per_second=3.526
2025-06-26 14:19:12.602	g3085:0	olmo.train.trainer:759	INFO	[step=470/1000]
    train/CrossEntropyLoss=10.72
    train/ZLoss=0.0138
    train/Accuracy=0.2045
    throughput/total_tokens=8,663,040
    throughput/device/tokens_per_second=64,913
    throughput/device/batches_per_second=3.522
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:14.027	g3085:0	olmo.train.trainer:759	INFO	[step=475/1000]
    train/CrossEntropyLoss=10.80
    train/ZLoss=0.0138
    train/Accuracy=0.1833
    throughput/total_tokens=8,755,200
    throughput/device/tokens_per_second=64,828
    throughput/device/batches_per_second=3.517
2025-06-26 14:19:15.451	g3085:0	olmo.train.trainer:759	INFO	[step=480/1000]
    train/CrossEntropyLoss=10.84
    train/ZLoss=0.0138
    train/Accuracy=0.1579
    throughput/total_tokens=8,847,360
    throughput/device/tokens_per_second=64,805
    throughput/device/batches_per_second=3.516
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:15.452	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:15.813	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:15.829	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:15.829	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:17.248	g3085:0	olmo.train.trainer:759	INFO	[step=485/1000]
    train/CrossEntropyLoss=10.87
    train/ZLoss=0.0138
    train/Accuracy=0.1429
    throughput/total_tokens=8,939,520
    throughput/device/tokens_per_second=65,129
    throughput/device/batches_per_second=3.533
2025-06-26 14:19:18.670	g3085:0	olmo.train.trainer:759	INFO	[step=490/1000]
    train/CrossEntropyLoss=10.99
    train/ZLoss=0.0138
    train/Accuracy=0.1818
    throughput/total_tokens=9,031,680
    throughput/device/tokens_per_second=64,961
    throughput/device/batches_per_second=3.524
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:20.087	g3085:0	olmo.train.trainer:759	INFO	[step=495/1000]
    train/CrossEntropyLoss=10.62
    train/ZLoss=0.0138
    train/Accuracy=0.2750
    throughput/total_tokens=9,123,840
    throughput/device/tokens_per_second=64,978
    throughput/device/batches_per_second=3.525
2025-06-26 14:19:21.504	g3085:0	olmo.train.trainer:759	INFO	[step=500/1000]
    train/CrossEntropyLoss=10.88
    train/ZLoss=0.0138
    train/Accuracy=0.1562
    throughput/total_tokens=9,216,000
    throughput/device/tokens_per_second=65,005
    throughput/device/batches_per_second=3.527
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:21.505	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:21.858	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:21.874	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:21.875	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:23.295	g3085:0	olmo.train.trainer:759	INFO	[step=505/1000]
    train/CrossEntropyLoss=10.87
    train/ZLoss=0.0138
    train/Accuracy=0.1667
    throughput/total_tokens=9,308,160
    throughput/device/tokens_per_second=65,036
    throughput/device/batches_per_second=3.528
2025-06-26 14:19:24.718	g3085:0	olmo.train.trainer:759	INFO	[step=510/1000]
    train/CrossEntropyLoss=10.71
    train/ZLoss=0.0138
    train/Accuracy=0.1957
    throughput/total_tokens=9,400,320
    throughput/device/tokens_per_second=64,904
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:26.141	g3085:0	olmo.train.trainer:759	INFO	[step=515/1000]
    train/CrossEntropyLoss=10.72
    train/ZLoss=0.0138
    train/Accuracy=0.1818
    throughput/total_tokens=9,492,480
    throughput/device/tokens_per_second=64,861
    throughput/device/batches_per_second=3.519
2025-06-26 14:19:27.562	g3085:0	olmo.train.trainer:759	INFO	[step=520/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0138
    train/Accuracy=0.1273
    throughput/total_tokens=9,584,640
    throughput/device/tokens_per_second=64,862
    throughput/device/batches_per_second=3.519
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:27.563	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:27.921	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:27.941	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:27.941	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:29.364	g3085:0	olmo.train.trainer:759	INFO	[step=525/1000]
    train/CrossEntropyLoss=10.72
    train/ZLoss=0.0138
    train/Accuracy=0.1731
    throughput/total_tokens=9,676,800
    throughput/device/tokens_per_second=64,934
    throughput/device/batches_per_second=3.523
2025-06-26 14:19:30.786	g3085:0	olmo.train.trainer:759	INFO	[step=530/1000]
    train/CrossEntropyLoss=10.81
    train/ZLoss=0.0138
    train/Accuracy=0.1429
    throughput/total_tokens=9,768,960
    throughput/device/tokens_per_second=64,886
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:32.210	g3085:0	olmo.train.trainer:759	INFO	[step=535/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0138
    train/Accuracy=0.1667
    throughput/total_tokens=9,861,120
    throughput/device/tokens_per_second=64,819
    throughput/device/batches_per_second=3.517
2025-06-26 14:19:33.632	g3085:0	olmo.train.trainer:759	INFO	[step=540/1000]
    train/CrossEntropyLoss=10.74
    train/ZLoss=0.0138
    train/Accuracy=0.1837
    throughput/total_tokens=9,953,280
    throughput/device/tokens_per_second=64,820
    throughput/device/batches_per_second=3.517
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:33.634	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:33.997	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:34.013	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:34.013	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:35.433	g3085:0	olmo.train.trainer:759	INFO	[step=545/1000]
    train/CrossEntropyLoss=10.68
    train/ZLoss=0.0138
    train/Accuracy=0.2653
    throughput/total_tokens=10,045,440
    throughput/device/tokens_per_second=65,058
    throughput/device/batches_per_second=3.530
2025-06-26 14:19:36.858	g3085:0	olmo.train.trainer:759	INFO	[step=550/1000]
    train/CrossEntropyLoss=10.69
    train/ZLoss=0.0138
    train/Accuracy=0.1837
    throughput/total_tokens=10,137,600
    throughput/device/tokens_per_second=64,883
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:38.278	g3085:0	olmo.train.trainer:759	INFO	[step=555/1000]
    train/CrossEntropyLoss=10.61
    train/ZLoss=0.0137
    train/Accuracy=0.2041
    throughput/total_tokens=10,229,760
    throughput/device/tokens_per_second=64,877
    throughput/device/batches_per_second=3.520
2025-06-26 14:19:39.696	g3085:0	olmo.train.trainer:759	INFO	[step=560/1000]
    train/CrossEntropyLoss=10.80
    train/ZLoss=0.0137
    train/Accuracy=0.1778
    throughput/total_tokens=10,321,920
    throughput/device/tokens_per_second=64,910
    throughput/device/batches_per_second=3.522
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:39.698	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:40.054	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:40.069	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:40.070	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:41.495	g3085:0	olmo.train.trainer:759	INFO	[step=565/1000]
    train/CrossEntropyLoss=10.57
    train/ZLoss=0.0137
    train/Accuracy=0.2821
    throughput/total_tokens=10,414,080
    throughput/device/tokens_per_second=64,828
    throughput/device/batches_per_second=3.517
2025-06-26 14:19:42.919	g3085:0	olmo.train.trainer:759	INFO	[step=570/1000]
    train/CrossEntropyLoss=10.71
    train/ZLoss=0.0137
    train/Accuracy=0.1628
    throughput/total_tokens=10,506,240
    throughput/device/tokens_per_second=64,780
    throughput/device/batches_per_second=3.515
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:44.344	g3085:0	olmo.train.trainer:759	INFO	[step=575/1000]
    train/CrossEntropyLoss=10.92
    train/ZLoss=0.0137
    train/Accuracy=0.1556
    throughput/total_tokens=10,598,400
    throughput/device/tokens_per_second=64,733
    throughput/device/batches_per_second=3.512
2025-06-26 14:19:45.765	g3085:0	olmo.train.trainer:759	INFO	[step=580/1000]
    train/CrossEntropyLoss=10.64
    train/ZLoss=0.0137
    train/Accuracy=0.2632
    throughput/total_tokens=10,690,560
    throughput/device/tokens_per_second=64,764
    throughput/device/batches_per_second=3.514
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:45.767	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:46.122	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:46.138	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:46.139	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:47.564	g3085:0	olmo.train.trainer:759	INFO	[step=585/1000]
    train/CrossEntropyLoss=10.93
    train/ZLoss=0.0137
    train/Accuracy=0.2500
    throughput/total_tokens=10,782,720
    throughput/device/tokens_per_second=64,803
    throughput/device/batches_per_second=3.516
2025-06-26 14:19:48.988	g3085:0	olmo.train.trainer:759	INFO	[step=590/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0137
    train/Accuracy=0.2353
    throughput/total_tokens=10,874,880
    throughput/device/tokens_per_second=64,769
    throughput/device/batches_per_second=3.514
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:50.420	g3085:0	olmo.train.trainer:759	INFO	[step=595/1000]
    train/CrossEntropyLoss=10.57
    train/ZLoss=0.0137
    train/Accuracy=0.3250
    throughput/total_tokens=10,967,040
    throughput/device/tokens_per_second=64,627
    throughput/device/batches_per_second=3.506
2025-06-26 14:19:51.837	g3085:0	olmo.train.trainer:759	INFO	[step=600/1000]
    train/CrossEntropyLoss=10.59
    train/ZLoss=0.0137
    train/Accuracy=0.2553
    throughput/total_tokens=11,059,200
    throughput/device/tokens_per_second=64,728
    throughput/device/batches_per_second=3.512
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:51.839	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:52.197	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:52.213	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:52.213	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:53.638	g3085:0	olmo.train.trainer:759	INFO	[step=605/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0137
    train/Accuracy=0.2619
    throughput/total_tokens=11,151,360
    throughput/device/tokens_per_second=64,840
    throughput/device/batches_per_second=3.518
2025-06-26 14:19:55.064	g3085:0	olmo.train.trainer:759	INFO	[step=610/1000]
    train/CrossEntropyLoss=10.68
    train/ZLoss=0.0137
    train/Accuracy=0.2826
    throughput/total_tokens=11,243,520
    throughput/device/tokens_per_second=64,731
    throughput/device/batches_per_second=3.512
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:56.485	g3085:0	olmo.train.trainer:759	INFO	[step=615/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0137
    train/Accuracy=0.2857
    throughput/total_tokens=11,335,680
    throughput/device/tokens_per_second=64,776
    throughput/device/batches_per_second=3.514
2025-06-26 14:19:57.912	g3085:0	olmo.train.trainer:759	INFO	[step=620/1000]
    train/CrossEntropyLoss=10.59
    train/ZLoss=0.0137
    train/Accuracy=0.2500
    throughput/total_tokens=11,427,840
    throughput/device/tokens_per_second=64,731
    throughput/device/batches_per_second=3.512
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:19:57.913	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:19:58.268	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:19:58.284	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:19:58.284	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:19:59.708	g3085:0	olmo.train.trainer:759	INFO	[step=625/1000]
    train/CrossEntropyLoss=10.73
    train/ZLoss=0.0137
    train/Accuracy=0.2273
    throughput/total_tokens=11,520,000
    throughput/device/tokens_per_second=64,860
    throughput/device/batches_per_second=3.519
2025-06-26 14:20:01.131	g3085:0	olmo.train.trainer:759	INFO	[step=630/1000]
    train/CrossEntropyLoss=10.52
    train/ZLoss=0.0137
    train/Accuracy=0.2857
    throughput/total_tokens=11,612,160
    throughput/device/tokens_per_second=64,840
    throughput/device/batches_per_second=3.518
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:02.556	g3085:0	olmo.train.trainer:759	INFO	[step=635/1000]
    train/CrossEntropyLoss=10.54
    train/ZLoss=0.0137
    train/Accuracy=0.2353
    throughput/total_tokens=11,704,320
    throughput/device/tokens_per_second=64,768
    throughput/device/batches_per_second=3.514
2025-06-26 14:20:03.979	g3085:0	olmo.train.trainer:759	INFO	[step=640/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0137
    train/Accuracy=0.3256
    throughput/total_tokens=11,796,480
    throughput/device/tokens_per_second=64,774
    throughput/device/batches_per_second=3.514
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:03.981	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:04.337	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:04.353	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:04.354	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:20:05.786	g3085:0	olmo.train.trainer:759	INFO	[step=645/1000]
    train/CrossEntropyLoss=10.52
    train/ZLoss=0.0137
    train/Accuracy=0.2553
    throughput/total_tokens=11,888,640
    throughput/device/tokens_per_second=64,519
    throughput/device/batches_per_second=3.500
2025-06-26 14:20:07.212	g3085:0	olmo.train.trainer:759	INFO	[step=650/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0137
    train/Accuracy=0.2157
    throughput/total_tokens=11,980,800
    throughput/device/tokens_per_second=64,561
    throughput/device/batches_per_second=3.503
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:08.638	g3085:0	olmo.train.trainer:759	INFO	[step=655/1000]
    train/CrossEntropyLoss=10.65
    train/ZLoss=0.0137
    train/Accuracy=0.2647
    throughput/total_tokens=12,072,960
    throughput/device/tokens_per_second=64,589
    throughput/device/batches_per_second=3.504
2025-06-26 14:20:10.061	g3085:0	olmo.train.trainer:759	INFO	[step=660/1000]
    train/CrossEntropyLoss=10.59
    train/ZLoss=0.0137
    train/Accuracy=0.2826
    throughput/total_tokens=12,165,120
    throughput/device/tokens_per_second=64,637
    throughput/device/batches_per_second=3.507
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:10.063	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:10.418	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:10.434	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:10.435	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:20:11.862	g3085:0	olmo.train.trainer:759	INFO	[step=665/1000]
    train/CrossEntropyLoss=10.47
    train/ZLoss=0.0137
    train/Accuracy=0.2432
    throughput/total_tokens=12,257,280
    throughput/device/tokens_per_second=64,747
    throughput/device/batches_per_second=3.513
2025-06-26 14:20:13.280	g3085:0	olmo.train.trainer:759	INFO	[step=670/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0137
    train/Accuracy=0.2500
    throughput/total_tokens=12,349,440
    throughput/device/tokens_per_second=64,878
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:14.690	g3085:0	olmo.train.trainer:759	INFO	[step=675/1000]
    train/CrossEntropyLoss=10.57
    train/ZLoss=0.0137
    train/Accuracy=0.2500
    throughput/total_tokens=12,441,600
    throughput/device/tokens_per_second=65,018
    throughput/device/batches_per_second=3.528
2025-06-26 14:20:16.088	g3085:0	olmo.train.trainer:759	INFO	[step=680/1000]
    train/CrossEntropyLoss=10.40
    train/ZLoss=0.0137
    train/Accuracy=0.3256
    throughput/total_tokens=12,533,760
    throughput/device/tokens_per_second=65,248
    throughput/device/batches_per_second=3.540
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:16.090	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:16.420	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:16.435	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:16.436	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.3 seconds
2025-06-26 14:20:17.834	g3085:0	olmo.train.trainer:759	INFO	[step=685/1000]
    train/CrossEntropyLoss=10.45
    train/ZLoss=0.0136
    train/Accuracy=0.2857
    throughput/total_tokens=12,625,920
    throughput/device/tokens_per_second=66,027
    throughput/device/batches_per_second=3.582
2025-06-26 14:20:19.233	g3085:0	olmo.train.trainer:759	INFO	[step=690/1000]
    train/CrossEntropyLoss=10.78
    train/ZLoss=0.0136
    train/Accuracy=0.2500
    throughput/total_tokens=12,718,080
    throughput/device/tokens_per_second=65,964
    throughput/device/batches_per_second=3.579
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:20.632	g3085:0	olmo.train.trainer:759	INFO	[step=695/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0136
    train/Accuracy=0.2821
    throughput/total_tokens=12,810,240
    throughput/device/tokens_per_second=65,917
    throughput/device/batches_per_second=3.576
2025-06-26 14:20:22.033	g3085:0	olmo.train.trainer:759	INFO	[step=700/1000]
    train/CrossEntropyLoss=10.46
    train/ZLoss=0.0136
    train/Accuracy=0.3529
    throughput/total_tokens=12,902,400
    throughput/device/tokens_per_second=65,888
    throughput/device/batches_per_second=3.575
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:22.035	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:22.366	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:22.382	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:22.382	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.3 seconds
2025-06-26 14:20:23.777	g3085:0	olmo.train.trainer:759	INFO	[step=705/1000]
    train/CrossEntropyLoss=10.68
    train/ZLoss=0.0136
    train/Accuracy=0.2041
    throughput/total_tokens=12,994,560
    throughput/device/tokens_per_second=66,193
    throughput/device/batches_per_second=3.591
2025-06-26 14:20:25.177	g3085:0	olmo.train.trainer:759	INFO	[step=710/1000]
    train/CrossEntropyLoss=10.51
    train/ZLoss=0.0136
    train/Accuracy=0.3256
    throughput/total_tokens=13,086,720
    throughput/device/tokens_per_second=66,006
    throughput/device/batches_per_second=3.581
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:26.576	g3085:0	olmo.train.trainer:759	INFO	[step=715/1000]
    train/CrossEntropyLoss=10.63
    train/ZLoss=0.0136
    train/Accuracy=0.2791
    throughput/total_tokens=13,178,880
    throughput/device/tokens_per_second=65,966
    throughput/device/batches_per_second=3.579
2025-06-26 14:20:27.975	g3085:0	olmo.train.trainer:759	INFO	[step=720/1000]
    train/CrossEntropyLoss=10.72
    train/ZLoss=0.0136
    train/Accuracy=0.2174
    throughput/total_tokens=13,271,040
    throughput/device/tokens_per_second=65,945
    throughput/device/batches_per_second=3.578
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:27.976	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:28.309	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:28.324	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:28.324	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.3 seconds
2025-06-26 14:20:29.727	g3085:0	olmo.train.trainer:759	INFO	[step=725/1000]
    train/CrossEntropyLoss=10.53
    train/ZLoss=0.0136
    train/Accuracy=0.2500
    throughput/total_tokens=13,363,200
    throughput/device/tokens_per_second=65,808
    throughput/device/batches_per_second=3.570
2025-06-26 14:20:31.127	g3085:0	olmo.train.trainer:759	INFO	[step=730/1000]
    train/CrossEntropyLoss=10.60
    train/ZLoss=0.0136
    train/Accuracy=0.2667
    throughput/total_tokens=13,455,360
    throughput/device/tokens_per_second=65,834
    throughput/device/batches_per_second=3.572
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:32.529	g3085:0	olmo.train.trainer:759	INFO	[step=735/1000]
    train/CrossEntropyLoss=10.56
    train/ZLoss=0.0136
    train/Accuracy=0.2449
    throughput/total_tokens=13,547,520
    throughput/device/tokens_per_second=65,795
    throughput/device/batches_per_second=3.570
2025-06-26 14:20:33.925	g3085:0	olmo.train.trainer:759	INFO	[step=740/1000]
    train/CrossEntropyLoss=10.95
    train/ZLoss=0.0136
    train/Accuracy=0.2069
    throughput/total_tokens=13,639,680
    throughput/device/tokens_per_second=65,851
    throughput/device/batches_per_second=3.573
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:33.927	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:34.259	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:34.274	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:34.274	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.3 seconds
2025-06-26 14:20:35.679	g3085:0	olmo.train.trainer:759	INFO	[step=745/1000]
    train/CrossEntropyLoss=10.52
    train/ZLoss=0.0136
    train/Accuracy=0.2500
    throughput/total_tokens=13,731,840
    throughput/device/tokens_per_second=65,718
    throughput/device/batches_per_second=3.565
2025-06-26 14:20:37.079	g3085:0	olmo.train.trainer:759	INFO	[step=750/1000]
    train/CrossEntropyLoss=10.45
    train/ZLoss=0.0136
    train/Accuracy=0.2917
    throughput/total_tokens=13,824,000
    throughput/device/tokens_per_second=65,801
    throughput/device/batches_per_second=3.570
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:38.476	g3085:0	olmo.train.trainer:759	INFO	[step=755/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0136
    train/Accuracy=0.2821
    throughput/total_tokens=13,916,160
    throughput/device/tokens_per_second=65,844
    throughput/device/batches_per_second=3.572
2025-06-26 14:20:39.879	g3085:0	olmo.train.trainer:759	INFO	[step=760/1000]
    train/CrossEntropyLoss=10.68
    train/ZLoss=0.0136
    train/Accuracy=0.2692
    throughput/total_tokens=14,008,320
    throughput/device/tokens_per_second=65,814
    throughput/device/batches_per_second=3.571
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:39.880	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:40.212	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:40.227	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:40.228	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.3 seconds
2025-06-26 14:20:41.630	g3085:0	olmo.train.trainer:759	INFO	[step=765/1000]
    train/CrossEntropyLoss=10.60
    train/ZLoss=0.0136
    train/Accuracy=0.2791
    throughput/total_tokens=14,100,480
    throughput/device/tokens_per_second=65,838
    throughput/device/batches_per_second=3.572
2025-06-26 14:20:43.026	g3085:0	olmo.train.trainer:759	INFO	[step=770/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0136
    train/Accuracy=0.2340
    throughput/total_tokens=14,192,640
    throughput/device/tokens_per_second=65,933
    throughput/device/batches_per_second=3.577
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:44.442	g3085:0	olmo.train.trainer:759	INFO	[step=775/1000]
    train/CrossEntropyLoss=10.41
    train/ZLoss=0.0136
    train/Accuracy=0.3023
    throughput/total_tokens=14,284,800
    throughput/device/tokens_per_second=65,642
    throughput/device/batches_per_second=3.561
2025-06-26 14:20:45.854	g3085:0	olmo.train.trainer:759	INFO	[step=780/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0136
    train/Accuracy=0.3171
    throughput/total_tokens=14,376,960
    throughput/device/tokens_per_second=65,554
    throughput/device/batches_per_second=3.557
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:45.855	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:46.210	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:46.226	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:46.227	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:20:47.647	g3085:0	olmo.train.trainer:759	INFO	[step=785/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0136
    train/Accuracy=0.2826
    throughput/total_tokens=14,469,120
    throughput/device/tokens_per_second=65,064
    throughput/device/batches_per_second=3.530
2025-06-26 14:20:49.066	g3085:0	olmo.train.trainer:759	INFO	[step=790/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0136
    train/Accuracy=0.2642
    throughput/total_tokens=14,561,280
    throughput/device/tokens_per_second=65,000
    throughput/device/batches_per_second=3.526
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:50.484	g3085:0	olmo.train.trainer:759	INFO	[step=795/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0136
    train/Accuracy=0.3500
    throughput/total_tokens=14,653,440
    throughput/device/tokens_per_second=64,993
    throughput/device/batches_per_second=3.526
2025-06-26 14:20:51.906	g3085:0	olmo.train.trainer:759	INFO	[step=800/1000]
    train/CrossEntropyLoss=10.54
    train/ZLoss=0.0136
    train/Accuracy=0.2927
    throughput/total_tokens=14,745,600
    throughput/device/tokens_per_second=64,949
    throughput/device/batches_per_second=3.524
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:51.908	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:52.263	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:52.279	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:52.279	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:20:53.697	g3085:0	olmo.train.trainer:759	INFO	[step=805/1000]
    train/CrossEntropyLoss=10.40
    train/ZLoss=0.0136
    train/Accuracy=0.3404
    throughput/total_tokens=14,837,760
    throughput/device/tokens_per_second=65,150
    throughput/device/batches_per_second=3.535
2025-06-26 14:20:55.115	g3085:0	olmo.train.trainer:759	INFO	[step=810/1000]
    train/CrossEntropyLoss=10.54
    train/ZLoss=0.0136
    train/Accuracy=0.2459
    throughput/total_tokens=14,929,920
    throughput/device/tokens_per_second=65,084
    throughput/device/batches_per_second=3.531
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:56.543	g3085:0	olmo.train.trainer:759	INFO	[step=815/1000]
    train/CrossEntropyLoss=10.39
    train/ZLoss=0.0136
    train/Accuracy=0.2800
    throughput/total_tokens=15,022,080
    throughput/device/tokens_per_second=64,886
    throughput/device/batches_per_second=3.520
2025-06-26 14:20:57.965	g3085:0	olmo.train.trainer:759	INFO	[step=820/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0136
    train/Accuracy=0.3171
    throughput/total_tokens=15,114,240
    throughput/device/tokens_per_second=64,879
    throughput/device/batches_per_second=3.520
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:20:57.966	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:20:58.320	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:20:58.336	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:20:58.337	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:20:59.765	g3085:0	olmo.train.trainer:759	INFO	[step=825/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0136
    train/Accuracy=0.2000
    throughput/total_tokens=15,206,400
    throughput/device/tokens_per_second=64,663
    throughput/device/batches_per_second=3.508
2025-06-26 14:21:01.183	g3085:0	olmo.train.trainer:759	INFO	[step=830/1000]
    train/CrossEntropyLoss=10.50
    train/ZLoss=0.0136
    train/Accuracy=0.2667
    throughput/total_tokens=15,298,560
    throughput/device/tokens_per_second=64,834
    throughput/device/batches_per_second=3.517
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:02.601	g3085:0	olmo.train.trainer:759	INFO	[step=835/1000]
    train/CrossEntropyLoss=10.65
    train/ZLoss=0.0136
    train/Accuracy=0.2553
    throughput/total_tokens=15,390,720
    throughput/device/tokens_per_second=64,886
    throughput/device/batches_per_second=3.520
2025-06-26 14:21:04.020	g3085:0	olmo.train.trainer:759	INFO	[step=840/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0136
    train/Accuracy=0.3409
    throughput/total_tokens=15,482,880
    throughput/device/tokens_per_second=64,901
    throughput/device/batches_per_second=3.521
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:04.022	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:04.376	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:04.392	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:04.393	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:05.809	g3085:0	olmo.train.trainer:759	INFO	[step=845/1000]
    train/CrossEntropyLoss=10.40
    train/ZLoss=0.0136
    train/Accuracy=0.3333
    throughput/total_tokens=15,575,040
    throughput/device/tokens_per_second=65,202
    throughput/device/batches_per_second=3.537
2025-06-26 14:21:07.228	g3085:0	olmo.train.trainer:759	INFO	[step=850/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0136
    train/Accuracy=0.3043
    throughput/total_tokens=15,667,200
    throughput/device/tokens_per_second=65,085
    throughput/device/batches_per_second=3.531
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:08.649	g3085:0	olmo.train.trainer:759	INFO	[step=855/1000]
    train/CrossEntropyLoss=10.37
    train/ZLoss=0.0136
    train/Accuracy=0.3750
    throughput/total_tokens=15,759,360
    throughput/device/tokens_per_second=65,003
    throughput/device/batches_per_second=3.527
2025-06-26 14:21:10.065	g3085:0	olmo.train.trainer:759	INFO	[step=860/1000]
    train/CrossEntropyLoss=10.42
    train/ZLoss=0.0136
    train/Accuracy=0.3111
    throughput/total_tokens=15,851,520
    throughput/device/tokens_per_second=65,026
    throughput/device/batches_per_second=3.528
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:10.067	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:10.423	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:10.438	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:10.439	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:11.858	g3085:0	olmo.train.trainer:759	INFO	[step=865/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0136
    train/Accuracy=0.3125
    throughput/total_tokens=15,943,680
    throughput/device/tokens_per_second=65,093
    throughput/device/batches_per_second=3.532
2025-06-26 14:21:13.274	g3085:0	olmo.train.trainer:759	INFO	[step=870/1000]
    train/CrossEntropyLoss=10.47
    train/ZLoss=0.0136
    train/Accuracy=0.2791
    throughput/total_tokens=16,035,840
    throughput/device/tokens_per_second=65,087
    throughput/device/batches_per_second=3.531
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:14.699	g3085:0	olmo.train.trainer:759	INFO	[step=875/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0136
    train/Accuracy=0.3056
    throughput/total_tokens=16,128,000
    throughput/device/tokens_per_second=64,942
    throughput/device/batches_per_second=3.523
2025-06-26 14:21:16.118	g3085:0	olmo.train.trainer:759	INFO	[step=880/1000]
    train/CrossEntropyLoss=10.51
    train/ZLoss=0.0136
    train/Accuracy=0.2444
    throughput/total_tokens=16,220,160
    throughput/device/tokens_per_second=64,947
    throughput/device/batches_per_second=3.524
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:16.120	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:16.474	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:16.490	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:16.490	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:17.911	g3085:0	olmo.train.trainer:759	INFO	[step=885/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0136
    train/Accuracy=0.2889
    throughput/total_tokens=16,312,320
    throughput/device/tokens_per_second=65,004
    throughput/device/batches_per_second=3.527
2025-06-26 14:21:19.331	g3085:0	olmo.train.trainer:759	INFO	[step=890/1000]
    train/CrossEntropyLoss=10.64
    train/ZLoss=0.0136
    train/Accuracy=0.2500
    throughput/total_tokens=16,404,480
    throughput/device/tokens_per_second=64,977
    throughput/device/batches_per_second=3.525
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:20.748	g3085:0	olmo.train.trainer:759	INFO	[step=895/1000]
    train/CrossEntropyLoss=10.61
    train/ZLoss=0.0136
    train/Accuracy=0.2456
    throughput/total_tokens=16,496,640
    throughput/device/tokens_per_second=64,989
    throughput/device/batches_per_second=3.526
2025-06-26 14:21:22.164	g3085:0	olmo.train.trainer:759	INFO	[step=900/1000]
    train/CrossEntropyLoss=10.32
    train/ZLoss=0.0136
    train/Accuracy=0.3721
    throughput/total_tokens=16,588,800
    throughput/device/tokens_per_second=65,019
    throughput/device/batches_per_second=3.528
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:22.165	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:22.523	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:22.539	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:22.539	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:23.957	g3085:0	olmo.train.trainer:759	INFO	[step=905/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0136
    train/Accuracy=0.3261
    throughput/total_tokens=16,680,960
    throughput/device/tokens_per_second=65,120
    throughput/device/batches_per_second=3.533
2025-06-26 14:21:25.377	g3085:0	olmo.train.trainer:759	INFO	[step=910/1000]
    train/CrossEntropyLoss=10.45
    train/ZLoss=0.0136
    train/Accuracy=0.3261
    throughput/total_tokens=16,773,120
    throughput/device/tokens_per_second=65,020
    throughput/device/batches_per_second=3.528
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:26.793	g3085:0	olmo.train.trainer:759	INFO	[step=915/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0136
    train/Accuracy=0.2895
    throughput/total_tokens=16,865,280
    throughput/device/tokens_per_second=65,043
    throughput/device/batches_per_second=3.529
2025-06-26 14:21:28.211	g3085:0	olmo.train.trainer:759	INFO	[step=920/1000]
    train/CrossEntropyLoss=10.41
    train/ZLoss=0.0136
    train/Accuracy=0.3333
    throughput/total_tokens=16,957,440
    throughput/device/tokens_per_second=65,033
    throughput/device/batches_per_second=3.528
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:28.213	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:28.568	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:28.584	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:28.584	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:30.001	g3085:0	olmo.train.trainer:759	INFO	[step=925/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0136
    train/Accuracy=0.2857
    throughput/total_tokens=17,049,600
    throughput/device/tokens_per_second=65,180
    throughput/device/batches_per_second=3.536
2025-06-26 14:21:31.418	g3085:0	olmo.train.trainer:759	INFO	[step=930/1000]
    train/CrossEntropyLoss=10.33
    train/ZLoss=0.0136
    train/Accuracy=0.3488
    throughput/total_tokens=17,141,760
    throughput/device/tokens_per_second=65,132
    throughput/device/batches_per_second=3.534
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:32.837	g3085:0	olmo.train.trainer:759	INFO	[step=935/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0136
    train/Accuracy=0.3158
    throughput/total_tokens=17,233,920
    throughput/device/tokens_per_second=65,061
    throughput/device/batches_per_second=3.530
2025-06-26 14:21:34.256	g3085:0	olmo.train.trainer:759	INFO	[step=940/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0136
    train/Accuracy=0.2750
    throughput/total_tokens=17,326,080
    throughput/device/tokens_per_second=65,040
    throughput/device/batches_per_second=3.529
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:34.257	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:34.611	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:34.627	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:34.627	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:36.040	g3085:0	olmo.train.trainer:759	INFO	[step=945/1000]
    train/CrossEntropyLoss=10.50
    train/ZLoss=0.0136
    train/Accuracy=0.2449
    throughput/total_tokens=17,418,240
    throughput/device/tokens_per_second=65,403
    throughput/device/batches_per_second=3.548
2025-06-26 14:21:37.460	g3085:0	olmo.train.trainer:759	INFO	[step=950/1000]
    train/CrossEntropyLoss=10.39
    train/ZLoss=0.0136
    train/Accuracy=0.2889
    throughput/total_tokens=17,510,400
    throughput/device/tokens_per_second=65,151
    throughput/device/batches_per_second=3.535
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:38.872	g3085:0	olmo.train.trainer:759	INFO	[step=955/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0136
    train/Accuracy=0.2619
    throughput/total_tokens=17,602,560
    throughput/device/tokens_per_second=65,178
    throughput/device/batches_per_second=3.536
2025-06-26 14:21:40.290	g3085:0	olmo.train.trainer:759	INFO	[step=960/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0136
    train/Accuracy=0.3261
    throughput/total_tokens=17,694,720
    throughput/device/tokens_per_second=65,143
    throughput/device/batches_per_second=3.534
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:40.291	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:40.646	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:40.663	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:40.663	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:42.083	g3085:0	olmo.train.trainer:759	INFO	[step=965/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0136
    train/Accuracy=0.2791
    throughput/total_tokens=17,786,880
    throughput/device/tokens_per_second=65,053
    throughput/device/batches_per_second=3.529
2025-06-26 14:21:43.502	g3085:0	olmo.train.trainer:759	INFO	[step=970/1000]
    train/CrossEntropyLoss=10.46
    train/ZLoss=0.0136
    train/Accuracy=0.3056
    throughput/total_tokens=17,879,040
    throughput/device/tokens_per_second=64,997
    throughput/device/batches_per_second=3.526
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:44.936	g3085:0	olmo.train.trainer:759	INFO	[step=975/1000]
    train/CrossEntropyLoss=10.40
    train/ZLoss=0.0136
    train/Accuracy=0.2889
    throughput/total_tokens=17,971,200
    throughput/device/tokens_per_second=64,756
    throughput/device/batches_per_second=3.513
2025-06-26 14:21:46.352	g3085:0	olmo.train.trainer:759	INFO	[step=980/1000]
    train/CrossEntropyLoss=10.40
    train/ZLoss=0.0136
    train/Accuracy=0.3191
    throughput/total_tokens=18,063,360
    throughput/device/tokens_per_second=64,843
    throughput/device/batches_per_second=3.518
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:46.353	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:46.706	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:46.722	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:46.723	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:48.139	g3085:0	olmo.train.trainer:759	INFO	[step=985/1000]
    train/CrossEntropyLoss=10.77
    train/ZLoss=0.0136
    train/Accuracy=0.2407
    throughput/total_tokens=18,155,520
    throughput/device/tokens_per_second=65,198
    throughput/device/batches_per_second=3.537
2025-06-26 14:21:49.555	g3085:0	olmo.train.trainer:759	INFO	[step=990/1000]
    train/CrossEntropyLoss=10.37
    train/ZLoss=0.0136
    train/Accuracy=0.3514
    throughput/total_tokens=18,247,680
    throughput/device/tokens_per_second=65,148
    throughput/device/batches_per_second=3.535
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:50.972	g3085:0	olmo.train.trainer:759	INFO	[step=995/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0136
    train/Accuracy=0.2857
    throughput/total_tokens=18,339,840
    throughput/device/tokens_per_second=65,111
    throughput/device/batches_per_second=3.533
2025-06-26 14:21:52.390	g3085:0	olmo.train.trainer:759	INFO	[step=1000/1000]
    train/CrossEntropyLoss=10.52
    train/ZLoss=0.0136
    train/Accuracy=0.2955
    throughput/total_tokens=18,432,000
    throughput/device/tokens_per_second=65,090
    throughput/device/batches_per_second=3.531
    System/Peak GPU Memory (MB)=28,934
2025-06-26 14:21:52.391	g3085:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-26 14:21:52.745	g3085:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-26 14:21:52.760	g3085:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-26 14:21:52.761	g3085:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-26 14:21:54.263	g3085:0	olmo.train.trainer:1135	INFO	Saving final checkpoint...
2025-06-26 14:21:56.064	g3085:0	olmo.train.trainer:1137	INFO	Checkpoint saved to debug_run/step1000
2025-06-26 14:21:56.064	g3085:0	train:332	INFO	Training complete
2025-06-27 10:13:05.720	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 10:13:05.862	g3085:0	train:54	INFO	Configuration:
2025-06-27 10:13:05.863	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 10:13:05.892	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 10:16:49.864	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 131, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 164, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 114, in build_tokenizer
    assert len(tokenizer) <= pad_tokenizer_to
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 114, in build_tokenizer
    assert len(tokenizer) <= pad_tokenizer_to
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 10:20:10.480	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 10:20:10.583	g3085:0	train:54	INFO	Configuration:
2025-06-27 10:20:10.584	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 10:20:10.613	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 10:21:51.680	g3085:0	root:121	INFO	Padding tokenizer with 418 tokens
2025-06-27 10:26:06.543	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 131, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 164, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 136, in build_tokenizer
    if pad_tokenizer_to is not None:
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 136, in build_tokenizer
    if pad_tokenizer_to is not None:
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 10:26:33.373	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 10:26:33.442	g3085:0	train:54	INFO	Configuration:
2025-06-27 10:26:33.442	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 10:26:33.484	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 10:26:35.624	g3085:0	root:122	INFO	Padding tokenizer with 418 tokens
2025-06-27 10:41:19.714	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 131, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 165, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 142, in build_tokenizer
    tok = HfTokenizerWrapper(tokenizer, bos_token_id=bos_token_id, adds_space=False)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 58, in __init__
    import pdb; pdb.set_trace();
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 10:56:10.096	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 10:56:10.167	g3085:0	train:54	INFO	Configuration:
2025-06-27 10:56:10.167	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 10:56:10.213	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 11:01:45.925	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 127, in __init__
    self.transformer: Llm = self.config.llm.build(self.__cache, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 572, in build
    return Llm(self, cache, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 626, in __init__
    self.activation_checkpointing_fn = None
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 626, in __init__
    self.activation_checkpointing_fn = None
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 11:13:11.023	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 11:13:11.092	g3085:0	train:54	INFO	Configuration:
2025-06-27 11:13:11.093	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 11:13:11.151	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 11:13:13.285	g3085:0	root:121	INFO	Padding tokenizer with 418 tokens
2025-06-27 11:13:13.773	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-27 11:26:09.586	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 142, in run_trainer
    if cfg.activation_checkpointing:
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 142, in run_trainer
    if cfg.activation_checkpointing:
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 11:35:00.276	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 11:35:00.354	g3085:0	train:54	INFO	Configuration:
2025-06-27 11:35:00.355	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 11:35:00.384	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 11:35:18.522	g3085:0	root:121	INFO	Padding tokenizer with 418 tokens
2025-06-27 11:42:45.195	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 11:46:17.142	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 11:46:17.212	g3085:0	train:54	INFO	Configuration:
2025-06-27 11:46:17.213	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 11:46:17.246	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 11:47:09.948	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 11:50:53.861	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 11:51:08.614	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 11:51:08.683	g3085:0	train:54	INFO	Configuration:
2025-06-27 11:51:08.684	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 11:51:08.726	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 11:52:32.054	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 12:38:04.114	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 147, in __init__
    import pdb;pdb.set_trace()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 12:45:54.716	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 12:45:54.797	g3085:0	train:54	INFO	Configuration:
2025-06-27 12:45:54.797	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 12:45:54.825	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 12:48:35.627	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 12:51:47.404	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 132, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 182, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 159, in build_tokenizer
    tok = HfTokenizerWrapper(tokenizer, bos_token_id=bos_token_id, adds_space=False)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 58, in __init__
    import pdb; pdb.set_trace()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 12:54:33.085	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 12:54:33.154	g3085:0	train:54	INFO	Configuration:
2025-06-27 12:54:33.155	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 12:54:33.195	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 12:54:35.352	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 12:54:35.822	g3085:0	olmo.util:177	CRITICAL	Uncaught AttributeError: 'list' object has no attribute 'items'
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 132, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 183, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 145, in build_tokenizer
    tokenizer.add_special_tokens(extra_tokens_pad)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 955, in add_special_tokens
    for key, value in special_tokens_dict.items():
AttributeError: 'list' object has no attribute 'items'
2025-06-27 12:56:53.422	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 12:56:53.491	g3085:0	train:54	INFO	Configuration:
2025-06-27 12:56:53.491	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 12:56:53.531	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 12:56:55.599	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 12:56:56.086	g3085:0	olmo.util:177	CRITICAL	Uncaught AssertionError: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 132, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 183, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 158, in build_tokenizer
    assert ids == [pad_tokenizer_to + ix]
AssertionError
2025-06-27 12:58:39.800	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 12:58:39.871	g3085:0	train:54	INFO	Configuration:
2025-06-27 12:58:39.871	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 12:58:39.899	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 12:58:42.093	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 12:59:13.884	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-27 12:59:18.326	g3085:0	train:177	INFO	Wrapping model with FSDP2...
2025-06-27 12:59:18.899	g3085:0	train:188	INFO	Total number of parameters: 57,810,304
2025-06-27 12:59:18.900	g3085:0	train:189	INFO	Number of non-embedding parameters: 38,329,728
2025-06-27 12:59:19.022	g3085:0	train:192	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 12:59:19.022	g3085:0	train:193	INFO	Model:
2025-06-27 12:59:19.023	g3085:0	train:194	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152064, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 12:59:19.032	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 12:59:19.032	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 12:59:19.033	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 12:59:19.077	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 12:59:19.180	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 12:59:19.180	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 12:59:19.182	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 12:59:19.183	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 12:59:19.183	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 12:59:19.187	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 12:59:19.191	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 12:59:19.286	g3085:0	train:311	INFO	Resuming from checkpoint debug_run/step1000
2025-06-27 13:00:01.280	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:00:01.350	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:00:01.350	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:00:01.378	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:00:03.514	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:03:38.299	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 132, in __init__
    self.special_ids = tokenizer.get_special_token_ids(self.config.build_tokenizer())
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 76, in build_tokenizer
    return self.llm.build_tokenizer()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/nn/llm.py", line 579, in build_tokenizer
    return self.tokenizer.build(pad_tokenizer_to=pad_tokenizer_to)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 183, in build
    return build_tokenizer(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 160, in build_tokenizer
    tok = HfTokenizerWrapper(tokenizer, bos_token_id=bos_token_id, adds_space=False)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/tokenizer.py", line 58, in __init__
    import pdb; pdb.set_trace()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 13:04:06.300	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:04:06.371	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:04:06.372	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:04:06.400	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:05:00.558	g3085:0	root:134	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:28:59.972	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 114, in build_model
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 147, in __init__
    def reset_parameters(self):
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 13:29:28.107	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:29:28.182	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:29:28.182	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:29:28.213	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:29:30.516	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:32:15.564	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 146, in __init__
    import pdb;pdb.set_trace()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 13:32:30.203	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:32:30.272	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:32:30.273	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=128, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:32:30.300	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:32:32.387	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:37:45.738	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:44:49.063	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 283, in <module>
    if model_cfg.llm.max_sequence_length < args.seq_len:
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 283, in <module>
    if model_cfg.llm.max_sequence_length < args.seq_len:
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 13:45:10.641	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:46:11.173	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:46:11.174	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:46:11.203	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:46:11.324	g3085:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 13:46:13.586	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:46:52.573	g3085:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 392, in <module>
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 105, in run_trainer
    olmo_model = model_cfg.build_model()
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 113, in build_model
    return Molmo(self, device)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/models/molmo/molmo.py", line 146, in __init__
    import pdb;pdb.set_trace()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 94, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 156, in dispatch_return
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 13:49:00.714	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:49:00.783	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:49:00.783	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:49:00.811	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:49:00.929	g3085:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 13:49:02.902	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:49:03.378	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-27 13:49:06.842	g3085:0	train:177	INFO	Wrapping model with FSDP2...
2025-06-27 13:49:07.288	g3085:0	train:188	INFO	Total number of parameters: 57,794,434
2025-06-27 13:49:07.288	g3085:0	train:189	INFO	Number of non-embedding parameters: 38,329,986
2025-06-27 13:49:07.328	g3085:0	train:192	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 13:49:07.329	g3085:0	train:193	INFO	Model:
2025-06-27 13:49:07.329	g3085:0	train:194	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152066, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 13:49:07.339	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 13:49:07.339	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 13:49:07.340	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 13:49:07.392	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 13:49:07.519	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 13:49:07.519	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 13:49:07.521	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 13:49:07.522	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 13:49:07.522	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 13:49:07.526	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 13:49:07.529	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 13:49:07.683	g3085:0	train:311	INFO	Resuming from checkpoint debug_run/step1000
2025-06-27 13:49:08.597	g3085:0	olmo.util:177	CRITICAL	Uncaught CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 315, in run_trainer
    trainer.restore_checkpoint(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 525, in restore_checkpoint
    trainer_state = self.checkpointer.load(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/checkpointer.py", line 237, in load
    load_model_and_optim_state(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/distributed_checkpointing.py", line 267, in load_model_and_optim_state
    dist_cp.load(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 465, in inner_func
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 177, in load
    _load_state_dict(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 234, in _load_state_dict
    central_plan: LoadPlan = distW.reduce_scatter("plan", local_step, global_step)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 219, in reduce_scatter
    raise result
torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding

2025-06-27 13:55:18.506	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:55:18.624	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:55:18.625	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:55:18.653	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:55:18.772	g3085:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 13:55:21.052	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:55:21.578	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-27 13:55:23.451	g3085:0	train:177	INFO	Wrapping model with FSDP2...
2025-06-27 13:55:23.632	g3085:0	train:188	INFO	Total number of parameters: 57,794,434
2025-06-27 13:55:23.633	g3085:0	train:189	INFO	Number of non-embedding parameters: 38,329,986
2025-06-27 13:55:23.634	g3085:0	train:192	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 13:55:23.634	g3085:0	train:193	INFO	Model:
2025-06-27 13:55:23.635	g3085:0	train:194	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152066, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 13:55:23.645	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 13:55:23.645	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 13:55:23.646	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 13:55:23.695	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 13:55:23.831	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 13:55:23.832	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 13:55:23.834	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 13:55:23.834	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 13:55:23.835	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 13:55:23.838	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 13:55:23.842	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 13:55:23.847	g3085:0	train:311	INFO	Resuming from checkpoint debug_run/step1000
2025-06-27 13:55:24.399	g3085:0	olmo.util:177	CRITICAL	Uncaught CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 315, in run_trainer
    trainer.restore_checkpoint(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 525, in restore_checkpoint
    trainer_state = self.checkpointer.load(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/checkpointer.py", line 237, in load
    load_model_and_optim_state(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/distributed_checkpointing.py", line 267, in load_model_and_optim_state
    dist_cp.load(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 465, in inner_func
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 177, in load
    _load_state_dict(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 234, in _load_state_dict
    central_plan: LoadPlan = distW.reduce_scatter("plan", local_step, global_step)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 219, in reduce_scatter
    raise result
torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding

2025-06-27 13:56:32.313	g3085:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 13:56:32.402	g3085:0	train:54	INFO	Configuration:
2025-06-27 13:56:32.403	g3085:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 13:56:32.450	g3085:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 13:56:32.570	g3085:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 13:56:34.523	g3085:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 13:56:34.992	g3085:0	train:138	INFO	Freezing LLM: wte
2025-06-27 13:56:36.347	g3085:0	train:177	INFO	Wrapping model with FSDP2...
2025-06-27 13:56:36.421	g3085:0	train:188	INFO	Total number of parameters: 57,794,434
2025-06-27 13:56:36.422	g3085:0	train:189	INFO	Number of non-embedding parameters: 38,329,986
2025-06-27 13:56:36.423	g3085:0	train:192	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 13:56:36.423	g3085:0	train:193	INFO	Model:
2025-06-27 13:56:36.423	g3085:0	train:194	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152066, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 13:56:36.433	g3085:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 13:56:36.433	g3085:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 13:56:36.434	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 13:56:36.457	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 13:56:36.537	g3085:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 13:56:36.538	g3085:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 13:56:36.539	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 13:56:36.540	g3085:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 13:56:36.540	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 13:56:36.544	g3085:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 13:56:36.547	g3085:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 13:56:36.552	g3085:0	train:311	INFO	Resuming from checkpoint debug_run/step1000
2025-06-27 13:56:37.073	g3085:0	olmo.util:177	CRITICAL	Uncaught CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 315, in run_trainer
    trainer.restore_checkpoint(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 525, in restore_checkpoint
    trainer_state = self.checkpointer.load(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/checkpointer.py", line 237, in load
    load_model_and_optim_state(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/distributed_checkpointing.py", line 267, in load_model_and_optim_state
    dist_cp.load(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 465, in inner_func
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 177, in load
    _load_state_dict(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 234, in _load_state_dict
    central_plan: LoadPlan = distW.reduce_scatter("plan", local_step, global_step)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 219, in reduce_scatter
    raise result
torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding

2025-06-27 14:14:23.498	g3082:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 14:14:23.788	g3082:0	train:54	INFO	Configuration:
2025-06-27 14:14:23.788	g3082:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 14:14:23.822	g3082:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 14:14:24.045	g3082:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 14:14:26.682	g3082:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 14:14:27.246	g3082:0	train:138	INFO	Freezing LLM: wte
2025-06-27 14:14:33.357	g3082:0	train:177	INFO	Wrapping model with FSDP2...
2025-06-27 14:14:33.675	g3082:0	train:188	INFO	Total number of parameters: 57,794,434
2025-06-27 14:14:33.676	g3082:0	train:189	INFO	Number of non-embedding parameters: 38,329,986
2025-06-27 14:14:33.677	g3082:0	train:192	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 14:14:33.677	g3082:0	train:193	INFO	Model:
2025-06-27 14:14:33.678	g3082:0	train:194	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152066, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 14:14:33.704	g3082:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 14:14:33.705	g3082:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 14:14:33.706	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 14:14:33.760	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 14:14:33.897	g3082:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 14:14:33.897	g3082:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 14:14:33.922	g3082:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 14:14:33.923	g3082:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 14:14:33.923	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 14:14:33.955	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 14:14:33.976	g3082:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 14:14:33.982	g3082:0	train:311	INFO	Resuming from checkpoint debug_run/step1000
2025-06-27 14:14:35.161	g3082:0	olmo.util:177	CRITICAL	Uncaught CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 315, in run_trainer
    trainer.restore_checkpoint(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/trainer.py", line 525, in restore_checkpoint
    trainer_state = self.checkpointer.load(
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/checkpointer.py", line 237, in load
    load_model_and_optim_state(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/Umolmo/olmo/train/distributed_checkpointing.py", line 267, in load_model_and_optim_state
    dist_cp.load(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 465, in inner_func
    return func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 177, in load
    _load_state_dict(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 234, in _load_state_dict
    central_plan: LoadPlan = distW.reduce_scatter("plan", local_step, global_step)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 219, in reduce_scatter
    raise result
torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])
Traceback (most recent call last): (RANK 0)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py", line 192, in reduce_scatter
    local_data = map_fun()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py", line 223, in local_step
    local_plan = planner.create_local_plan()
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 334, in create_local_plan
    return create_default_local_load_plan(
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py", line 465, in create_default_local_load_plan
    raise ValueError(
ValueError: Size mismatch between saved torch.Size([128, 128]) and current: torch.Size([2, 128]) for model.transformer.wte.new_embedding

2025-06-27 14:30:06.166	g3082:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 14:30:06.261	g3082:0	train:54	INFO	Configuration:
2025-06-27 14:30:06.262	g3082:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 14:30:06.292	g3082:0	train:69	INFO	Resuming from debug_run/step1000
2025-06-27 14:30:06.417	g3082:0	train:73	WARNING	Model config does not match the one resuming from
2025-06-27 14:32:42.279	g3082:0	olmo.util:177	CRITICAL	Uncaught BdbQuit: 
Traceback (most recent call last):
  File "/mmfs1/gscratch/krishna/mahtab/Umolmo/launch_scripts/train_multitask_model.py", line 391, in <module>
    run_trainer(cfg)
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 84, in run_trainer
    if start_from is None and cfg.load_path:
  File "/gscratch/krishna/mahtab/Umolmo/scripts/train.py", line 84, in run_trainer
    if start_from is None and cfg.load_path:
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/gscratch/krishna/mahtab/miniconda3/envs/molmo/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
2025-06-27 14:33:38.535	g3082:0	olmo.util:269	INFO	Set up torchrun environment
2025-06-27 14:33:38.610	g3082:0	train:54	INFO	Configuration:
2025-06-27 14:33:38.610	g3082:0	train:55	INFO	TrainConfig(run_name='multitask_train', model=MolmoConfig(model_name='molmo', llm=LlmConfig(d_model=128, n_heads=2, n_kv_heads=None, head_dim=None, qkv_bias=False, clip_qkv=None, n_layers=1, mlp_ratio=4, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', rope=True, rope_full_precision=True, rope_theta=10000.0, rope_type='default', rope_factor=None, rope_high_freq_factor=None, rope_low_freq_factor=None, rope_original_max_position_embeddings=None, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, attention_layer_norm_type='olmo', residual_dropout=0.1, response_residual_dropout=0.0, layer_norm_type='default', layer_norm_with_affine=True, layer_norm_eps=None, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=True, bias_for_layer_norm=None, norm_after=False, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25, embedding_dropout=0.1, scale_logits=False, vocab_size=152064, additional_vocab_size=2, weight_tying=False, embedding_size=None, use_position_ids=True, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), init_path=None, init_incremental=None, new_embedding_init_range=0.02, initializer_range=0.02, normalize_input_embeds=False, activation_checkpoint='whole_layer', compile='blocks', fix_pad_tokenizer=False, init_std=0.02, init_fn='normal', init_cutoff_factor=None), vision_backbone=MolmoVisionBackboneConfig(vit=VitConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=1, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, float32_attention=True, attention_type='sdpa', activation_checkpointing=True, init_path=None, resize_mode='default', pad_value=0.0, normalize='openai'), image_pooling_2d='attention_meanq', pooling_attention_mask=False, image_projector='mlp', image_padding_embed=None, vit_layers=(-1,), skip_unused_layers=True, image_feature_dropout=0.0, connector_activation_checkpointing=True, compile_vit='blocks'), data_formatter=DataFormatter(prompt_templates='uber_model', message_format='role', system_prompt='demo_or_style', always_start_with_space=False, default_inference_len=65, select_answer='best', debug=False, image_last=False, format_message_list=None, p_one_message=0.0), mm_preprocessor=MolmoPreprocessorConfig(crop_mode='resize', max_crops=1, max_images=None, pooling_w=2, pooling_h=2, overlap_margins=[4, 4], use_col_tokens=True, loss_token_weighting='root_subsegments', legacy_image_mask=False), bi_directional_attn=None), seed=6198, epoch=None, dry_run=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=5e-06, vit_learning_rate=5e-06, llm_learning_rate=1e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=-1), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=200, llm_t_warmup=200, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataLoaderConfig(dataset=None, mixture=None, root_size_mixture=[RootSizeMixture(rate=1.0, mixture={'chart_qa': None})], kwargs_mixture=None, split='train', seed=50189, pad='to_max', sequence_length=2304, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=20, inf_evaluators=[InfDatasetEvaluatorConfig(label='chart_qa', data=DataLoaderConfig(dataset='chart_qa', mixture=None, root_size_mixture=None, kwargs_mixture=None, split='validation', seed=691203, pad='to_max', sequence_length=1792, max_text_seq_len=None, shuffle=True, start_index=0, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), evaluator=EvaluatorConfig(n_to_log=0, num_wandb_examples=32, save_predictions=None, save_tokens=False, vqa_eval='relaxed_correctness,scifi_relaxed_correctness,em', pointing_eval=False, count_eval=False, point_count_eval=False, android_eval=False, clock_eval=False, clock_bench_eval=False, math_vista_eval=False, temp_compass_eval='', temp_compass_disable_api=False, video_mme_eval='', mlvu_gen_eval=False, long_video_bench_eval=False, plm_fgqa_eval=False), max_new_tokens=12, device_batch_size=4, subset_num_batches=None, max_examples=16, console_log_interval=5, include_image=False)], inf_eval_interval=20, eval_on_last_step=True, eval_on_load=False, save_folder='debug_run', checkpointer_config=CheckpointerConfig(save_thread_count=None, load_thread_count=None, pre_download=False, work_dir=None, throttle_uploads=False), canceled_check_interval=50, save_interval=2000, save_num_checkpoints_to_keep=1, save_final_unsharded_checkpoint=False, save_interval_ephemeral=None, save_overwrite=True, load_path=None, reset_optimizer_state=False, reset_trainer_state=False, initial_model_checkpoint=None, allow_resume=True, max_duration=1000, global_train_batch_size=8, device_train_microbatch_size=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, beaker_log_interval=50, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=5, gen1_gc_interval=1, compile=CompilerConfig(mode='default', fullgraph=False, dynamic=False, backend='inductor'), activation_checkpointing=True, fsdp=FSDPConfig(fsdp2=True, precision='float', use_orig_params=True, wrapping_strategy='by_block_and_size', sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=1000, stop_after=None, fused_loss=False, compile_loss=True, runtime_data=None)
2025-06-27 14:33:38.682	g3082:0	train:82	INFO	Not resuming since no latest checkpoint found
2025-06-27 14:33:41.095	g3082:0	root:133	INFO	Padding tokenizer with 418 tokens
2025-06-27 14:33:41.726	g3082:0	train:137	INFO	Freezing LLM: wte
2025-06-27 14:33:43.297	g3082:0	train:176	INFO	Wrapping model with FSDP2...
2025-06-27 14:33:44.667	g3082:0	train:187	INFO	Total number of parameters: 57,794,434
2025-06-27 14:33:44.668	g3082:0	train:188	INFO	Number of non-embedding parameters: 38,329,986
2025-06-27 14:33:44.669	g3082:0	train:191	INFO	Peak GPU Memory (MB) after FSDP: 233
2025-06-27 14:33:44.669	g3082:0	train:192	INFO	Model:
2025-06-27 14:33:44.669	g3082:0	train:193	INFO	FSDPMolmo(
  (transformer): Llm(
    (blocks): ModuleList(
      (0): FSDPCheckpointWrapper(
        (_checkpoint_wrapped_module): OLMoSequentialBlock(
          (dropout): Dropout(p=0.1, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=128, out_features=128, bias=True)
          (ff_out): Linear(in_features=256, out_features=128, bias=True)
          (rotary_emb): RotaryEmbedding()
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
          (att_proj): Linear(in_features=128, out_features=384, bias=True)
          (ff_proj): Linear(in_features=128, out_features=512, bias=True)
        )
      )
    )
    (ln_f): FSDPLayerNorm()
    (emb_drop): Dropout(p=0.1, inplace=False)
    (wte): FSDPEmbedding()
    (ff_out): FSDPLinear(in_features=128, out_features=152066, bias=True)
  )
  (vision_backbone): FSDPMolmoVisionBackbone(
    (image_pooling_2d): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ViTMultiHeadDotProductAttention(
        (wq): Linear(in_features=1024, out_features=1024, bias=True)
        (wk): Linear(in_features=1024, out_features=1024, bias=True)
        (wv): Linear(in_features=1024, out_features=1024, bias=True)
        (wo): Linear(in_features=1024, out_features=1024, bias=True)
        (residual_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_projector): FSDPCheckpointWrapper(
      (_checkpoint_wrapped_module): ImageProjectorMLP(
        (w1): Linear(in_features=1024, out_features=256, bias=False)
        (w2): Linear(in_features=256, out_features=128, bias=False)
        (w3): Linear(in_features=1024, out_features=256, bias=False)
        (act): LlamaSwiGLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (image_feature_dropout): Dropout(p=0.0, inplace=False)
    (image_vit): FSDPVisionTransformer(
      (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
      (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): BlockCollection(
        (resblocks): ModuleList(
          (0): FSDPResidualAttentionBlock(
            (attention): ViTMultiHeadDotProductAttention(
              (wq): Linear(in_features=1024, out_features=1024, bias=True)
              (wk): Linear(in_features=1024, out_features=1024, bias=True)
              (wv): Linear(in_features=1024, out_features=1024, bias=True)
              (wo): Linear(in_features=1024, out_features=1024, bias=True)
              (residual_dropout): Dropout(p=0.0, inplace=False)
            )
            (feed_forward): ViTMLP(
              (w1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELUActivation()
              (w2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
2025-06-27 14:33:44.679	g3082:0	olmo.train.optim:167	INFO	Constructing optimizer with 3 param groups
2025-06-27 14:33:44.680	g3082:0	olmo.data.data_loader:197	INFO	Loading train dataset chart_qa/train
2025-06-27 14:33:44.680	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_human.json
2025-06-27 14:33:44.721	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/train/train_augmented.json
2025-06-27 14:33:44.859	g3082:0	olmo.data.data_loader:216	INFO	Sampling rates:
2025-06-27 14:33:44.859	g3082:0	olmo.data.data_loader:219	INFO	chart_qa: 100.00
2025-06-27 14:33:44.861	g3082:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=2304 images=1 pooled_patches_idx=158
2025-06-27 14:33:44.862	g3082:0	olmo.data.data_loader:100	INFO	Loading eval dataset: chart_qa/validation
2025-06-27 14:33:44.862	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_human.json
2025-06-27 14:33:44.867	g3082:0	root:437	INFO	Loading chartqa data from /gscratch/krishna/mahtab/Umolmo/Data/torch_datasets/chartqa/val/val_augmented.json
2025-06-27 14:33:44.871	g3082:0	olmo.models.molmo.molmo:102	INFO	Building collator, pad=to_max seq_len=1792 images=1 pooled_patches_idx=158
2025-06-27 14:33:45.458	g3082:0	train:328	INFO	Starting training...
2025-06-27 14:33:45.459	g3082:0	olmo.train.trainer:759	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=369.5
2025-06-27 14:34:52.880	g3082:0	olmo.train.trainer:759	INFO	[step=5/1000]
    train/CrossEntropyLoss=12.00
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=92,160
    throughput/device/tokens_per_second=61,554
    throughput/device/batches_per_second=3.340
2025-06-27 14:34:54.381	g3082:0	olmo.train.trainer:759	INFO	[step=10/1000]
    train/CrossEntropyLoss=11.96
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=184,320
    throughput/device/tokens_per_second=61,453
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:34:55.882	g3082:0	olmo.train.trainer:759	INFO	[step=15/1000]
    train/CrossEntropyLoss=12.00
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=276,480
    throughput/device/tokens_per_second=61,430
    throughput/device/batches_per_second=3.333
2025-06-27 14:34:57.384	g3082:0	olmo.train.trainer:759	INFO	[step=20/1000]
    train/CrossEntropyLoss=11.93
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=368,640
    throughput/device/tokens_per_second=61,417
    throughput/device/batches_per_second=3.332
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:34:57.386	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:13.997	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:14.015	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:14.015	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 16.6 seconds
2025-06-27 14:35:15.519	g3082:0	olmo.train.trainer:759	INFO	[step=25/1000]
    train/CrossEntropyLoss=11.99
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=460,800
    throughput/device/tokens_per_second=61,458
    throughput/device/batches_per_second=3.334
2025-06-27 14:35:17.019	g3082:0	olmo.train.trainer:759	INFO	[step=30/1000]
    train/CrossEntropyLoss=11.92
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=552,960
    throughput/device/tokens_per_second=61,444
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:18.520	g3082:0	olmo.train.trainer:759	INFO	[step=35/1000]
    train/CrossEntropyLoss=11.86
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=645,120
    throughput/device/tokens_per_second=61,431
    throughput/device/batches_per_second=3.333
2025-06-27 14:35:20.021	g3082:0	olmo.train.trainer:759	INFO	[step=40/1000]
    train/CrossEntropyLoss=11.84
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=737,280
    throughput/device/tokens_per_second=61,427
    throughput/device/batches_per_second=3.333
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:20.023	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:20.417	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:20.434	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:20.435	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:21.934	g3082:0	olmo.train.trainer:759	INFO	[step=45/1000]
    train/CrossEntropyLoss=11.79
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=829,440
    throughput/device/tokens_per_second=61,631
    throughput/device/batches_per_second=3.344
2025-06-27 14:35:23.433	g3082:0	olmo.train.trainer:759	INFO	[step=50/1000]
    train/CrossEntropyLoss=11.76
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=921,600
    throughput/device/tokens_per_second=61,563
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:24.934	g3082:0	olmo.train.trainer:759	INFO	[step=55/1000]
    train/CrossEntropyLoss=11.77
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,013,760
    throughput/device/tokens_per_second=61,494
    throughput/device/batches_per_second=3.336
2025-06-27 14:35:26.435	g3082:0	olmo.train.trainer:759	INFO	[step=60/1000]
    train/CrossEntropyLoss=11.73
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,105,920
    throughput/device/tokens_per_second=61,478
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:26.437	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:26.829	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:26.846	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:26.847	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:28.351	g3082:0	olmo.train.trainer:759	INFO	[step=65/1000]
    train/CrossEntropyLoss=11.65
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,198,080
    throughput/device/tokens_per_second=61,411
    throughput/device/batches_per_second=3.332
2025-06-27 14:35:29.853	g3082:0	olmo.train.trainer:759	INFO	[step=70/1000]
    train/CrossEntropyLoss=11.66
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,290,240
    throughput/device/tokens_per_second=61,402
    throughput/device/batches_per_second=3.331
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:31.355	g3082:0	olmo.train.trainer:759	INFO	[step=75/1000]
    train/CrossEntropyLoss=11.66
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,382,400
    throughput/device/tokens_per_second=61,381
    throughput/device/batches_per_second=3.330
2025-06-27 14:35:32.854	g3082:0	olmo.train.trainer:759	INFO	[step=80/1000]
    train/CrossEntropyLoss=11.61
    train/ZLoss=0.0143
    train/Accuracy=0.0
    throughput/total_tokens=1,474,560
    throughput/device/tokens_per_second=61,407
    throughput/device/batches_per_second=3.332
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:32.856	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:33.247	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:33.264	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:33.265	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:34.765	g3082:0	olmo.train.trainer:759	INFO	[step=85/1000]
    train/CrossEntropyLoss=11.55
    train/ZLoss=0.0143
    train/Accuracy=0.0250
    throughput/total_tokens=1,566,720
    throughput/device/tokens_per_second=61,597
    throughput/device/batches_per_second=3.342
2025-06-27 14:35:36.265	g3082:0	olmo.train.trainer:759	INFO	[step=90/1000]
    train/CrossEntropyLoss=11.55
    train/ZLoss=0.0143
    train/Accuracy=0.0233
    throughput/total_tokens=1,658,880
    throughput/device/tokens_per_second=61,519
    throughput/device/batches_per_second=3.338
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:37.770	g3082:0	olmo.train.trainer:759	INFO	[step=95/1000]
    train/CrossEntropyLoss=11.55
    train/ZLoss=0.0143
    train/Accuracy=0.1026
    throughput/total_tokens=1,751,040
    throughput/device/tokens_per_second=61,427
    throughput/device/batches_per_second=3.333
2025-06-27 14:35:39.267	g3082:0	olmo.train.trainer:759	INFO	[step=100/1000]
    train/CrossEntropyLoss=11.46
    train/ZLoss=0.0143
    train/Accuracy=0.0870
    throughput/total_tokens=1,843,200
    throughput/device/tokens_per_second=61,459
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:39.269	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:39.661	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:39.678	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:39.679	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:41.175	g3082:0	olmo.train.trainer:759	INFO	[step=105/1000]
    train/CrossEntropyLoss=11.46
    train/ZLoss=0.0143
    train/Accuracy=0.0851
    throughput/total_tokens=1,935,360
    throughput/device/tokens_per_second=61,726
    throughput/device/batches_per_second=3.349
2025-06-27 14:35:42.671	g3082:0	olmo.train.trainer:759	INFO	[step=110/1000]
    train/CrossEntropyLoss=11.46
    train/ZLoss=0.0143
    train/Accuracy=0.0750
    throughput/total_tokens=2,027,520
    throughput/device/tokens_per_second=61,672
    throughput/device/batches_per_second=3.346
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:44.165	g3082:0	olmo.train.trainer:759	INFO	[step=115/1000]
    train/CrossEntropyLoss=11.38
    train/ZLoss=0.0143
    train/Accuracy=0.1778
    throughput/total_tokens=2,119,680
    throughput/device/tokens_per_second=61,682
    throughput/device/batches_per_second=3.346
2025-06-27 14:35:45.659	g3082:0	olmo.train.trainer:759	INFO	[step=120/1000]
    train/CrossEntropyLoss=11.36
    train/ZLoss=0.0143
    train/Accuracy=0.2093
    throughput/total_tokens=2,211,840
    throughput/device/tokens_per_second=61,686
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:45.660	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:46.048	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:46.064	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:46.065	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:47.565	g3082:0	olmo.train.trainer:759	INFO	[step=125/1000]
    train/CrossEntropyLoss=11.38
    train/ZLoss=0.0143
    train/Accuracy=0.2432
    throughput/total_tokens=2,304,000
    throughput/device/tokens_per_second=61,620
    throughput/device/batches_per_second=3.343
2025-06-27 14:35:49.063	g3082:0	olmo.train.trainer:759	INFO	[step=130/1000]
    train/CrossEntropyLoss=11.38
    train/ZLoss=0.0143
    train/Accuracy=0.1698
    throughput/total_tokens=2,396,160
    throughput/device/tokens_per_second=61,574
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:50.560	g3082:0	olmo.train.trainer:759	INFO	[step=135/1000]
    train/CrossEntropyLoss=11.33
    train/ZLoss=0.0143
    train/Accuracy=0.2821
    throughput/total_tokens=2,488,320
    throughput/device/tokens_per_second=61,561
    throughput/device/batches_per_second=3.340
2025-06-27 14:35:52.056	g3082:0	olmo.train.trainer:759	INFO	[step=140/1000]
    train/CrossEntropyLoss=11.38
    train/ZLoss=0.0143
    train/Accuracy=0.2075
    throughput/total_tokens=2,580,480
    throughput/device/tokens_per_second=61,576
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:52.058	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:52.451	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:52.468	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:52.468	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:35:53.969	g3082:0	olmo.train.trainer:759	INFO	[step=145/1000]
    train/CrossEntropyLoss=11.29
    train/ZLoss=0.0143
    train/Accuracy=0.2895
    throughput/total_tokens=2,672,640
    throughput/device/tokens_per_second=61,571
    throughput/device/batches_per_second=3.340
2025-06-27 14:35:55.466	g3082:0	olmo.train.trainer:759	INFO	[step=150/1000]
    train/CrossEntropyLoss=11.26
    train/ZLoss=0.0142
    train/Accuracy=0.3529
    throughput/total_tokens=2,764,800
    throughput/device/tokens_per_second=61,573
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:56.962	g3082:0	olmo.train.trainer:759	INFO	[step=155/1000]
    train/CrossEntropyLoss=11.29
    train/ZLoss=0.0142
    train/Accuracy=0.2128
    throughput/total_tokens=2,856,960
    throughput/device/tokens_per_second=61,576
    throughput/device/batches_per_second=3.341
2025-06-27 14:35:58.459	g3082:0	olmo.train.trainer:759	INFO	[step=160/1000]
    train/CrossEntropyLoss=11.27
    train/ZLoss=0.0142
    train/Accuracy=0.2115
    throughput/total_tokens=2,949,120
    throughput/device/tokens_per_second=61,586
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:35:58.460	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:35:58.849	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:35:58.866	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:35:58.866	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:00.372	g3082:0	olmo.train.trainer:759	INFO	[step=165/1000]
    train/CrossEntropyLoss=11.30
    train/ZLoss=0.0142
    train/Accuracy=0.1064
    throughput/total_tokens=3,041,280
    throughput/device/tokens_per_second=61,378
    throughput/device/batches_per_second=3.330
2025-06-27 14:36:01.870	g3082:0	olmo.train.trainer:759	INFO	[step=170/1000]
    train/CrossEntropyLoss=11.33
    train/ZLoss=0.0142
    train/Accuracy=0.2941
    throughput/total_tokens=3,133,440
    throughput/device/tokens_per_second=61,458
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:03.367	g3082:0	olmo.train.trainer:759	INFO	[step=175/1000]
    train/CrossEntropyLoss=11.17
    train/ZLoss=0.0142
    train/Accuracy=0.3611
    throughput/total_tokens=3,225,600
    throughput/device/tokens_per_second=61,484
    throughput/device/batches_per_second=3.336
2025-06-27 14:36:04.868	g3082:0	olmo.train.trainer:759	INFO	[step=180/1000]
    train/CrossEntropyLoss=11.29
    train/ZLoss=0.0142
    train/Accuracy=0.1957
    throughput/total_tokens=3,317,760
    throughput/device/tokens_per_second=61,468
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:04.870	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:05.263	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:05.280	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:05.280	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:06.781	g3082:0	olmo.train.trainer:759	INFO	[step=185/1000]
    train/CrossEntropyLoss=11.25
    train/ZLoss=0.0142
    train/Accuracy=0.2667
    throughput/total_tokens=3,409,920
    throughput/device/tokens_per_second=61,566
    throughput/device/batches_per_second=3.340
2025-06-27 14:36:08.283	g3082:0	olmo.train.trainer:759	INFO	[step=190/1000]
    train/CrossEntropyLoss=11.11
    train/ZLoss=0.0142
    train/Accuracy=0.2571
    throughput/total_tokens=3,502,080
    throughput/device/tokens_per_second=61,459
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:09.778	g3082:0	olmo.train.trainer:759	INFO	[step=195/1000]
    train/CrossEntropyLoss=11.30
    train/ZLoss=0.0142
    train/Accuracy=0.3226
    throughput/total_tokens=3,594,240
    throughput/device/tokens_per_second=61,522
    throughput/device/batches_per_second=3.338
2025-06-27 14:36:11.273	g3082:0	olmo.train.trainer:759	INFO	[step=200/1000]
    train/CrossEntropyLoss=11.22
    train/ZLoss=0.0142
    train/Accuracy=0.2093
    throughput/total_tokens=3,686,400
    throughput/device/tokens_per_second=61,552
    throughput/device/batches_per_second=3.339
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:11.275	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:11.665	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:11.682	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:11.683	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:13.184	g3082:0	olmo.train.trainer:759	INFO	[step=205/1000]
    train/CrossEntropyLoss=11.28
    train/ZLoss=0.0142
    train/Accuracy=0.2000
    throughput/total_tokens=3,778,560
    throughput/device/tokens_per_second=61,546
    throughput/device/batches_per_second=3.339
2025-06-27 14:36:14.684	g3082:0	olmo.train.trainer:759	INFO	[step=210/1000]
    train/CrossEntropyLoss=11.17
    train/ZLoss=0.0142
    train/Accuracy=0.1600
    throughput/total_tokens=3,870,720
    throughput/device/tokens_per_second=61,499
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:16.182	g3082:0	olmo.train.trainer:759	INFO	[step=215/1000]
    train/CrossEntropyLoss=11.14
    train/ZLoss=0.0142
    train/Accuracy=0.2444
    throughput/total_tokens=3,962,880
    throughput/device/tokens_per_second=61,499
    throughput/device/batches_per_second=3.337
2025-06-27 14:36:17.684	g3082:0	olmo.train.trainer:759	INFO	[step=220/1000]
    train/CrossEntropyLoss=11.14
    train/ZLoss=0.0142
    train/Accuracy=0.1481
    throughput/total_tokens=4,055,040
    throughput/device/tokens_per_second=61,468
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:17.686	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:18.073	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:18.090	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:18.090	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:19.589	g3082:0	olmo.train.trainer:759	INFO	[step=225/1000]
    train/CrossEntropyLoss=11.19
    train/ZLoss=0.0142
    train/Accuracy=0.1778
    throughput/total_tokens=4,147,200
    throughput/device/tokens_per_second=61,674
    throughput/device/batches_per_second=3.346
2025-06-27 14:36:21.087	g3082:0	olmo.train.trainer:759	INFO	[step=230/1000]
    train/CrossEntropyLoss=11.16
    train/ZLoss=0.0142
    train/Accuracy=0.2381
    throughput/total_tokens=4,239,360
    throughput/device/tokens_per_second=61,601
    throughput/device/batches_per_second=3.342
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:22.588	g3082:0	olmo.train.trainer:759	INFO	[step=235/1000]
    train/CrossEntropyLoss=11.04
    train/ZLoss=0.0141
    train/Accuracy=0.3043
    throughput/total_tokens=4,331,520
    throughput/device/tokens_per_second=61,528
    throughput/device/batches_per_second=3.338
2025-06-27 14:36:24.085	g3082:0	olmo.train.trainer:759	INFO	[step=240/1000]
    train/CrossEntropyLoss=11.12
    train/ZLoss=0.0141
    train/Accuracy=0.2097
    throughput/total_tokens=4,423,680
    throughput/device/tokens_per_second=61,539
    throughput/device/batches_per_second=3.339
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:24.087	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:24.477	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:24.493	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:24.494	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:25.993	g3082:0	olmo.train.trainer:759	INFO	[step=245/1000]
    train/CrossEntropyLoss=11.03
    train/ZLoss=0.0141
    train/Accuracy=0.2222
    throughput/total_tokens=4,515,840
    throughput/device/tokens_per_second=61,634
    throughput/device/batches_per_second=3.344
2025-06-27 14:36:27.494	g3082:0	olmo.train.trainer:759	INFO	[step=250/1000]
    train/CrossEntropyLoss=11.12
    train/ZLoss=0.0141
    train/Accuracy=0.2250
    throughput/total_tokens=4,608,000
    throughput/device/tokens_per_second=61,530
    throughput/device/batches_per_second=3.338
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:28.992	g3082:0	olmo.train.trainer:759	INFO	[step=255/1000]
    train/CrossEntropyLoss=11.13
    train/ZLoss=0.0141
    train/Accuracy=0.1837
    throughput/total_tokens=4,700,160
    throughput/device/tokens_per_second=61,520
    throughput/device/batches_per_second=3.338
2025-06-27 14:36:30.493	g3082:0	olmo.train.trainer:759	INFO	[step=260/1000]
    train/CrossEntropyLoss=11.01
    train/ZLoss=0.0141
    train/Accuracy=0.2292
    throughput/total_tokens=4,792,320
    throughput/device/tokens_per_second=61,500
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:30.494	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:30.878	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:30.894	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:30.895	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:32.394	g3082:0	olmo.train.trainer:759	INFO	[step=265/1000]
    train/CrossEntropyLoss=10.98
    train/ZLoss=0.0141
    train/Accuracy=0.2368
    throughput/total_tokens=4,884,480
    throughput/device/tokens_per_second=61,652
    throughput/device/batches_per_second=3.345
2025-06-27 14:36:33.888	g3082:0	olmo.train.trainer:759	INFO	[step=270/1000]
    train/CrossEntropyLoss=11.01
    train/ZLoss=0.0141
    train/Accuracy=0.2037
    throughput/total_tokens=4,976,640
    throughput/device/tokens_per_second=61,672
    throughput/device/batches_per_second=3.346
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:35.387	g3082:0	olmo.train.trainer:759	INFO	[step=275/1000]
    train/CrossEntropyLoss=10.97
    train/ZLoss=0.0141
    train/Accuracy=0.2292
    throughput/total_tokens=5,068,800
    throughput/device/tokens_per_second=61,598
    throughput/device/batches_per_second=3.342
2025-06-27 14:36:36.886	g3082:0	olmo.train.trainer:759	INFO	[step=280/1000]
    train/CrossEntropyLoss=11.00
    train/ZLoss=0.0141
    train/Accuracy=0.1818
    throughput/total_tokens=5,160,960
    throughput/device/tokens_per_second=61,576
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:36.889	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:37.282	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:37.298	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:37.299	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:38.802	g3082:0	olmo.train.trainer:759	INFO	[step=285/1000]
    train/CrossEntropyLoss=11.01
    train/ZLoss=0.0141
    train/Accuracy=0.2500
    throughput/total_tokens=5,253,120
    throughput/device/tokens_per_second=61,472
    throughput/device/batches_per_second=3.335
2025-06-27 14:36:40.302	g3082:0	olmo.train.trainer:759	INFO	[step=290/1000]
    train/CrossEntropyLoss=11.13
    train/ZLoss=0.0141
    train/Accuracy=0.1481
    throughput/total_tokens=5,345,280
    throughput/device/tokens_per_second=61,471
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:41.800	g3082:0	olmo.train.trainer:759	INFO	[step=295/1000]
    train/CrossEntropyLoss=10.96
    train/ZLoss=0.0141
    train/Accuracy=0.1887
    throughput/total_tokens=5,437,440
    throughput/device/tokens_per_second=61,486
    throughput/device/batches_per_second=3.336
2025-06-27 14:36:43.298	g3082:0	olmo.train.trainer:759	INFO	[step=300/1000]
    train/CrossEntropyLoss=10.89
    train/ZLoss=0.0140
    train/Accuracy=0.1778
    throughput/total_tokens=5,529,600
    throughput/device/tokens_per_second=61,496
    throughput/device/batches_per_second=3.336
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:43.300	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:43.690	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:43.707	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:43.708	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:45.206	g3082:0	olmo.train.trainer:759	INFO	[step=305/1000]
    train/CrossEntropyLoss=10.96
    train/ZLoss=0.0140
    train/Accuracy=0.3235
    throughput/total_tokens=5,621,760
    throughput/device/tokens_per_second=61,654
    throughput/device/batches_per_second=3.345
2025-06-27 14:36:46.706	g3082:0	olmo.train.trainer:759	INFO	[step=310/1000]
    train/CrossEntropyLoss=10.97
    train/ZLoss=0.0140
    train/Accuracy=0.2766
    throughput/total_tokens=5,713,920
    throughput/device/tokens_per_second=61,559
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:48.202	g3082:0	olmo.train.trainer:759	INFO	[step=315/1000]
    train/CrossEntropyLoss=10.88
    train/ZLoss=0.0140
    train/Accuracy=0.3143
    throughput/total_tokens=5,806,080
    throughput/device/tokens_per_second=61,563
    throughput/device/batches_per_second=3.340
2025-06-27 14:36:49.700	g3082:0	olmo.train.trainer:759	INFO	[step=320/1000]
    train/CrossEntropyLoss=10.89
    train/ZLoss=0.0140
    train/Accuracy=0.3158
    throughput/total_tokens=5,898,240
    throughput/device/tokens_per_second=61,558
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:49.702	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:50.088	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:50.104	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:50.105	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:51.603	g3082:0	olmo.train.trainer:759	INFO	[step=325/1000]
    train/CrossEntropyLoss=10.96
    train/ZLoss=0.0140
    train/Accuracy=0.2667
    throughput/total_tokens=5,990,400
    throughput/device/tokens_per_second=61,693
    throughput/device/batches_per_second=3.347
2025-06-27 14:36:53.101	g3082:0	olmo.train.trainer:759	INFO	[step=330/1000]
    train/CrossEntropyLoss=10.80
    train/ZLoss=0.0140
    train/Accuracy=0.3333
    throughput/total_tokens=6,082,560
    throughput/device/tokens_per_second=61,621
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:54.596	g3082:0	olmo.train.trainer:759	INFO	[step=335/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0140
    train/Accuracy=0.3077
    throughput/total_tokens=6,174,720
    throughput/device/tokens_per_second=61,626
    throughput/device/batches_per_second=3.343
2025-06-27 14:36:56.095	g3082:0	olmo.train.trainer:759	INFO	[step=340/1000]
    train/CrossEntropyLoss=10.88
    train/ZLoss=0.0140
    train/Accuracy=0.2826
    throughput/total_tokens=6,266,880
    throughput/device/tokens_per_second=61,586
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:36:56.097	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:36:56.488	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:36:56.504	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:36:56.505	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:36:58.006	g3082:0	olmo.train.trainer:759	INFO	[step=345/1000]
    train/CrossEntropyLoss=10.81
    train/ZLoss=0.0140
    train/Accuracy=0.3250
    throughput/total_tokens=6,359,040
    throughput/device/tokens_per_second=61,567
    throughput/device/batches_per_second=3.340
2025-06-27 14:36:59.500	g3082:0	olmo.train.trainer:759	INFO	[step=350/1000]
    train/CrossEntropyLoss=10.82
    train/ZLoss=0.0140
    train/Accuracy=0.3158
    throughput/total_tokens=6,451,200
    throughput/device/tokens_per_second=61,622
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:00.997	g3082:0	olmo.train.trainer:759	INFO	[step=355/1000]
    train/CrossEntropyLoss=10.88
    train/ZLoss=0.0140
    train/Accuracy=0.2826
    throughput/total_tokens=6,543,360
    throughput/device/tokens_per_second=61,605
    throughput/device/batches_per_second=3.342
2025-06-27 14:37:02.493	g3082:0	olmo.train.trainer:759	INFO	[step=360/1000]
    train/CrossEntropyLoss=10.83
    train/ZLoss=0.0140
    train/Accuracy=0.3095
    throughput/total_tokens=6,635,520
    throughput/device/tokens_per_second=61,603
    throughput/device/batches_per_second=3.342
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:02.495	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:02.883	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:02.900	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:02.900	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:04.400	g3082:0	olmo.train.trainer:759	INFO	[step=365/1000]
    train/CrossEntropyLoss=10.83
    train/ZLoss=0.0140
    train/Accuracy=0.3077
    throughput/total_tokens=6,727,680
    throughput/device/tokens_per_second=61,602
    throughput/device/batches_per_second=3.342
2025-06-27 14:37:05.899	g3082:0	olmo.train.trainer:759	INFO	[step=370/1000]
    train/CrossEntropyLoss=10.78
    train/ZLoss=0.0139
    train/Accuracy=0.3256
    throughput/total_tokens=6,819,840
    throughput/device/tokens_per_second=61,547
    throughput/device/batches_per_second=3.339
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:07.402	g3082:0	olmo.train.trainer:759	INFO	[step=375/1000]
    train/CrossEntropyLoss=10.78
    train/ZLoss=0.0139
    train/Accuracy=0.3171
    throughput/total_tokens=6,912,000
    throughput/device/tokens_per_second=61,476
    throughput/device/batches_per_second=3.335
2025-06-27 14:37:08.902	g3082:0	olmo.train.trainer:759	INFO	[step=380/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0139
    train/Accuracy=0.3438
    throughput/total_tokens=7,004,160
    throughput/device/tokens_per_second=61,469
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:08.903	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:09.295	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:09.312	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:09.312	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:10.813	g3082:0	olmo.train.trainer:759	INFO	[step=385/1000]
    train/CrossEntropyLoss=10.88
    train/ZLoss=0.0139
    train/Accuracy=0.2927
    throughput/total_tokens=7,096,320
    throughput/device/tokens_per_second=61,591
    throughput/device/batches_per_second=3.342
2025-06-27 14:37:12.310	g3082:0	olmo.train.trainer:759	INFO	[step=390/1000]
    train/CrossEntropyLoss=10.85
    train/ZLoss=0.0139
    train/Accuracy=0.2826
    throughput/total_tokens=7,188,480
    throughput/device/tokens_per_second=61,586
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:13.809	g3082:0	olmo.train.trainer:759	INFO	[step=395/1000]
    train/CrossEntropyLoss=10.91
    train/ZLoss=0.0139
    train/Accuracy=0.2326
    throughput/total_tokens=7,280,640
    throughput/device/tokens_per_second=61,539
    throughput/device/batches_per_second=3.339
2025-06-27 14:37:15.305	g3082:0	olmo.train.trainer:759	INFO	[step=400/1000]
    train/CrossEntropyLoss=10.75
    train/ZLoss=0.0139
    train/Accuracy=0.2791
    throughput/total_tokens=7,372,800
    throughput/device/tokens_per_second=61,564
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:15.306	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:15.706	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:15.723	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:15.724	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:17.229	g3082:0	olmo.train.trainer:759	INFO	[step=405/1000]
    train/CrossEntropyLoss=10.77
    train/ZLoss=0.0139
    train/Accuracy=0.2444
    throughput/total_tokens=7,464,960
    throughput/device/tokens_per_second=61,384
    throughput/device/batches_per_second=3.330
2025-06-27 14:37:18.729	g3082:0	olmo.train.trainer:759	INFO	[step=410/1000]
    train/CrossEntropyLoss=10.84
    train/ZLoss=0.0139
    train/Accuracy=0.3333
    throughput/total_tokens=7,557,120
    throughput/device/tokens_per_second=61,420
    throughput/device/batches_per_second=3.332
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:20.233	g3082:0	olmo.train.trainer:759	INFO	[step=415/1000]
    train/CrossEntropyLoss=10.74
    train/ZLoss=0.0139
    train/Accuracy=0.2889
    throughput/total_tokens=7,649,280
    throughput/device/tokens_per_second=61,371
    throughput/device/batches_per_second=3.330
2025-06-27 14:37:21.732	g3082:0	olmo.train.trainer:759	INFO	[step=420/1000]
    train/CrossEntropyLoss=10.83
    train/ZLoss=0.0139
    train/Accuracy=0.2727
    throughput/total_tokens=7,741,440
    throughput/device/tokens_per_second=61,402
    throughput/device/batches_per_second=3.331
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:21.734	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:22.127	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:22.143	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:22.144	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:23.645	g3082:0	olmo.train.trainer:759	INFO	[step=425/1000]
    train/CrossEntropyLoss=10.73
    train/ZLoss=0.0139
    train/Accuracy=0.2745
    throughput/total_tokens=7,833,600
    throughput/device/tokens_per_second=61,553
    throughput/device/batches_per_second=3.339
2025-06-27 14:37:25.148	g3082:0	olmo.train.trainer:759	INFO	[step=430/1000]
    train/CrossEntropyLoss=10.77
    train/ZLoss=0.0139
    train/Accuracy=0.2593
    throughput/total_tokens=7,925,760
    throughput/device/tokens_per_second=61,453
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:26.648	g3082:0	olmo.train.trainer:759	INFO	[step=435/1000]
    train/CrossEntropyLoss=11.27
    train/ZLoss=0.0139
    train/Accuracy=0.1639
    throughput/total_tokens=8,017,920
    throughput/device/tokens_per_second=61,446
    throughput/device/batches_per_second=3.334
2025-06-27 14:37:28.153	g3082:0	olmo.train.trainer:759	INFO	[step=440/1000]
    train/CrossEntropyLoss=10.69
    train/ZLoss=0.0138
    train/Accuracy=0.2800
    throughput/total_tokens=8,110,080
    throughput/device/tokens_per_second=61,394
    throughput/device/batches_per_second=3.331
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:28.155	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:28.553	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:28.569	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:28.570	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:30.076	g3082:0	olmo.train.trainer:759	INFO	[step=445/1000]
    train/CrossEntropyLoss=10.74
    train/ZLoss=0.0138
    train/Accuracy=0.2885
    throughput/total_tokens=8,202,240
    throughput/device/tokens_per_second=61,363
    throughput/device/batches_per_second=3.329
2025-06-27 14:37:31.574	g3082:0	olmo.train.trainer:759	INFO	[step=450/1000]
    train/CrossEntropyLoss=10.62
    train/ZLoss=0.0138
    train/Accuracy=0.3590
    throughput/total_tokens=8,294,400
    throughput/device/tokens_per_second=61,435
    throughput/device/batches_per_second=3.333
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:33.070	g3082:0	olmo.train.trainer:759	INFO	[step=455/1000]
    train/CrossEntropyLoss=10.76
    train/ZLoss=0.0138
    train/Accuracy=0.3333
    throughput/total_tokens=8,386,560
    throughput/device/tokens_per_second=61,496
    throughput/device/batches_per_second=3.336
2025-06-27 14:37:34.569	g3082:0	olmo.train.trainer:759	INFO	[step=460/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0138
    train/Accuracy=0.3023
    throughput/total_tokens=8,478,720
    throughput/device/tokens_per_second=61,493
    throughput/device/batches_per_second=3.336
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:34.571	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:34.960	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:34.976	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:34.976	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:36.480	g3082:0	olmo.train.trainer:759	INFO	[step=465/1000]
    train/CrossEntropyLoss=10.66
    train/ZLoss=0.0138
    train/Accuracy=0.3590
    throughput/total_tokens=8,570,880
    throughput/device/tokens_per_second=61,445
    throughput/device/batches_per_second=3.334
2025-06-27 14:37:37.978	g3082:0	olmo.train.trainer:759	INFO	[step=470/1000]
    train/CrossEntropyLoss=10.60
    train/ZLoss=0.0138
    train/Accuracy=0.3636
    throughput/total_tokens=8,663,040
    throughput/device/tokens_per_second=61,489
    throughput/device/batches_per_second=3.336
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:39.477	g3082:0	olmo.train.trainer:759	INFO	[step=475/1000]
    train/CrossEntropyLoss=10.74
    train/ZLoss=0.0138
    train/Accuracy=0.2167
    throughput/total_tokens=8,755,200
    throughput/device/tokens_per_second=61,492
    throughput/device/batches_per_second=3.336
2025-06-27 14:37:40.967	g3082:0	olmo.train.trainer:759	INFO	[step=480/1000]
    train/CrossEntropyLoss=10.75
    train/ZLoss=0.0138
    train/Accuracy=0.3421
    throughput/total_tokens=8,847,360
    throughput/device/tokens_per_second=61,576
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:40.969	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:41.355	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:41.371	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:41.372	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:42.864	g3082:0	olmo.train.trainer:759	INFO	[step=485/1000]
    train/CrossEntropyLoss=10.76
    train/ZLoss=0.0138
    train/Accuracy=0.2500
    throughput/total_tokens=8,939,520
    throughput/device/tokens_per_second=61,927
    throughput/device/batches_per_second=3.360
2025-06-27 14:37:44.361	g3082:0	olmo.train.trainer:759	INFO	[step=490/1000]
    train/CrossEntropyLoss=10.83
    train/ZLoss=0.0138
    train/Accuracy=0.3182
    throughput/total_tokens=9,031,680
    throughput/device/tokens_per_second=61,738
    throughput/device/batches_per_second=3.350
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:45.857	g3082:0	olmo.train.trainer:759	INFO	[step=495/1000]
    train/CrossEntropyLoss=10.50
    train/ZLoss=0.0138
    train/Accuracy=0.3750
    throughput/total_tokens=9,123,840
    throughput/device/tokens_per_second=61,687
    throughput/device/batches_per_second=3.347
2025-06-27 14:37:47.352	g3082:0	olmo.train.trainer:759	INFO	[step=500/1000]
    train/CrossEntropyLoss=10.69
    train/ZLoss=0.0138
    train/Accuracy=0.3438
    throughput/total_tokens=9,216,000
    throughput/device/tokens_per_second=61,687
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:47.354	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:47.740	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:47.756	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:47.757	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:49.253	g3082:0	olmo.train.trainer:759	INFO	[step=505/1000]
    train/CrossEntropyLoss=10.80
    train/ZLoss=0.0138
    train/Accuracy=0.2708
    throughput/total_tokens=9,308,160
    throughput/device/tokens_per_second=61,724
    throughput/device/batches_per_second=3.349
2025-06-27 14:37:50.749	g3082:0	olmo.train.trainer:759	INFO	[step=510/1000]
    train/CrossEntropyLoss=10.64
    train/ZLoss=0.0138
    train/Accuracy=0.3261
    throughput/total_tokens=9,400,320
    throughput/device/tokens_per_second=61,682
    throughput/device/batches_per_second=3.346
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:52.247	g3082:0	olmo.train.trainer:759	INFO	[step=515/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0138
    train/Accuracy=0.3409
    throughput/total_tokens=9,492,480
    throughput/device/tokens_per_second=61,629
    throughput/device/batches_per_second=3.344
2025-06-27 14:37:53.746	g3082:0	olmo.train.trainer:759	INFO	[step=520/1000]
    train/CrossEntropyLoss=10.71
    train/ZLoss=0.0138
    train/Accuracy=0.2545
    throughput/total_tokens=9,584,640
    throughput/device/tokens_per_second=61,590
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:53.748	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:37:54.134	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:37:54.150	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:37:54.151	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:37:55.651	g3082:0	olmo.train.trainer:759	INFO	[step=525/1000]
    train/CrossEntropyLoss=10.65
    train/ZLoss=0.0138
    train/Accuracy=0.2692
    throughput/total_tokens=9,676,800
    throughput/device/tokens_per_second=61,603
    throughput/device/batches_per_second=3.342
2025-06-27 14:37:57.147	g3082:0	olmo.train.trainer:759	INFO	[step=530/1000]
    train/CrossEntropyLoss=10.70
    train/ZLoss=0.0137
    train/Accuracy=0.2381
    throughput/total_tokens=9,768,960
    throughput/device/tokens_per_second=61,607
    throughput/device/batches_per_second=3.342
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:37:58.642	g3082:0	olmo.train.trainer:759	INFO	[step=535/1000]
    train/CrossEntropyLoss=10.56
    train/ZLoss=0.0137
    train/Accuracy=0.2917
    throughput/total_tokens=9,861,120
    throughput/device/tokens_per_second=61,614
    throughput/device/batches_per_second=3.343
2025-06-27 14:38:00.140	g3082:0	olmo.train.trainer:759	INFO	[step=540/1000]
    train/CrossEntropyLoss=10.67
    train/ZLoss=0.0137
    train/Accuracy=0.3061
    throughput/total_tokens=9,953,280
    throughput/device/tokens_per_second=61,600
    throughput/device/batches_per_second=3.342
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:00.141	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:00.531	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:00.546	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:00.547	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:02.044	g3082:0	olmo.train.trainer:759	INFO	[step=545/1000]
    train/CrossEntropyLoss=10.54
    train/ZLoss=0.0137
    train/Accuracy=0.3265
    throughput/total_tokens=10,045,440
    throughput/device/tokens_per_second=61,714
    throughput/device/batches_per_second=3.348
2025-06-27 14:38:03.539	g3082:0	olmo.train.trainer:759	INFO	[step=550/1000]
    train/CrossEntropyLoss=10.63
    train/ZLoss=0.0137
    train/Accuracy=0.3061
    throughput/total_tokens=10,137,600
    throughput/device/tokens_per_second=61,704
    throughput/device/batches_per_second=3.348
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:05.030	g3082:0	olmo.train.trainer:759	INFO	[step=555/1000]
    train/CrossEntropyLoss=10.51
    train/ZLoss=0.0137
    train/Accuracy=0.2857
    throughput/total_tokens=10,229,760
    throughput/device/tokens_per_second=61,735
    throughput/device/batches_per_second=3.349
2025-06-27 14:38:06.526	g3082:0	olmo.train.trainer:759	INFO	[step=560/1000]
    train/CrossEntropyLoss=10.62
    train/ZLoss=0.0137
    train/Accuracy=0.3111
    throughput/total_tokens=10,321,920
    throughput/device/tokens_per_second=61,704
    throughput/device/batches_per_second=3.348
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:06.527	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:06.924	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:06.940	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:06.941	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:08.442	g3082:0	olmo.train.trainer:759	INFO	[step=565/1000]
    train/CrossEntropyLoss=10.43
    train/ZLoss=0.0137
    train/Accuracy=0.3590
    throughput/total_tokens=10,414,080
    throughput/device/tokens_per_second=61,572
    throughput/device/batches_per_second=3.341
2025-06-27 14:38:09.939	g3082:0	olmo.train.trainer:759	INFO	[step=570/1000]
    train/CrossEntropyLoss=10.54
    train/ZLoss=0.0137
    train/Accuracy=0.3256
    throughput/total_tokens=10,506,240
    throughput/device/tokens_per_second=61,559
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:11.439	g3082:0	olmo.train.trainer:759	INFO	[step=575/1000]
    train/CrossEntropyLoss=10.75
    train/ZLoss=0.0137
    train/Accuracy=0.2889
    throughput/total_tokens=10,598,400
    throughput/device/tokens_per_second=61,524
    throughput/device/batches_per_second=3.338
2025-06-27 14:38:12.939	g3082:0	olmo.train.trainer:759	INFO	[step=580/1000]
    train/CrossEntropyLoss=10.46
    train/ZLoss=0.0137
    train/Accuracy=0.3684
    throughput/total_tokens=10,690,560
    throughput/device/tokens_per_second=61,504
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:12.941	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:13.334	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:13.350	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:13.351	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:14.852	g3082:0	olmo.train.trainer:759	INFO	[step=585/1000]
    train/CrossEntropyLoss=10.80
    train/ZLoss=0.0137
    train/Accuracy=0.3056
    throughput/total_tokens=10,782,720
    throughput/device/tokens_per_second=61,548
    throughput/device/batches_per_second=3.339
2025-06-27 14:38:16.352	g3082:0	olmo.train.trainer:759	INFO	[step=590/1000]
    train/CrossEntropyLoss=10.36
    train/ZLoss=0.0137
    train/Accuracy=0.4118
    throughput/total_tokens=10,874,880
    throughput/device/tokens_per_second=61,503
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:17.850	g3082:0	olmo.train.trainer:759	INFO	[step=595/1000]
    train/CrossEntropyLoss=10.45
    train/ZLoss=0.0137
    train/Accuracy=0.3750
    throughput/total_tokens=10,967,040
    throughput/device/tokens_per_second=61,500
    throughput/device/batches_per_second=3.337
2025-06-27 14:38:19.347	g3082:0	olmo.train.trainer:759	INFO	[step=600/1000]
    train/CrossEntropyLoss=10.39
    train/ZLoss=0.0137
    train/Accuracy=0.3404
    throughput/total_tokens=11,059,200
    throughput/device/tokens_per_second=61,521
    throughput/device/batches_per_second=3.338
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:19.349	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:19.744	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:19.761	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:19.761	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:21.262	g3082:0	olmo.train.trainer:759	INFO	[step=605/1000]
    train/CrossEntropyLoss=10.35
    train/ZLoss=0.0137
    train/Accuracy=0.3810
    throughput/total_tokens=11,151,360
    throughput/device/tokens_per_second=61,556
    throughput/device/batches_per_second=3.340
2025-06-27 14:38:22.768	g3082:0	olmo.train.trainer:759	INFO	[step=610/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0137
    train/Accuracy=0.3261
    throughput/total_tokens=11,243,520
    throughput/device/tokens_per_second=61,388
    throughput/device/batches_per_second=3.331
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:24.269	g3082:0	olmo.train.trainer:759	INFO	[step=615/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0137
    train/Accuracy=0.3333
    throughput/total_tokens=11,335,680
    throughput/device/tokens_per_second=61,384
    throughput/device/batches_per_second=3.330
2025-06-27 14:38:25.769	g3082:0	olmo.train.trainer:759	INFO	[step=620/1000]
    train/CrossEntropyLoss=10.41
    train/ZLoss=0.0137
    train/Accuracy=0.3333
    throughput/total_tokens=11,427,840
    throughput/device/tokens_per_second=61,400
    throughput/device/batches_per_second=3.331
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:25.771	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:26.172	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:26.189	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:26.190	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:27.687	g3082:0	olmo.train.trainer:759	INFO	[step=625/1000]
    train/CrossEntropyLoss=10.60
    train/ZLoss=0.0137
    train/Accuracy=0.2955
    throughput/total_tokens=11,520,000
    throughput/device/tokens_per_second=61,701
    throughput/device/batches_per_second=3.348
2025-06-27 14:38:29.182	g3082:0	olmo.train.trainer:759	INFO	[step=630/1000]
    train/CrossEntropyLoss=10.36
    train/ZLoss=0.0137
    train/Accuracy=0.4000
    throughput/total_tokens=11,612,160
    throughput/device/tokens_per_second=61,699
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:30.678	g3082:0	olmo.train.trainer:759	INFO	[step=635/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0137
    train/Accuracy=0.2941
    throughput/total_tokens=11,704,320
    throughput/device/tokens_per_second=61,657
    throughput/device/batches_per_second=3.345
2025-06-27 14:38:32.177	g3082:0	olmo.train.trainer:759	INFO	[step=640/1000]
    train/CrossEntropyLoss=10.30
    train/ZLoss=0.0137
    train/Accuracy=0.3721
    throughput/total_tokens=11,796,480
    throughput/device/tokens_per_second=61,622
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:32.178	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:32.564	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:32.580	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:32.581	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:34.080	g3082:0	olmo.train.trainer:759	INFO	[step=645/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0137
    train/Accuracy=0.3404
    throughput/total_tokens=11,888,640
    throughput/device/tokens_per_second=61,609
    throughput/device/batches_per_second=3.343
2025-06-27 14:38:35.574	g3082:0	olmo.train.trainer:759	INFO	[step=650/1000]
    train/CrossEntropyLoss=10.69
    train/ZLoss=0.0136
    train/Accuracy=0.2549
    throughput/total_tokens=11,980,800
    throughput/device/tokens_per_second=61,672
    throughput/device/batches_per_second=3.346
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:37.070	g3082:0	olmo.train.trainer:759	INFO	[step=655/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0137
    train/Accuracy=0.3529
    throughput/total_tokens=12,072,960
    throughput/device/tokens_per_second=61,642
    throughput/device/batches_per_second=3.344
2025-06-27 14:38:38.564	g3082:0	olmo.train.trainer:759	INFO	[step=660/1000]
    train/CrossEntropyLoss=10.41
    train/ZLoss=0.0136
    train/Accuracy=0.3261
    throughput/total_tokens=12,165,120
    throughput/device/tokens_per_second=61,650
    throughput/device/batches_per_second=3.345
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:38.566	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:38.949	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:38.964	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:38.965	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:40.465	g3082:0	olmo.train.trainer:759	INFO	[step=665/1000]
    train/CrossEntropyLoss=10.31
    train/ZLoss=0.0136
    train/Accuracy=0.4054
    throughput/total_tokens=12,257,280
    throughput/device/tokens_per_second=61,615
    throughput/device/batches_per_second=3.343
2025-06-27 14:38:41.964	g3082:0	olmo.train.trainer:759	INFO	[step=670/1000]
    train/CrossEntropyLoss=10.45
    train/ZLoss=0.0136
    train/Accuracy=0.3182
    throughput/total_tokens=12,349,440
    throughput/device/tokens_per_second=61,561
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:43.461	g3082:0	olmo.train.trainer:759	INFO	[step=675/1000]
    train/CrossEntropyLoss=10.39
    train/ZLoss=0.0136
    train/Accuracy=0.3409
    throughput/total_tokens=12,441,600
    throughput/device/tokens_per_second=61,549
    throughput/device/batches_per_second=3.339
2025-06-27 14:38:44.956	g3082:0	olmo.train.trainer:759	INFO	[step=680/1000]
    train/CrossEntropyLoss=10.24
    train/ZLoss=0.0136
    train/Accuracy=0.3721
    throughput/total_tokens=12,533,760
    throughput/device/tokens_per_second=61,572
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:44.958	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:45.343	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:45.359	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:45.359	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:46.857	g3082:0	olmo.train.trainer:759	INFO	[step=685/1000]
    train/CrossEntropyLoss=10.21
    train/ZLoss=0.0136
    train/Accuracy=0.3810
    throughput/total_tokens=12,625,920
    throughput/device/tokens_per_second=61,693
    throughput/device/batches_per_second=3.347
2025-06-27 14:38:48.355	g3082:0	olmo.train.trainer:759	INFO	[step=690/1000]
    train/CrossEntropyLoss=10.64
    train/ZLoss=0.0136
    train/Accuracy=0.2708
    throughput/total_tokens=12,718,080
    throughput/device/tokens_per_second=61,622
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:49.851	g3082:0	olmo.train.trainer:759	INFO	[step=695/1000]
    train/CrossEntropyLoss=10.27
    train/ZLoss=0.0136
    train/Accuracy=0.3846
    throughput/total_tokens=12,810,240
    throughput/device/tokens_per_second=61,609
    throughput/device/batches_per_second=3.343
2025-06-27 14:38:51.350	g3082:0	olmo.train.trainer:759	INFO	[step=700/1000]
    train/CrossEntropyLoss=10.32
    train/ZLoss=0.0136
    train/Accuracy=0.4118
    throughput/total_tokens=12,902,400
    throughput/device/tokens_per_second=61,577
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:51.352	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:51.743	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:51.759	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:51.760	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:53.263	g3082:0	olmo.train.trainer:759	INFO	[step=705/1000]
    train/CrossEntropyLoss=10.63
    train/ZLoss=0.0136
    train/Accuracy=0.2857
    throughput/total_tokens=12,994,560
    throughput/device/tokens_per_second=61,462
    throughput/device/batches_per_second=3.335
2025-06-27 14:38:54.757	g3082:0	olmo.train.trainer:759	INFO	[step=710/1000]
    train/CrossEntropyLoss=10.32
    train/ZLoss=0.0136
    train/Accuracy=0.3256
    throughput/total_tokens=13,086,720
    throughput/device/tokens_per_second=61,583
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:56.252	g3082:0	olmo.train.trainer:759	INFO	[step=715/1000]
    train/CrossEntropyLoss=10.52
    train/ZLoss=0.0136
    train/Accuracy=0.3256
    throughput/total_tokens=13,178,880
    throughput/device/tokens_per_second=61,602
    throughput/device/batches_per_second=3.342
2025-06-27 14:38:57.747	g3082:0	olmo.train.trainer:759	INFO	[step=720/1000]
    train/CrossEntropyLoss=10.63
    train/ZLoss=0.0136
    train/Accuracy=0.2826
    throughput/total_tokens=13,271,040
    throughput/device/tokens_per_second=61,612
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:38:57.749	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:38:58.137	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:38:58.153	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:38:58.154	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:38:59.651	g3082:0	olmo.train.trainer:759	INFO	[step=725/1000]
    train/CrossEntropyLoss=10.39
    train/ZLoss=0.0136
    train/Accuracy=0.2679
    throughput/total_tokens=13,363,200
    throughput/device/tokens_per_second=61,702
    throughput/device/batches_per_second=3.348
2025-06-27 14:39:01.143	g3082:0	olmo.train.trainer:759	INFO	[step=730/1000]
    train/CrossEntropyLoss=10.51
    train/ZLoss=0.0136
    train/Accuracy=0.3111
    throughput/total_tokens=13,455,360
    throughput/device/tokens_per_second=61,738
    throughput/device/batches_per_second=3.350
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:02.642	g3082:0	olmo.train.trainer:759	INFO	[step=735/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0136
    train/Accuracy=0.2857
    throughput/total_tokens=13,547,520
    throughput/device/tokens_per_second=61,648
    throughput/device/batches_per_second=3.345
2025-06-27 14:39:04.138	g3082:0	olmo.train.trainer:759	INFO	[step=740/1000]
    train/CrossEntropyLoss=10.86
    train/ZLoss=0.0136
    train/Accuracy=0.2241
    throughput/total_tokens=13,639,680
    throughput/device/tokens_per_second=61,643
    throughput/device/batches_per_second=3.344
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:04.140	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:04.533	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:04.549	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:04.550	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:06.052	g3082:0	olmo.train.trainer:759	INFO	[step=745/1000]
    train/CrossEntropyLoss=10.33
    train/ZLoss=0.0136
    train/Accuracy=0.3125
    throughput/total_tokens=13,731,840
    throughput/device/tokens_per_second=61,524
    throughput/device/batches_per_second=3.338
2025-06-27 14:39:07.548	g3082:0	olmo.train.trainer:759	INFO	[step=750/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0136
    train/Accuracy=0.2917
    throughput/total_tokens=13,824,000
    throughput/device/tokens_per_second=61,566
    throughput/device/batches_per_second=3.340
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:09.043	g3082:0	olmo.train.trainer:759	INFO	[step=755/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0136
    train/Accuracy=0.3333
    throughput/total_tokens=13,916,160
    throughput/device/tokens_per_second=61,581
    throughput/device/batches_per_second=3.341
2025-06-27 14:39:10.543	g3082:0	olmo.train.trainer:759	INFO	[step=760/1000]
    train/CrossEntropyLoss=10.58
    train/ZLoss=0.0136
    train/Accuracy=0.2885
    throughput/total_tokens=14,008,320
    throughput/device/tokens_per_second=61,548
    throughput/device/batches_per_second=3.339
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:10.545	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:10.931	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:10.948	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:10.948	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:12.449	g3082:0	olmo.train.trainer:759	INFO	[step=765/1000]
    train/CrossEntropyLoss=10.47
    train/ZLoss=0.0136
    train/Accuracy=0.3256
    throughput/total_tokens=14,100,480
    throughput/device/tokens_per_second=61,546
    throughput/device/batches_per_second=3.339
2025-06-27 14:39:13.949	g3082:0	olmo.train.trainer:759	INFO	[step=770/1000]
    train/CrossEntropyLoss=10.29
    train/ZLoss=0.0136
    train/Accuracy=0.3191
    throughput/total_tokens=14,192,640
    throughput/device/tokens_per_second=61,506
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:15.450	g3082:0	olmo.train.trainer:759	INFO	[step=775/1000]
    train/CrossEntropyLoss=10.19
    train/ZLoss=0.0136
    train/Accuracy=0.3721
    throughput/total_tokens=14,284,800
    throughput/device/tokens_per_second=61,465
    throughput/device/batches_per_second=3.335
2025-06-27 14:39:16.951	g3082:0	olmo.train.trainer:759	INFO	[step=780/1000]
    train/CrossEntropyLoss=10.42
    train/ZLoss=0.0136
    train/Accuracy=0.3171
    throughput/total_tokens=14,376,960
    throughput/device/tokens_per_second=61,452
    throughput/device/batches_per_second=3.334
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:16.953	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:17.345	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:17.361	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:17.362	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:18.865	g3082:0	olmo.train.trainer:759	INFO	[step=785/1000]
    train/CrossEntropyLoss=10.49
    train/ZLoss=0.0136
    train/Accuracy=0.3043
    throughput/total_tokens=14,469,120
    throughput/device/tokens_per_second=61,480
    throughput/device/batches_per_second=3.336
2025-06-27 14:39:20.366	g3082:0	olmo.train.trainer:759	INFO	[step=790/1000]
    train/CrossEntropyLoss=10.29
    train/ZLoss=0.0136
    train/Accuracy=0.3019
    throughput/total_tokens=14,561,280
    throughput/device/tokens_per_second=61,434
    throughput/device/batches_per_second=3.333
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:21.863	g3082:0	olmo.train.trainer:759	INFO	[step=795/1000]
    train/CrossEntropyLoss=10.12
    train/ZLoss=0.0136
    train/Accuracy=0.4000
    throughput/total_tokens=14,653,440
    throughput/device/tokens_per_second=61,476
    throughput/device/batches_per_second=3.335
2025-06-27 14:39:23.359	g3082:0	olmo.train.trainer:759	INFO	[step=800/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0136
    train/Accuracy=0.3171
    throughput/total_tokens=14,745,600
    throughput/device/tokens_per_second=61,510
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:23.361	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:23.748	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:23.764	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:23.765	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:25.265	g3082:0	olmo.train.trainer:759	INFO	[step=805/1000]
    train/CrossEntropyLoss=10.24
    train/ZLoss=0.0136
    train/Accuracy=0.3191
    throughput/total_tokens=14,837,760
    throughput/device/tokens_per_second=61,580
    throughput/device/batches_per_second=3.341
2025-06-27 14:39:26.766	g3082:0	olmo.train.trainer:759	INFO	[step=810/1000]
    train/CrossEntropyLoss=10.38
    train/ZLoss=0.0136
    train/Accuracy=0.2459
    throughput/total_tokens=14,929,920
    throughput/device/tokens_per_second=61,498
    throughput/device/batches_per_second=3.336
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:28.264	g3082:0	olmo.train.trainer:759	INFO	[step=815/1000]
    train/CrossEntropyLoss=10.26
    train/ZLoss=0.0136
    train/Accuracy=0.3000
    throughput/total_tokens=15,022,080
    throughput/device/tokens_per_second=61,495
    throughput/device/batches_per_second=3.336
2025-06-27 14:39:29.760	g3082:0	olmo.train.trainer:759	INFO	[step=820/1000]
    train/CrossEntropyLoss=10.32
    train/ZLoss=0.0136
    train/Accuracy=0.3659
    throughput/total_tokens=15,114,240
    throughput/device/tokens_per_second=61,529
    throughput/device/batches_per_second=3.338
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:29.762	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:30.156	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:30.173	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:30.173	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:31.671	g3082:0	olmo.train.trainer:759	INFO	[step=825/1000]
    train/CrossEntropyLoss=10.43
    train/ZLoss=0.0136
    train/Accuracy=0.2800
    throughput/total_tokens=15,206,400
    throughput/device/tokens_per_second=61,715
    throughput/device/batches_per_second=3.348
2025-06-27 14:39:33.161	g3082:0	olmo.train.trainer:759	INFO	[step=830/1000]
    train/CrossEntropyLoss=10.32
    train/ZLoss=0.0136
    train/Accuracy=0.3111
    throughput/total_tokens=15,298,560
    throughput/device/tokens_per_second=61,773
    throughput/device/batches_per_second=3.351
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:34.657	g3082:0	olmo.train.trainer:759	INFO	[step=835/1000]
    train/CrossEntropyLoss=10.47
    train/ZLoss=0.0136
    train/Accuracy=0.2553
    throughput/total_tokens=15,390,720
    throughput/device/tokens_per_second=61,712
    throughput/device/batches_per_second=3.348
2025-06-27 14:39:36.153	g3082:0	olmo.train.trainer:759	INFO	[step=840/1000]
    train/CrossEntropyLoss=10.19
    train/ZLoss=0.0136
    train/Accuracy=0.3636
    throughput/total_tokens=15,482,880
    throughput/device/tokens_per_second=61,692
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:36.155	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:36.557	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:36.573	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:36.573	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:38.069	g3082:0	olmo.train.trainer:759	INFO	[step=845/1000]
    train/CrossEntropyLoss=10.27
    train/ZLoss=0.0136
    train/Accuracy=0.3111
    throughput/total_tokens=15,575,040
    throughput/device/tokens_per_second=61,780
    throughput/device/batches_per_second=3.352
2025-06-27 14:39:39.566	g3082:0	olmo.train.trainer:759	INFO	[step=850/1000]
    train/CrossEntropyLoss=10.37
    train/ZLoss=0.0136
    train/Accuracy=0.3043
    throughput/total_tokens=15,667,200
    throughput/device/tokens_per_second=61,685
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:41.062	g3082:0	olmo.train.trainer:759	INFO	[step=855/1000]
    train/CrossEntropyLoss=10.21
    train/ZLoss=0.0136
    train/Accuracy=0.3750
    throughput/total_tokens=15,759,360
    throughput/device/tokens_per_second=61,648
    throughput/device/batches_per_second=3.345
2025-06-27 14:39:42.559	g3082:0	olmo.train.trainer:759	INFO	[step=860/1000]
    train/CrossEntropyLoss=10.21
    train/ZLoss=0.0136
    train/Accuracy=0.3333
    throughput/total_tokens=15,851,520
    throughput/device/tokens_per_second=61,633
    throughput/device/batches_per_second=3.344
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:42.561	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:42.944	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:42.960	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:42.961	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:44.460	g3082:0	olmo.train.trainer:759	INFO	[step=865/1000]
    train/CrossEntropyLoss=10.21
    train/ZLoss=0.0136
    train/Accuracy=0.3333
    throughput/total_tokens=15,943,680
    throughput/device/tokens_per_second=61,622
    throughput/device/batches_per_second=3.343
2025-06-27 14:39:45.953	g3082:0	olmo.train.trainer:759	INFO	[step=870/1000]
    train/CrossEntropyLoss=10.30
    train/ZLoss=0.0136
    train/Accuracy=0.3256
    throughput/total_tokens=16,035,840
    throughput/device/tokens_per_second=61,689
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:47.445	g3082:0	olmo.train.trainer:759	INFO	[step=875/1000]
    train/CrossEntropyLoss=10.42
    train/ZLoss=0.0136
    train/Accuracy=0.3611
    throughput/total_tokens=16,128,000
    throughput/device/tokens_per_second=61,709
    throughput/device/batches_per_second=3.348
2025-06-27 14:39:48.940	g3082:0	olmo.train.trainer:759	INFO	[step=880/1000]
    train/CrossEntropyLoss=10.35
    train/ZLoss=0.0136
    train/Accuracy=0.3111
    throughput/total_tokens=16,220,160
    throughput/device/tokens_per_second=61,691
    throughput/device/batches_per_second=3.347
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:48.942	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:49.328	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:49.344	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:49.345	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:50.840	g3082:0	olmo.train.trainer:759	INFO	[step=885/1000]
    train/CrossEntropyLoss=10.51
    train/ZLoss=0.0135
    train/Accuracy=0.2889
    throughput/total_tokens=16,312,320
    throughput/device/tokens_per_second=61,777
    throughput/device/batches_per_second=3.352
2025-06-27 14:39:52.333	g3082:0	olmo.train.trainer:759	INFO	[step=890/1000]
    train/CrossEntropyLoss=10.55
    train/ZLoss=0.0136
    train/Accuracy=0.3250
    throughput/total_tokens=16,404,480
    throughput/device/tokens_per_second=61,753
    throughput/device/batches_per_second=3.350
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:53.828	g3082:0	olmo.train.trainer:759	INFO	[step=895/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0135
    train/Accuracy=0.2632
    throughput/total_tokens=16,496,640
    throughput/device/tokens_per_second=61,717
    throughput/device/batches_per_second=3.348
2025-06-27 14:39:55.323	g3082:0	olmo.train.trainer:759	INFO	[step=900/1000]
    train/CrossEntropyLoss=10.15
    train/ZLoss=0.0135
    train/Accuracy=0.3721
    throughput/total_tokens=16,588,800
    throughput/device/tokens_per_second=61,710
    throughput/device/batches_per_second=3.348
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:39:55.324	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:39:55.713	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:39:55.728	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:39:55.729	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:39:57.225	g3082:0	olmo.train.trainer:759	INFO	[step=905/1000]
    train/CrossEntropyLoss=10.24
    train/ZLoss=0.0135
    train/Accuracy=0.3261
    throughput/total_tokens=16,680,960
    throughput/device/tokens_per_second=61,754
    throughput/device/batches_per_second=3.350
2025-06-27 14:39:58.727	g3082:0	olmo.train.trainer:759	INFO	[step=910/1000]
    train/CrossEntropyLoss=10.30
    train/ZLoss=0.0135
    train/Accuracy=0.3261
    throughput/total_tokens=16,773,120
    throughput/device/tokens_per_second=61,578
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:00.229	g3082:0	olmo.train.trainer:759	INFO	[step=915/1000]
    train/CrossEntropyLoss=10.48
    train/ZLoss=0.0135
    train/Accuracy=0.3158
    throughput/total_tokens=16,865,280
    throughput/device/tokens_per_second=61,489
    throughput/device/batches_per_second=3.336
2025-06-27 14:40:01.730	g3082:0	olmo.train.trainer:759	INFO	[step=920/1000]
    train/CrossEntropyLoss=10.23
    train/ZLoss=0.0135
    train/Accuracy=0.3333
    throughput/total_tokens=16,957,440
    throughput/device/tokens_per_second=61,476
    throughput/device/batches_per_second=3.335
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:01.732	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:40:02.125	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:40:02.141	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:40:02.142	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:40:03.641	g3082:0	olmo.train.trainer:759	INFO	[step=925/1000]
    train/CrossEntropyLoss=10.31
    train/ZLoss=0.0135
    train/Accuracy=0.3714
    throughput/total_tokens=17,049,600
    throughput/device/tokens_per_second=61,636
    throughput/device/batches_per_second=3.344
2025-06-27 14:40:05.140	g3082:0	olmo.train.trainer:759	INFO	[step=930/1000]
    train/CrossEntropyLoss=10.16
    train/ZLoss=0.0135
    train/Accuracy=0.3721
    throughput/total_tokens=17,141,760
    throughput/device/tokens_per_second=61,574
    throughput/device/batches_per_second=3.341
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:06.639	g3082:0	olmo.train.trainer:759	INFO	[step=935/1000]
    train/CrossEntropyLoss=10.29
    train/ZLoss=0.0135
    train/Accuracy=0.3684
    throughput/total_tokens=17,233,920
    throughput/device/tokens_per_second=61,528
    throughput/device/batches_per_second=3.338
2025-06-27 14:40:08.140	g3082:0	olmo.train.trainer:759	INFO	[step=940/1000]
    train/CrossEntropyLoss=10.44
    train/ZLoss=0.0135
    train/Accuracy=0.3250
    throughput/total_tokens=17,326,080
    throughput/device/tokens_per_second=61,500
    throughput/device/batches_per_second=3.337
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:08.142	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:40:08.532	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:40:08.547	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:40:08.548	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:40:10.051	g3082:0	olmo.train.trainer:759	INFO	[step=945/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0135
    train/Accuracy=0.3061
    throughput/total_tokens=17,418,240
    throughput/device/tokens_per_second=61,498
    throughput/device/batches_per_second=3.337
2025-06-27 14:40:11.544	g3082:0	olmo.train.trainer:759	INFO	[step=950/1000]
    train/CrossEntropyLoss=10.22
    train/ZLoss=0.0135
    train/Accuracy=0.3333
    throughput/total_tokens=17,510,400
    throughput/device/tokens_per_second=61,610
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:13.040	g3082:0	olmo.train.trainer:759	INFO	[step=955/1000]
    train/CrossEntropyLoss=10.30
    train/ZLoss=0.0135
    train/Accuracy=0.3333
    throughput/total_tokens=17,602,560
    throughput/device/tokens_per_second=61,605
    throughput/device/batches_per_second=3.342
2025-06-27 14:40:14.537	g3082:0	olmo.train.trainer:759	INFO	[step=960/1000]
    train/CrossEntropyLoss=10.22
    train/ZLoss=0.0135
    train/Accuracy=0.3261
    throughput/total_tokens=17,694,720
    throughput/device/tokens_per_second=61,601
    throughput/device/batches_per_second=3.342
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:14.538	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:40:14.925	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:40:14.941	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:40:14.942	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:40:16.438	g3082:0	olmo.train.trainer:759	INFO	[step=965/1000]
    train/CrossEntropyLoss=10.28
    train/ZLoss=0.0135
    train/Accuracy=0.3256
    throughput/total_tokens=17,786,880
    throughput/device/tokens_per_second=61,773
    throughput/device/batches_per_second=3.351
2025-06-27 14:40:17.936	g3082:0	olmo.train.trainer:759	INFO	[step=970/1000]
    train/CrossEntropyLoss=10.30
    train/ZLoss=0.0135
    train/Accuracy=0.3611
    throughput/total_tokens=17,879,040
    throughput/device/tokens_per_second=61,654
    throughput/device/batches_per_second=3.345
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:19.432	g3082:0	olmo.train.trainer:759	INFO	[step=975/1000]
    train/CrossEntropyLoss=10.28
    train/ZLoss=0.0135
    train/Accuracy=0.3333
    throughput/total_tokens=17,971,200
    throughput/device/tokens_per_second=61,630
    throughput/device/batches_per_second=3.344
2025-06-27 14:40:20.928	g3082:0	olmo.train.trainer:759	INFO	[step=980/1000]
    train/CrossEntropyLoss=10.22
    train/ZLoss=0.0135
    train/Accuracy=0.3191
    throughput/total_tokens=18,063,360
    throughput/device/tokens_per_second=61,625
    throughput/device/batches_per_second=3.343
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:20.930	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:40:21.319	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:40:21.335	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:40:21.335	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:40:22.833	g3082:0	olmo.train.trainer:759	INFO	[step=985/1000]
    train/CrossEntropyLoss=10.63
    train/ZLoss=0.0135
    train/Accuracy=0.2222
    throughput/total_tokens=18,155,520
    throughput/device/tokens_per_second=61,697
    throughput/device/batches_per_second=3.347
2025-06-27 14:40:24.330	g3082:0	olmo.train.trainer:759	INFO	[step=990/1000]
    train/CrossEntropyLoss=10.25
    train/ZLoss=0.0135
    train/Accuracy=0.3514
    throughput/total_tokens=18,247,680
    throughput/device/tokens_per_second=61,639
    throughput/device/batches_per_second=3.344
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:25.828	g3082:0	olmo.train.trainer:759	INFO	[step=995/1000]
    train/CrossEntropyLoss=10.33
    train/ZLoss=0.0135
    train/Accuracy=0.3333
    throughput/total_tokens=18,339,840
    throughput/device/tokens_per_second=61,604
    throughput/device/batches_per_second=3.342
2025-06-27 14:40:27.336	g3082:0	olmo.train.trainer:759	INFO	[step=1000/1000]
    train/CrossEntropyLoss=10.34
    train/ZLoss=0.0135
    train/Accuracy=0.3182
    throughput/total_tokens=18,432,000
    throughput/device/tokens_per_second=61,480
    throughput/device/batches_per_second=3.336
    System/Peak GPU Memory (MB)=28,935
2025-06-27 14:40:27.338	g3082:0	olmo.train.trainer:805	INFO	Running evaluation for 'chart_qa'...
2025-06-27 14:40:27.728	g3082:0	olmo.eval.inf_evaluator:241	INFO	[eval_step=4/4]
2025-06-27 14:40:27.744	g3082:0	olmo.train.trainer:759	INFO	chart_qa
    em=0.0
    em_aug=0.0
    em_human=0.0
    relaxed_correctness=0.0
    relaxed_correctness_aug=0.0
    relaxed_correctness_human=0.0
    scifi_relaxed_correctness=0.0
    scifi_relaxed_correctness_aug=0.0
    scifi_relaxed_correctness_human=0.0
2025-06-27 14:40:27.745	g3082:0	olmo.train.trainer:815	INFO	Eval for 'chart_qa' done in 0.4 seconds
2025-06-27 14:40:29.496	g3082:0	olmo.train.trainer:1135	INFO	Saving final checkpoint...
2025-06-27 14:40:31.030	g3082:0	olmo.train.trainer:1137	INFO	Checkpoint saved to debug_run/step1000
2025-06-27 14:40:31.031	g3082:0	train:330	INFO	Training complete
